{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMIJXQKTVQtyHTMgdBY70dz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zia207/r-colab/blob/main/NoteBook/Advance_Regression/02-02-00-regularized-glm-introduction-r.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![alt text](http://drive.google.com/uc?export=view&id=1bLQ3nhDbZrCCqy_WCxxckOne2lgVvn3l)"
      ],
      "metadata": {
        "id": "hhSTeRVWCXq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularized Generalized Linear Model\n",
        "\n",
        "A Regularized Generalized Linear Model (GLM) extends the traditional GLM framework by incorporating regularization techniques to prevent overfitting and improve model generalization. Regularization is particularly useful when dealing with high-dimensional data or when the model is prone to overfitting due to multicollinearity, complex interactions, or a limited number of observations relative to the number of predictors.\n",
        "\n",
        "1.  [Regularized Generalized Linear Model (Gaussian)](https://github.com/zia207/r-colab/blob/main/NoteBook/Advance_Regression/02-02-01-regularized-glm-regression-r.ipynb)\n",
        "\n",
        "2.  [Regularized Generalized Logistic Model (Binary)](https://github.com/zia207/r-colab/blob/main/NoteBook/Advance_Regression/02-02-02-regularized-glm-logistic-r.ipynb)\n",
        "\n",
        "3.  [Regularized Multinominal Logistic Model](https://github.com/zia207/r-colab/blob/main/NoteBook/Advance_Regression/02-02-03-regularized-glm-multinominal-r.ipynb)\n",
        "\n",
        "4.  [Regularized Poisson Regression Model](https://github.com/zia207/r-colab/blob/main/NoteBook/Advance_Regression/02-02-04-regularized-glm-poisson-r.ipynb)"
      ],
      "metadata": {
        "id": "fgQxAeZ77GBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Regularized GLM {.unnumbered}\n",
        "\n",
        "In a standard GLM, the model assumes that the outcome $y$ is drawn from an exponential family distribution (e.g., normal, binomial, Poisson). The relationship between the predictor variables $\\mathbf{X}$ and the expected value of $y$ is defined through a link function $g$ as follows:\n",
        "\n",
        "$$ g(\\mathbb{E}[y]) = \\mathbf{X} \\beta $$\n",
        "\n",
        "where:\n",
        "\n",
        "-   $\\mathbf{X}$ is the design matrix containing $p$ predictor variables,\n",
        "-   $\\beta$) is the vector of coefficients.\n",
        "\n",
        "The likelihood function for GLM coefficients $\\beta$ is maximized to estimate the parameters."
      ],
      "metadata": {
        "id": "Abzb6nIlCqY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regularization in GLM\n",
        "\n",
        "In a Regularized GLM, the model includes a penalty term to the objective function, which discourages large coefficient values and prevents overfitting. Two common types of regularization are **Lasso** (L1) and **Ridge** (L2) regularization:\n",
        "\n",
        "1.  **Lasso (L1) Regularization**: This adds a penalty proportional to the sum of the absolute values of the coefficients. Lasso encourages sparsity, meaning it often shrinks some coefficients to exactly zero. The objective function for Lasso regularization is given by:\n",
        "\n",
        "   $$  \\text{minimize} \\quad - \\log L(\\beta) + \\lambda \\sum_{j=1}^{p} |\\beta_j| $$\n",
        "\n",
        "2.  **Ridge (L2) Regularization**: This adds a penalty proportional to the sum of the squared values of the coefficients. Ridge tends to shrink coefficients uniformly rather than pushing them to zero. The objective function for Ridge regularization is:\n",
        "\n",
        "    $$  \\text{minimize} \\quad - \\log L(\\beta) + \\lambda \\sum_{j=1}^{p} \\beta_j^2 $$\n",
        "\n",
        "3.  **Elastic Net Regularization**: Elastic Net is a combination of Lasso and Ridge regularization, balancing both L1 and L2 penalties. It’s especially useful when there are correlated predictors. The objective function for Elastic Net is:\n",
        "\n",
        "    $$    \\text{minimize} \\quad - \\log L(\\beta) + \\lambda_1 \\sum_{j=1}^{p} |\\beta_j| + \\lambda_2 \\sum_{j=1}^{p} \\beta_j^2 $$"
      ],
      "metadata": {
        "id": "XUmPx_1DCqaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective Function with Regularization\n",
        "\n",
        "The objective function with regularization combines the primary goal of the model (such as minimizing error or maximizing likelihood) with a penalty term to discourage overfitting. This penalty imposes constraints on the model's parameters, typically to ensure simplicity, prevent large coefficients, or enforce sparsity.\n",
        "\n",
        "For a Generalized Linear Model (GLM), the regularized objective function can be written as:\n",
        "\n",
        "\n",
        "\n",
        "$$ \\text{minimize} \\quad - \\log L(\\beta) + \\lambda P $$\n",
        "\n",
        "where:\n",
        "\n",
        "-   $-\\log L(\\beta)$ is the negative log-likelihood for the GLM,\n",
        "-   $\\beta$ is the regularization penalty (e.g., L1, L2, or a combination),\n",
        "-   $\\lambda$ is the regularization parameter that controls the strength of the penalty.\n",
        "\n",
        "The regularization parameter $\\lambda$ is typically chosen through cross-validation to optimize the model’s performance on unseen data."
      ],
      "metadata": {
        "id": "wqHeEEd0DKOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Optimization of the Generalized Linear Model (GLM)  \n",
        "\n",
        "Optimization of the Generalized Linear Model (GLM) refers to the process of finding the best set of coefficients that minimize the loss function. The loss function quantifies the difference between the predicted values and the actual values in the training data. In the context of regularized GLMs, the loss function includes a penalty term that discourages large coefficients, thereby preventing overfitting.  In regularized GLM, we typically aim to find the parameters that maximize the likelihood function, but we also want to avoid overfitting by applying a **penalty** (regularization). Let's break down three common optimization techniques for regularized GLMs: **Maximum Likelihood Estimation (MLE)**, **Gradient Descent (GD)**, and **Coordinate Descent**. Each of these techniques can be used to fit regularized GLMs, with regularization terms like **Lasso (L1)**, **Ridge (L2)**, and **Elastic Net**.\n",
        "\n"
      ],
      "metadata": {
        "id": "ex7INFM7rCct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. **Maximum Likelihood Estimation (MLE)**\n",
        "\n",
        "**Maximum Likelihood Estimation (MLE)** is a fundamental method used in statistical modeling to estimate the parameters of a model. In the context of GLMs, the goal is to **maximize the likelihood** of observing the data given a set of model parameters (coefficients).\n",
        "\n",
        "***For a GLM***:\n",
        "\n",
        "Given the data {$X, y$}, where $X$ is the matrix of feature vectors and $y$ is the vector of responses, we want to estimate the coefficients $\\beta$ that maximize the likelihood function $L(\\beta)$.\n",
        "\n",
        "For a GLM with **logistic regression** (binary outcome), the likelihood is:\n",
        "\n",
        "$$ L(\\beta) = \\prod_{i=1}^{n} P(y_i | X_i, \\beta) = \\prod_{i=1}^{n} \\left(\\frac{1}{1 + e^{-X_i \\beta}}\\right)^{y_i} \\left(1 - \\frac{1}{1 + e^{-X_i \\beta}}\\right)^{1 - y_i} $$\n",
        "\n",
        "The **log-likelihood** is the natural log of this function:\n",
        "\n",
        "$$ \\log L(\\beta) = \\sum_{i=1}^{n} \\left[ y_i \\log(\\sigma(X_i \\beta)) + (1 - y_i) \\log(1 - \\sigma(X_i \\beta)) \\right] $$\n",
        "\n",
        "where $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the logistic function.\n",
        "\n",
        "***Regularized MLE***:\n",
        "\n",
        "In **regularized GLMs**, we add a penalty term to the log-likelihood to avoid overfitting. For example:\n",
        "\n",
        "- **Ridge (L2 regularization)**: $R(\\beta) = \\lambda \\sum_{j=1}^{p} \\beta_j^2$\n",
        "- **Lasso (L1 regularization)**: $R(\\beta) = \\lambda \\sum_{j=1}^{p} |\\beta_j|$\n",
        "- **Elastic Net**: A combination of L1 and L2 regularization.\n",
        "\n",
        "Thus, the **regularized log-likelihood** becomes:\n",
        "\n",
        "$$ \\log L(\\beta) - R(\\beta) = \\sum_{i=1}^{n} \\left[ y_i \\log(\\sigma(X_i \\beta)) + (1 - y_i) \\log(1 - \\sigma(X_i \\beta)) \\right] - \\lambda R(\\beta) $$\n",
        "\n",
        "The goal is to **maximize** this objective function with respect to $\\beta$. In practice, this maximization is typically done numerically, since there is no closed-form solution when regularization is applied.\n"
      ],
      "metadata": {
        "id": "-fvHrELHrLLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. **Gradient Descent (GD)**\n",
        "\n",
        "**Gradient Descent** is a first-order optimization algorithm that iteratively adjusts the parameters in the direction of the steepest decrease of the objective function. This is used to **minimize** a loss function, which in the case of regularized GLMs includes both the **log-likelihood** and the regularization term.\n",
        "\n",
        "**Steps in Gradient Descent**:\n",
        "\n",
        "- The **objective function** for regularized GLMs is the **negative log-likelihood** plus the regularization term.\n",
        "  \n",
        "  $$  \\mathcal{L}(\\beta) = -\\log L(\\beta) + \\lambda R(\\beta) $$\n",
        "  \n",
        "  where $L(\\beta)$ is the likelihood function, and $R(\\beta)$ is the regularization term.\n",
        "  \n",
        "- **Gradient**: We compute the gradient (the derivative) of the objective function with respect to the parameters $\\beta$:\n",
        "  \n",
        "  $$  \\nabla \\mathcal{L}(\\beta) = -\\frac{1}{n} \\sum_{i=1}^{n} X_i^T (y_i - \\sigma(X_i \\beta)) + \\lambda \\nabla R(\\beta) $$\n",
        "\n",
        "  where\n",
        "  $\\sigma(X_i \\beta)$ is the logistic function, and $\\nabla R(\\beta)$ is the gradient of the regularization term (e.g., $\\nabla R(\\beta)$ = $2\\beta$ for Ridge and $\\nabla R(\\beta) = \\text{sign}(\\beta)$ for Lasso).\n",
        "  \n",
        "- **Update Rule**: Using the gradient, we update the parameters in the direction of the negative gradient:\n",
        "  \n",
        "  $$  \\beta^{(t+1)} = \\beta^{(t)} - \\eta \\nabla \\mathcal{L}(\\beta^{(t)}) $$\n",
        "  \n",
        "  where $\\eta$ is the **learning rate** that determines the size of each step.\n",
        "\n",
        "***Variants***:\n",
        "\n",
        "- **Batch Gradient Descent**: Uses the entire dataset to compute the gradient.\n",
        "- **Stochastic Gradient Descent (SGD)**: Uses one sample at a time to compute the gradient, which can be more efficient for large datasets.\n",
        "- **Mini-batch Gradient Descent**: A compromise between Batch GD and SGD, using a small random subset of the data to compute the gradient.\n",
        "\n",
        "***Pros***:\n",
        "\n",
        "- **Flexibility**: Works with any regularization term (L1, L2, or Elastic Net).\n",
        "- **Scalability**: Works well for large datasets, especially with **SGD**.\n",
        "  \n",
        "***Cons***:\n",
        "\n",
        "- **Slow Convergence**: Can take many iterations to converge, especially if the learning rate is not chosen well.\n",
        "- **Requires tuning**: Needs careful tuning of the learning rate and regularization parameter $\\lambda$."
      ],
      "metadata": {
        "id": "x64_2RdFshUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. **Coordinate Descent**\n",
        "\n",
        "**Coordinate Descent** is an optimization algorithm that updates one parameter (or coordinate) at a time, while keeping all other parameters fixed. This method is particularly efficient for models with **Lasso (L1)** regularization and is used to solve the **Lasso regression** problem, which involves **sparse solutions** (many coefficients becoming zero).\n",
        "\n",
        "**Steps in Coordinate Descent**:\n",
        "\n",
        "- The **objective function** for regularized GLMs is the same as in Gradient Decent\n",
        "\n",
        "  $$ \\mathcal{L}(\\beta) = -\\log L(\\beta) + \\lambda R(\\beta) $$\n",
        "  \n",
        "  with the goal to minimize the negative log-likelihood and apply regularization.\n",
        "  \n",
        "- In **Coordinate Descent**, we update one coefficient at a time, while holding the others fixed. The update rule for each coefficient $\\beta_j$ is:\n",
        "\n",
        "  $$  \\beta_j^{(t+1)} = \\text{soft-threshold}( \\hat{\\beta}_j, \\lambda) $$\n",
        "  \n",
        "  where $\\hat{\\beta}_j$ is the coefficient obtained by solving the subproblem for $\\beta_j$, and **soft-thresholding** is used for Lasso (L1 penalty):\n",
        "  \n",
        "  $$ \\text{soft-threshold}(z, \\lambda) = \\text{sign}(z) \\cdot \\max(|z| - \\lambda, 0) $$\n",
        "  \n",
        "  This operation shrinks the coefficient $\\beta_j$ toward zero, and forces it to zero if the magnitude is smaller than $\\lambda$.\n",
        "\n",
        "- **Update**: For each coordinate $j$ , we compute the residuals and update $\\beta_j$ to minimize the objective with respect to that coordinate.\n",
        "\n",
        "***Pros**:\n",
        "\n",
        "- **Efficiency**: Especially efficient for **Lasso** (L1 regularization) and **Elastic Net**.\n",
        "- **Sparsity**: It leads to sparse solutions where many coefficients become exactly zero (important for feature selection).\n",
        "- **Simple to implement**.\n",
        "\n",
        "***Cons***:\n",
        "\n",
        "- **Inefficient for Ridge**: Coordinate Descent is not as efficient for **Ridge** (L2) regularization since it doesn’t exploit the closed-form solution.\n",
        "- **Convergence Speed**: It may take many iterations to converge, especially when the coefficients are not sparse or the regularization is small."
      ],
      "metadata": {
        "id": "yXwbFRUbtl1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary Comparison of the Methods:\n",
        "\n",
        "| Method                  | Description                                  | Best For                                   | Pros                                        | Cons                                      |\n",
        "|-------------------------|----------------------------------------------|--------------------------------------------|---------------------------------------------|-------------------------------------------|\n",
        "| **Maximum Likelihood Estimation (MLE)** | Maximizes the likelihood function, adding regularization as a penalty term | Logistic regression, GLMs in general | Provides the most statistically sound estimates | No closed-form solution with regularization, computationally expensive |\n",
        "| **Gradient Descent (GD)**  | Iterative optimization using the gradient of the objective function | General GLMs with regularization | Flexible for various regularization terms | Can be slow, requires tuning of learning rate |\n",
        "| **Coordinate Descent**     | Updates coefficients one by one, often used for Lasso (L1) | Lasso, Elastic Net | Efficient for sparse models, leads to sparse solutions | Slow for Ridge, needs many iterations for convergence |"
      ],
      "metadata": {
        "id": "U2U2HaH1tl2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benefits of Regularized GLM\n",
        "\n",
        "Regularized GLMs provide several advantages:\n",
        "\n",
        "-   **Improved Generalization**: By penalizing large coefficients, regularized GLMs tend to generalize better to new data.\n",
        "-   **Handling Multicollinearity**: Regularization can stabilize estimates when predictors are highly correlated.\n",
        "-   **Sparse Solutions**: L1 regularization promotes sparsity, which can result in simpler models by setting some coefficients to zero, making interpretation easier.\n",
        "\n",
        "Regularized GLMs are a powerful tool for modeling complex data, especially in high-dimensional settings, by balancing model complexity with predictive performance.\n",
        "\n",
        "The following [table](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/lambda.html) describes the type of penalized model that results based on the values specifed for the `lambda` and `alpha` options.\n",
        "\n",
        "\n",
        "![alt text](http://drive.google.com/uc?export=view&id=1b5dCUZR5vhBEnKq28wb5yJefUJ3Tf7B-)"
      ],
      "metadata": {
        "id": "8m88SGb5DYcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regularized GLM Models in R\n",
        "\n",
        "In R, the {glmnet} package provides a powerful and efficient way to fit Regularized Generalized Linear Models (GLMs) for different types of outcomes, including Gaussian, logistic, multinomial, and Poisson models.  It specializes in Lasso (L1) and Ridge (L2) regularization techniques, making it an ideal tool for dealing with high-dimensional data where the number of predictors exceeds the number of observations or when multicollinearity poses a challenge.\n",
        "\n",
        "\n",
        "![alt text](http://drive.google.com/uc?export=view&id=11lc7FGQbe05xm9q1zOPeXt4szIau2s2d)\n",
        "\n",
        "\n",
        "This package offers efficient algorithms for fitting penalized regression models, including the elastic net - a combination of Lasso and Ridge regression. Additionally, {glmnet} provides functions for cross-validation to identify the best regularization parameter and for extracting fitted model coefficients. \"glmnet\" is widely used in fields such as machine learning, statistics, and data science. It is an excellent choice for feature selection, prediction, and variable importance assessment tasks.\n",
        "\n",
        "When working with {glmnet}, users can utilize various arguments to adjust the fitting process to their needs. This flexibility allows for a more tailored approach to the analysis and can lead to more accurate results. To help with this customization, we have outlined some of the most commonly used arguments below. However, for a more comprehensive understanding of the {glmnet} package and its capabilities, we recommend referring to the official documentation by typing `?glmnet`.\n",
        "\n",
        "The `glmnet()` function, which is used for fitting generalized linear models with regularization (Lasso, Ridge, or Elastic Net), employs **Coordinate Descent**. optimization method.\n",
        "\n",
        "``` r\n",
        " glmnet(x, y, alpha = 1, lambda = NULL)\n",
        "```\n",
        "\n",
        "-   `x`: matrix of predictor variables\n",
        "\n",
        "-   `y`: the response or outcome variable, which is a binary variable.\n",
        "\n",
        "-   `alpha` is for the elastic net mixing parameter α, with range $α∈[0,1].α=1$ is lasso regression (default) and $α=0$ is ridge regression.\n",
        "\n",
        "-   `nlambda` is the number of $λ$ values in the sequence (default is 100).\n",
        "\n",
        "-   `lambda` can be provided if the user wants to specify the lambda sequence, but typical usage is for the program to construct the lambda sequence on its own. When automatically generated, the $λ$ sequence is determined by `lambda.max` and `lambda.min.ratio`. The latter is the ratio of smallest value of the generated $λ$ sequence (say `lambda.min`) to `lambda.max`. The program generates `nlambda` values linear on the log scale from `lambda.max` down to `lambda.min`. `lambda.max` is not user-specified but is computed from the input $x$ and $y$: it is the smallest value for `lambda` such that all the coefficients are zero. For `alpha = 0` (ridge) `lambda.max` would be $∞$: in this case we pick a value corresponding to a small value for `alpha` close to zero.)\n",
        "\n",
        "-   `standardize` is a logical flag for `x` variable standardization prior to fitting the model sequence. The coefficients are always returned on the original scale. Default is `standardize = TRUE`.\n",
        "\n",
        "**K-fold cross-validation** can be performed using the `cv.glmnet` function. In addition to all the glmnet parameters, `cv.glmnet` has its special parameters including `nfolds` (the number of folds), (user-supplied folds), and `type.measure` (the loss used for cross-validation): \\`” or “mse” for squared loss, and\n",
        "\n",
        "-   `mae` uses mean absolute error.\n",
        "\n",
        "-   `alpha` is for the elastic net mixing parameter `α`, with range $α∈[0,1].α=1$ is lasso regression (default) and $α=0$ is ridge regression.\n",
        "\n",
        "-   “1”: for lasso regression\n",
        "\n",
        "-   “0”: for ridge regression\n",
        "\n",
        "-   a value between 0 and 1 (say 0.3) for elastic net regression."
      ],
      "metadata": {
        "id": "oP_AVW6fDdDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step-by-Step Guide\n",
        "\n",
        "1.  **Install and Load the glmnet Package**\n",
        "\n",
        "    ``` r\n",
        "    install.packages(\"glmnet\")\n",
        "    library(glmnet)\n",
        "    ```\n",
        "\n",
        "2.  \\*\\*Prepare the Data\n",
        "\n",
        "    The `glmnet` package expects the predictor matrix $X$ and response vector $y$ in a specific format:\n",
        "\n",
        "    -   $X$: A matrix (not a data frame) of predictor variables.\n",
        "    -   \\$ y\\$: A vector (or factor for classification) of the response variable.\n",
        "\n",
        "    ``` r\n",
        "    # Example of creating matrix format data\n",
        "    X <- as.matrix(your_dataframe[, -ncol(your_dataframe)])  # predictors\n",
        "    y <- your_dataframe$response_variable  # response\n",
        "    ```\n",
        "\n",
        "3.  **Fitting Different Types of Regularized GLMs**\n",
        "\n",
        "    `glmnet` fits models using both Lasso (L1) and Ridge (L2) regularization, controlled by the `alpha` parameter:\n",
        "\n",
        "    -   `alpha = 1`: Lasso regression\n",
        "    -   `alpha = 0`: Ridge regression\n",
        "    -   `0 < alpha < 1`: Elastic Net, a mixture of L1 and L2 regularization"
      ],
      "metadata": {
        "id": "wog9fnScDeKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regularized GLMs for Different Distributions\n"
      ],
      "metadata": {
        "id": "WRHXkbKjDjTj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Gaussian Model (Linear Regression)\n",
        "\n",
        "For a continuous response variable, use the `family = \"gaussian\"` option:\n",
        "\n",
        "``` r\n",
        "# Gaussian model (Linear Regression)\n",
        "fit_gaussian <- glmnet(X, y, family = \"gaussian\", alpha = 1)  # Lasso regularization\n",
        "# Alternatively, use alpha = 0 for Ridge or alpha = 0.5 for Elastic Net\n",
        "```\n"
      ],
      "metadata": {
        "id": "pQLpWN_RDmSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Logistic Regression (Binary Classification)\n",
        "\n",
        "For binary classification, use the `family = \"binomial\"` option:\n",
        "\n",
        "``` r\n",
        "# Logistic model (Binary Classification)\n",
        "fit_logistic <- glmnet(X, y, family = \"binomial\", alpha = 1)  # Lasso regularization\n",
        "```\n"
      ],
      "metadata": {
        "id": "3yoogrxiEAPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Multinomial Model (Multi-Class Classification)\n",
        "\n",
        "For a categorical response with more than two classes, use `family = \"multinomial\"`:\n",
        "\n",
        "``` r\n",
        "# Multinomial model (Multi-Class Classification)\n",
        "fit_multinomial <- glmnet(X, y, family = \"multinomial\", alpha = 1)  # Lasso regularization\n",
        "```\n"
      ],
      "metadata": {
        "id": "TkYScG-WEDqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Poisson Model (Count Data)\n",
        "\n",
        "For count data, use the `family = \"poisson\"` option:\n",
        "\n",
        "``` r\n",
        "# Poisson model (Count Data)\n",
        "fit_poisson <- glmnet(X, y, family = \"poisson\", alpha = 1)  # Lasso regularization\n",
        "```"
      ],
      "metadata": {
        "id": "_n0aqDKyEHth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Validation to Select Optimal Lambda\n",
        "\n",
        "The `cv.glmnet` function performs cross-validation to find the optimal value of $\\lambda$), the regularization parameter. This ensures the best balance between bias and variance.\n",
        "\n",
        "``` r\n",
        "# Cross-validation for the Gaussian model\n",
        "cv_fit_gaussian <- cv.glmnet(X, y, family = \"gaussian\", alpha = 1)\n",
        "best_lambda_gaussian <- cv_fit_gaussian$lambda.min  # Optimal lambda value\n",
        "\n",
        "# Cross-validation for the Logistic model\n",
        "cv_fit_logistic <- cv.glmnet(X, y, family = \"binomial\", alpha = 1)\n",
        "best_lambda_logistic <- cv_fit_logistic$lambda.min\n",
        "\n",
        "# Cross-validation for the Multinomial model\n",
        "cv_fit_multinomial <- cv.glmnet(X, y, family = \"multinomial\", alpha = 1)\n",
        "best_lambda_multinomial <- cv_fit_multinomial$lambda.min\n",
        "\n",
        "# Cross-validation for the Poisson model\n",
        "cv_fit_poisson <- cv.glmnet(X, y, family = \"poisson\", alpha = 1)\n",
        "best_lambda_poisson <- cv_fit_poisson$lambda.min\n",
        "```"
      ],
      "metadata": {
        "id": "wbw6nf8EDq0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making Predictions\n",
        "\n",
        "After determining the best ( \\lambda ) value, you can use it to make predictions on new data:\n",
        "\n",
        "``` r\n",
        "# Predict on new data using the optimal lambda\n",
        "new_X <- as.matrix(new_data[, -ncol(new_data)])  # Convert predictors to matrix\n",
        "\n",
        "# Gaussian model prediction\n",
        "pred_gaussian <- predict(cv_fit_gaussian, new_X, s = best_lambda_gaussian)\n",
        "\n",
        "# Logistic model prediction\n",
        "pred_logistic <- predict(cv_fit_logistic, new_X, s = best_lambda_logistic, type = \"response\")\n",
        "\n",
        "# Multinomial model prediction\n",
        "pred_multinomial <- predict(cv_fit_multinomial, new_X, s = best_lambda_multinomial, type = \"response\")\n",
        "\n",
        "# Poisson model prediction\n",
        "pred_poisson <- predict(cv_fit_poisson, new_X, s = best_lambda_poisson, type = \"response\")\n",
        "```"
      ],
      "metadata": {
        "id": "gUAAk52VETnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary and Conlusion\n",
        "\n",
        "By following this approach, you can perform regularized GLMs for Gaussian, logistic, multinomial, and Poisson models in R using the `glmnet` package. The regularization parameter $\\lambda$ is chosen through cross-validation to enhance model performance on unseen data."
      ],
      "metadata": {
        "id": "cQd3VsaWEW0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Books\n",
        "\n",
        "Here are some books related to the Regularized Generalized Linear Model:\n",
        "\n",
        "1. [Generalized Linear Models - Taylor & Francis eBooks](https://www.taylorfrancis.com/books/mono/10.1201/9780203753736/generalized-linear-models-mccullagh)\n",
        "2. [Generalized Linear Models (Chapman & Hall/CRC Monographs on Statistics)](https://www.amazon.com/Generalized-Chapman-Monographs-Statistics-Probability/dp/0412317605)\n",
        "3. [An Introduction to Generalized Linear Models - Taylor & Francis eBooks](https://www.taylorfrancis.com/books/mono/10.1201/9781315182780/introduction-generalized-linear-models-adrian-barnett-annette-dobson)\n",
        "\n",
        "These books provide comprehensive coverage of generalized linear models and their applications."
      ],
      "metadata": {
        "id": "arh_0zIPE6TK"
      }
    }
  ]
}