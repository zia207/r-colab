{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zia207/r-colab/blob/main/NoteBook/R_Beginner/01-02-03-data-import-export-datatable-feather-arrow-r.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![alt text](http://drive.google.com/uc?export=view&id=1bLQ3nhDbZrCCqy_WCxxckOne2lgVvn3l)"
      ],
      "metadata": {
        "id": "d5rjJ3HatfX5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGoP0AbXEAUx"
      },
      "source": [
        "#  Big-Data Import/Export with {data.table}, {Feather} and {Arrow}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "When working with large and complex datasets in R, it is essential to have effective techniques for importing and exporting data. Since these datasets can be enormous, standard techniques for data transfer can often be insufficient and may result in inefficient and time-consuming processes. Therefore, it is crucial to use efficient data management methods to handle the size and complexity of the datasets involved. Doing so ensures that your data analysis is accurate, reliable, and fast, which is essential when working with big data in R.\n"
      ],
      "metadata": {
        "id": "dVKgDhn8thuA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdQJ-mgsEU9J"
      },
      "source": [
        "## Install rpy2\n",
        "\n",
        "Easy way to run R in Colab with Python runtime using **rpy2** python package. We have to install this package using the pip command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOD7NpajDy5k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab3e0a56-1f57-49d4-b0d7-17290605f591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: rpy2 3.5.1\n",
            "Uninstalling rpy2-3.5.1:\n",
            "  Successfully uninstalled rpy2-3.5.1\n",
            "Collecting rpy2==3.5.1\n",
            "  Using cached rpy2-3.5.1-cp310-cp310-linux_x86_64.whl\n",
            "Requirement already satisfied: cffi>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from rpy2==3.5.1) (1.16.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from rpy2==3.5.1) (3.1.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from rpy2==3.5.1) (2023.4)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from rpy2==3.5.1) (5.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.10.0->rpy2==3.5.1) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->rpy2==3.5.1) (2.1.5)\n",
            "Installing collected packages: rpy2\n",
            "Successfully installed rpy2-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall rpy2 -y\n",
        "! pip install rpy2==3.5.1\n",
        "%load_ext rpy2.ipython"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmEDD0ccEurf"
      },
      "source": [
        "##  Mount Google Drive\n",
        "\n",
        "Then you must create a folder in Goole drive named \"R\" to install all packages permanently. Before installing R-package in Python runtime. You have to mount Google Drive and follow on-screen instruction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lClKZUW1Eu_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65aaed1e-628a-4b2e-ced4-b88c60ade1a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check and Install Required R Packages"
      ],
      "metadata": {
        "id": "BSAv-kcUTTo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "packages <- c(\n",
        "          'tidyverse',\n",
        "          'data.table',\n",
        "          'feather',\n",
        "          'arrow'\n",
        ")"
      ],
      "metadata": {
        "id": "Nan0amGMyBv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Install missing packages\n",
        "new.packages <- packages[!(packages %in% installed.packages(lib='drive/My Drive/R/')[,\"Package\"])]\n",
        "if(length(new.packages)) install.packages(new.packages, lib='drive/My Drive/R/')\n",
        "\n",
        "# Verify installation\n",
        "cat(\"Installed packages:\\n\")\n",
        "print(sapply(packages, requireNamespace, quietly = TRUE))"
      ],
      "metadata": {
        "id": "BVmS9cXeDLFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Packages"
      ],
      "metadata": {
        "id": "1ldcblnJYfks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# set library path\n",
        ".libPaths('drive/My Drive/R')\n",
        "# Load packages with suppressed messages\n",
        "invisible(lapply(packages, function(pkg) {\n",
        "  suppressPackageStartupMessages(library(pkg, character.only = TRUE))\n",
        "}))"
      ],
      "metadata": {
        "id": "nCnpYUs9YftD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "769d7b4e-dab1-408d-c997-36cb8c7e7df9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n",
            "✔ dplyr     1.1.4     ✔ readr     2.1.5\n",
            "✔ forcats   1.0.0     ✔ stringr   1.5.1\n",
            "✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n",
            "✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n",
            "✔ purrr     1.0.2     \n",
            "── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n",
            "✖ dplyr::filter() masks stats::filter()\n",
            "✖ dplyr::lag()    masks stats::lag()\n",
            "ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: data.table 1.15.0 using 1 threads (see ?getDTthreads).  \n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: Latest news: r-datatable.com\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Check loaded packages\n",
        "cat(\"Successfully loaded packages:\\n\")\n",
        "print(search()[grepl(\"package:\", search())])"
      ],
      "metadata": {
        "id": "ji6z6i4fDkRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "All data set use in this exercise can be downloaded from my [Dropbox](https://www.dropbox.com/scl/fo/fohioij7h503duitpl040/h?rlkey=3voumajiklwhgqw75fe8kby3o&dl=0) or from my [Github](https://github.com/zia207/r-colab/tree/main/Data/R_Beginners) accounts.\n"
      ],
      "metadata": {
        "id": "gW6DZpwxZPjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "dataFolder = \"/content/drive/MyDrive/R_Website/R_Bigenner/Data/\"\n",
        "df<-readr::read_csv(paste0(dataFolder,\"nepal_df_balance.csv\")) |>\n",
        "  glimpse()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyIjilAaZjDV",
        "outputId": "152fa985-b9cb-4f6c-9b56-f88cb1aeb9d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows: 17865 Columns: 20\n",
            "── Column specification ────────────────────────────────────────────────────────\n",
            "Delimiter: \",\"\n",
            "chr  (4): Foodstatus_ID, Sex_ID, Region_ID, Livelihood_ID\n",
            "dbl (16): Foodstatus, Schooling_year, Age, Household_size, Rainfed_area, Irr...\n",
            "\n",
            "ℹ Use `spec()` to retrieve the full column specification for this data.\n",
            "ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n",
            "Rows: 17,865\n",
            "Columns: 20\n",
            "$ Foodstatus           <dbl> 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0…\n",
            "$ Schooling_year       <dbl> 0, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n",
            "$ Age                  <dbl> 55, 25, 25, 84, 84, 16, 65, 49, 60, 74, 69, 45, 6…\n",
            "$ Household_size       <dbl> 3, 3, 3, 2, 2, 2, 3, 3, 3, 1, 1, 2, 3, 3, 1, 3, 3…\n",
            "$ Rainfed_area         <dbl> 0.175, 0.258, 0.257, 0.334, 0.334, 0.127, 0.000, …\n",
            "$ Irrigated_area       <dbl> 0.076, 0.000, 0.000, 0.000, 0.000, 0.051, 0.000, …\n",
            "$ Remittance           <dbl> 0.000, 0.000, 0.000, 30.600, 30.600, 0.000, 0.000…\n",
            "$ No_livestock         <dbl> 3.210, 1.960, 1.961, 2.830, 2.830, 1.420, 1.480, …\n",
            "$ Infrastructure_Index <dbl> 0.381, 0.726, 0.727, 0.765, 0.765, 0.773, 0.785, …\n",
            "$ Region               <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n",
            "$ Sex                  <dbl> 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0…\n",
            "$ Caste                <dbl> 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1…\n",
            "$ Livelihood           <dbl> 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n",
            "$ School_Class         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n",
            "$ Household_Class      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n",
            "$ Remitance_Class      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n",
            "$ Foodstatus_ID        <chr> \"Food inadquate\", \"Food inadquate\", \"Food inadqua…\n",
            "$ Sex_ID               <chr> \"Male\", \"Male\", \"Male\", \"Female\", \"Female\", \"Male…\n",
            "$ Region_ID            <chr> \"central\", \"central\", \"central\", \"central\", \"cent…\n",
            "$ Livelihood_ID        <chr> \"Agriculture Household\", \"Agriculture Household\",…\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data.table\n",
        "\n",
        "The [**data.table**](https://rdatatable.gitlab.io/data.table/) is a powerful tool that offers a high-performance alternative to the standard \"data.frame\" object in base R. With a range of syntax and feature enhancements, this package provides unparalleled ease of use, convenience, and programming speed. Whether you're working with large datasets or complex queries, \"data.table\" is a versatile and efficient solution for all your data manipulation needs. With its intuitive syntax, powerful indexing capabilities, and seamless integration with other R packages, \"data.table\" is a must-have tool for any data scientist or analyst looking to optimize their workflow and get the most out of their data."
      ],
      "metadata": {
        "id": "k6hSv7PZYmvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "!> install.packages(\"data.table\")\n",
        "\n",
        "The latest development version (only if newer available)\n",
        "\n",
        "> data.table::update_dev_pkg()\n",
        "\n",
        "The atest development version (force install)\n",
        "\n",
        "> install.packages(\"data.table\", repos=\"https://rdatatable.gitlab.io/data.table\")\n",
        "\n",
        "**Importan Features of data.table**\n",
        "\n",
        "-   fast and friendly delimited **file reader**: [**`?fread`**](https://rdatatable.gitlab.io/data.table/reference/fread.html), see also [convenience features for *small* data](https://github.com/Rdatatable/data.table/wiki/Convenience-features-of-fread)\n",
        "\n",
        "-   fast and feature rich delimited **file writer**: [**`?fwrite`**](https://rdatatable.gitlab.io/data.table/reference/fwrite.html)\n",
        "\n",
        "-   low-level **parallelism**: many common operations are internally parallelized to use multiple CPU threads\n",
        "\n",
        "-   fast and scalable aggregations; e.g. 100GB in RAM (see [benchmarks](https://h2oai.github.io/db-benchmark/) on up to **two billion rows**)\n",
        "\n",
        "-   fast and feature rich joins: **ordered joins** (e.g. rolling forwards, backwards, nearest and limited staleness), [**overlapping range joins**](https://github.com/Rdatatable/data.table/wiki/talks/EARL2014_OverlapRangeJoin_Arun.pdf) (similar to `IRanges::findOverlaps`), [**non-equi joins**](https://github.com/Rdatatable/data.table/wiki/talks/ArunSrinivasanUseR2016.pdf) (i.e. joins using operators `>, >=, <, <=`), **aggregate on join** (`by=.EACHI`), **update on join**\n",
        "\n",
        "-   fast add/update/delete columns **by reference** by group using no copies at all\n",
        "\n",
        "-   fast and feature rich **reshaping** data: [**`?dcast`**](https://rdatatable.gitlab.io/data.table/reference/dcast.data.table.html) (*pivot/wider/spread*) and [**`?melt`**](https://rdatatable.gitlab.io/data.table/reference/melt.data.table.html) (*unpivot/longer/gather*)\n",
        "\n",
        "-   **any R function from any R package** can be used in queries not just the subset of functions made available by a database backend, also columns of type `list` are supported\n",
        "\n",
        "-   has [**no dependencies**](https://en.wikipedia.org/wiki/Dependency_hell) at all other than base R itself, for simpler production/maintenance\n",
        "\n",
        "-   the R dependency is **as old as possible for as long as possible**, dated April 2014, and we continuously test against that version; e.g. v1.11.0 released on 5 May 2018 bumped the dependency up from 5 year old R 3.0.0 to 4 year old R 3.1.0\n"
      ],
      "metadata": {
        "id": "4zaBS1wiPuYn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kroLnhoOPPoV"
      },
      "source": [
        "> install.packages(\"data.table\")\n",
        "\n",
        "The latest development version (only if newer available)\n",
        "\n",
        "> data.table::update_dev_pkg()\n",
        "\n",
        "The atest development version (force install)\n",
        "\n",
        "> install.packages(\"data.table\", repos=\"https://rdatatable.gitlab.io/data.table\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create data.table object\n",
        "\n",
        "We can create data.table object using the `data.table()` function. Here is an example:"
      ],
      "metadata": {
        "id": "vMcDdEiuY2tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "DT= data.table(\n",
        "    Variety =c(\"BR1\",\"BR3\", \"BR16\", \"BR17\", \"BR18\", \"BR19\",\"BR26\",\n",
        "\t      \"BR27\",\"BR28\",\"BR29\",\"BR35\",\"BR36\"),\n",
        "    Yield = c(5.2,6.0,6.6,5.6,4.7,5.2,5.7,\n",
        "\t            5.9,5.3,6.8,6.2,5.8))\n",
        "class(DT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vEj85B1Y3JB",
        "outputId": "ac3b1c31-1f44-481f-f40a-2809200a022c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"data.table\" \"data.frame\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convet data.frame to data.table\n",
        "\n",
        "You can also convert existing objects to  `data.table using`,  `setDT()` for `data.frame`"
      ],
      "metadata": {
        "id": "SV1RwmQkZAY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "DT<-setDT(df)\n",
        "class(DT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-f_sksaZAhX",
        "outputId": "345e49cd-4cf1-41cc-fe00-fdfe6dcbd0c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"data.table\" \"data.frame\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading/Writing CSV file with data.table  `fread()` and `fwrite()`\n",
        "\n",
        "If you're dealing with large datasets and looking for an efficient way to read files into R as data tables, the **data.table** package has got you covered with its highly efficient function called `fread()`. This function outperforms other alternatives like read.csv or read.table and is specifically designed to handle large datasets. So, if you want to save time and increase your productivity, consider using `fread()` for your file reading needs.\n",
        "\n",
        "The `fread()` function in data.table offers a great level of versatility when it comes to efficiently reading various types of delimited files. You can easily specify delimiters, select specific columns, and even set particular data types while reading to optimize memory usage. This function proves to be especially powerful when dealing with large datasets due to its exceptional speed and memory efficiency."
      ],
      "metadata": {
        "id": "21mFmvadZ_wS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# read with fread()\n",
        "df.DT<-data.table::fread(paste0(dataFolder,\"nepal_df_balance.csv\"), header= TRUE)\n",
        "str(df.DT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIJ6PoY6Z_4x",
        "outputId": "33514396-7c48-4910-9622-27d3e2a3832b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes ‘data.table’ and 'data.frame':\t17865 obs. of  20 variables:\n",
            " $ Foodstatus          : int  1 1 1 0 0 1 0 1 1 1 ...\n",
            " $ Schooling_year      : int  0 5 5 0 0 0 0 0 0 0 ...\n",
            " $ Age                 : int  55 25 25 84 84 16 65 49 60 74 ...\n",
            " $ Household_size      : int  3 3 3 2 2 2 3 3 3 1 ...\n",
            " $ Rainfed_area        : num  0.175 0.258 0.257 0.334 0.334 0.127 0 0.052 0.267 0 ...\n",
            " $ Irrigated_area      : num  0.076 0 0 0 0 0.051 0 0.016 0 0 ...\n",
            " $ Remittance          : num  0 0 0 30.6 30.6 0 0 0 0 0 ...\n",
            " $ No_livestock        : num  3.21 1.96 1.96 2.83 2.83 ...\n",
            " $ Infrastructure_Index: num  0.381 0.726 0.727 0.765 0.765 0.773 0.785 0.809 0.82 0.823 ...\n",
            " $ Region              : int  1 1 1 1 1 1 1 1 1 1 ...\n",
            " $ Sex                 : int  0 0 0 1 1 0 0 0 0 0 ...\n",
            " $ Caste               : int  0 0 0 1 1 1 1 1 1 1 ...\n",
            " $ Livelihood          : int  1 1 1 0 0 1 1 1 1 1 ...\n",
            " $ School_Class        : int  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ Household_Class     : int  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ Remitance_Class     : int  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ Foodstatus_ID       : chr  \"Food inadquate\" \"Food inadquate\" \"Food inadquate\" \"Food adquate\" ...\n",
            " $ Sex_ID              : chr  \"Male\" \"Male\" \"Male\" \"Female\" ...\n",
            " $ Region_ID           : chr  \"central\" \"central\" \"central\" \"central\" ...\n",
            " $ Livelihood_ID       : chr  \"Agriculture Household\" \"Agriculture Household\" \"Agriculture Household\" \"Non Agriculture Household\" ...\n",
            " - attr(*, \".internal.selfref\")=<externalptr> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the data.table package of R, `fwrite()` serves as the counterpart to `fread()`. It is primarily utilized for writing data tables to files, usually in CSV or other delimited formats. With a focus on speed and efficiency, `fwrite()` is optimized to handle large datasets effectively. Therefore, it is an excellent option for saving such datasets."
      ],
      "metadata": {
        "id": "TyWmoy78aMCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# read with fread()\n",
        "data.table::fwrite(df.DT,  paste0(dataFolder, \"DT.csv\"), row.names=F, quote=TRUE)"
      ],
      "metadata": {
        "id": "CXQLqLrCaQGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feather: A Fast On-Disk Format for Data Frames\n",
        "\n",
        "Feather is a binary columnar serialization tool that is specifically designed to make reading and writing data frames highly efficient, while also making it easier to share data across various data analysis languages. It offers bindings for both Python (written by Wes McKinney) and R (written by Hadley Wickham) and uses the Apache Arrow columnar memory specification to represent binary data on disk, which results in fast read and write operations. This feature is particularly useful when it comes to encoding null/NA values and variable-length types like UTF8 strings. Feather is an integral part of the Apache Arrow project and defines its own simplified schemas and metadata for on-disk representation.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_SZHht7wdXhy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![alt text](http://drive.google.com/uc?export=view&id=1olj1URtrJ9-vnmvEw3SY3IhBvoMT1GG1)\n",
        "\n"
      ],
      "metadata": {
        "id": "tFmfVDPjfdRe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feather is a fast, lightweight, and easy-to-use binary file format for storing data frames. It has a few specific design goals:\n",
        "\n",
        "-   Lightweight, minimal API: make pushing data frames in and out of memory as simple as possible\n",
        "\n",
        "-   Language agnostic: Feather files are the same whether written by Python or R code. Other languages can read and write Feather files, too.\n",
        "\n",
        "Feather is extremely fast. Since Feather does not currently use any compression internally, it works best when used with solid-state drives as come with most of today's laptop computers. For this first release, we prioritized a simple implementation and are thus writing unmodified Arrow memory to disk [source](https://www.rstudio.com/blog/feather/).\n",
        "\n",
        "Feather currently supports the following column types:\n",
        "\n",
        "-   A wide range of numeric types (int8, int16, int32, int64, uint8, uint16, uint32, uint64, float, double).\n",
        "\n",
        "-   Logical/boolean values.\n",
        "\n",
        "-   Dates, times, and timestamps.\n",
        "\n",
        "-   Factors/categorical variables that have fixed set of possible values.\n",
        "\n",
        "-   UTF-8 encoded strings.\n",
        "\n",
        "-   Arbitrary binary data.\n",
        "\n",
        "All column types support NA/null values.\n",
        "\n",
        "> install.packages(\"feather\")"
      ],
      "metadata": {
        "id": "nQxK-AXzfYYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read/Write with feather\n",
        "\n",
        "The feather package in R provides functions to read and write data in the Feather file format. Feather is a fast, lightweight, and cross-language columnar storage file format designed for efficient data interchange between programming languages."
      ],
      "metadata": {
        "id": "f4CsQrGfm6ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# write_feather()\n",
        "feather::write_feather(df, paste0(dataFolder, \"napal_data.feather\"))"
      ],
      "metadata": {
        "id": "EjvELYSum5oN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read feather file"
      ],
      "metadata": {
        "id": "0uXD1fmfbDj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we use  `read_feather()` function specifically reads data from a Feather file into an R data frame.\n"
      ],
      "metadata": {
        "id": "1uOjIdYWnHG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "df.feather <- feather::read_feather(paste0(dataFolder, \"napal_data.feather\"))\n",
        "str(df.feather)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3atJ6zVsnHv7",
        "outputId": "b013a605-69fa-467f-9747-441ac5126389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tibble [17,865 × 20] (S3: tbl_df/tbl/data.frame)\n",
            " $ Foodstatus          : num [1:17865] 1 1 1 0 0 1 0 1 1 1 ...\n",
            " $ Schooling_year      : num [1:17865] 0 5 5 0 0 0 0 0 0 0 ...\n",
            " $ Age                 : num [1:17865] 55 25 25 84 84 16 65 49 60 74 ...\n",
            " $ Household_size      : num [1:17865] 3 3 3 2 2 2 3 3 3 1 ...\n",
            " $ Rainfed_area        : num [1:17865] 0.175 0.258 0.257 0.334 0.334 0.127 0 0.052 0.267 0 ...\n",
            " $ Irrigated_area      : num [1:17865] 0.076 0 0 0 0 0.051 0 0.016 0 0 ...\n",
            " $ Remittance          : num [1:17865] 0 0 0 30.6 30.6 0 0 0 0 0 ...\n",
            " $ No_livestock        : num [1:17865] 3.21 1.96 1.96 2.83 2.83 ...\n",
            " $ Infrastructure_Index: num [1:17865] 0.381 0.726 0.727 0.765 0.765 0.773 0.785 0.809 0.82 0.823 ...\n",
            " $ Region              : num [1:17865] 1 1 1 1 1 1 1 1 1 1 ...\n",
            " $ Sex                 : num [1:17865] 0 0 0 1 1 0 0 0 0 0 ...\n",
            " $ Caste               : num [1:17865] 0 0 0 1 1 1 1 1 1 1 ...\n",
            " $ Livelihood          : num [1:17865] 1 1 1 0 0 1 1 1 1 1 ...\n",
            " $ School_Class        : num [1:17865] 0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ Household_Class     : num [1:17865] 0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ Remitance_Class     : num [1:17865] 0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ Foodstatus_ID       : chr [1:17865] \"Food inadquate\" \"Food inadquate\" \"Food inadquate\" \"Food adquate\" ...\n",
            " $ Sex_ID              : chr [1:17865] \"Male\" \"Male\" \"Male\" \"Female\" ...\n",
            " $ Region_ID           : chr [1:17865] \"central\" \"central\" \"central\" \"central\" ...\n",
            " $ Livelihood_ID       : chr [1:17865] \"Agriculture Household\" \"Agriculture Household\" \"Agriculture Household\" \"Non Agriculture Household\" ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apache Arrow\n",
        "\n",
        "[Apache Arrow](https://arrow.apache.org/docs/r/index.html) is a cross-language development platform for processing data, both in-memory and larger-than-memory. It provides a standardized, language-independent columnar memory format for flat and hierarchical data, organized to support fast analytic operations on modern hardware. Additionally, it offers computational libraries and zero-copy streaming, messaging, and interprocess communication.\n",
        "\n",
        "![alt text](http://drive.google.com/uc?export=view&id=1uJnX1RjsWQXSuGVZxVcnlBCxGsrsrc8_)\n",
        "\n",
        "\n",
        "The arrow R package exposes an interface to the `Arrow C++ library`, allowing access to many of its features in R. It provides not only low-level access to the Arrow `C++ library API` but also higher-level access through a `dplyr` backend and familiar R functions.\n",
        "\n",
        "The arrow package boasts several key features, including interoperability, columnar data representation, and high performance. Arrow offers seamless communication between different systems and languages, making it easy to exchange data between R and other programming languages such as `Python`, `Julia`, and `C++`. Arrow uses a columnar memory layout, which can be more efficient for many analytical tasks than traditional row-based formats. Arrow is designed for high-performance data processing, making it suitable for big data and parallel computing environments.\n",
        "\n",
        "The arrow package also provides several functionalities. It allows importing data from various sources into R and exporting R data to Arrow files. Arrow data can be manipulated in R for various tasks such as filtering, sorting, and aggregating. Arrow can be integrated with other R packages for advanced data analysis and visualization tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "zL3Z1Om6a1Je"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tabular data in Arrow\n",
        "\n",
        "Apache Arrow relies on its in-memory columnar format, a standardized, programming language-independent definition for representing structured, table-like datasets in memory. The arrow R package employs the Table class to store these objects, which behave like data frames. You can use the `arrow_table()` function to create new Arrow Tables, much like how `data. frame()` is utilized to produce new data frames."
      ],
      "metadata": {
        "id": "I2_J0ytQdiOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "dat <- arrow_table(x = 1:4, y = c(\"a\", \"b\", \"c\", \"d\"))\n",
        "dat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFFGzLOkdlpk",
        "outputId": "e2ab7df1-3ebe-4011-86f3-0ca22f63a7ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table\n",
            "4 rows x 2 columns\n",
            "$x <int32>\n",
            "$y <string>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also convert exiting data.frame to arrow.table:"
      ],
      "metadata": {
        "id": "_kwjPGtTdksj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "arrow.df <- arrow_table(name = rownames(df), df)\n",
        "dim(arrow.df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_K5vkBPKdtnG",
        "outputId": "a423c3ba-9e4b-4bf0-eb3d-d41face26890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] 17865    21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use `[` to specify subsets of Arrow Table in the same way you would for a data frame:"
      ],
      "metadata": {
        "id": "TcWjD9DGdx4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "dat[1:3, 1:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UYuTq4ldznJ",
        "outputId": "86803e69-2ae0-4069-96eb-4c8c67a28936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table\n",
            "3 rows x 2 columns\n",
            "$x <int32>\n",
            "$y <string>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Along the same lines, the `$` operator can be used to extract named columns:"
      ],
      "metadata": {
        "id": "EWDX5lKgd33N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "dat$y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddgk6wd6d5ya",
        "outputId": "8ddb4a33-67ef-4532-aec8-12730a93f649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChunkedArray\n",
            "<string>\n",
            "[\n",
            "  [\n",
            "    \"a\",\n",
            "    \"b\",\n",
            "    \"c\",\n",
            "    \"d\"\n",
            "  ]\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting Arrow Tables to data frames"
      ],
      "metadata": {
        "id": "mCmsCOSgd-KD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "as.data.frame(dat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDg24VQYeAjP",
        "outputId": "85b25901-dde2-40e9-ac91-0f7550930556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  x y\n",
            "1 1 a\n",
            "2 2 b\n",
            "3 3 c\n",
            "4 4 d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert data.fame to arrow.table\n",
        "\n",
        "We can also convert exiting data.frame to arrow.table:"
      ],
      "metadata": {
        "id": "Ga1etovteEoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "df.arrow <- arrow_table(name = rownames(df), df)\n",
        "dim(df.arrow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPte5Cg1eEx3",
        "outputId": "b5ffd6b2-7fd9-4809-8067-06a106b750f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] 17865    21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading and writing data with Arrow\n",
        "\n",
        "One of the critical features of Arrow is its ability to handle data in different formats, including `CSV,` `Parquet,` and `Arrow` (also called Feather). While many packages support `CSV,` Arrow's high-speed CSV reading and writing capabilities make it stand out. Additionally, Arrow supports data formats like Parquet and Arrow, which are not widely supported in other packages, making it an excellent choice for handling complex data structures.\n",
        "\n",
        "Another unique feature of Arrow is its support for multi-file datasets. It can store a single rectangular dataset across multiple files, thus making it possible to work with large datasets that cannot fit into memory. This feature is handy for data scientists and analysts who work with big data and must process large datasets efficiently.\n",
        "\n",
        "When the goal is to read a single data file into memory, there are several functions you can use:\n",
        "\n",
        "`read_parquet()`: read a file in Parquet format\n",
        "\n",
        "`read_feather()`: read a file in Arrow/Feather format\n",
        "\n",
        "`read_delim_arrow()`: read a delimited text file\n",
        "\n",
        "`read_csv_arrow()`: read a comma-separated values (CSV) file\n",
        "\n",
        "`read_tsv_arrow()`: read a tab-separated values (TSV) file\n",
        "\n",
        "`read_json_arrow()`: read a JSON data file\n",
        "\n",
        "For writing data to single files, the arrow package provides the following functions, which can be used with both R data frames and Arrow Tables:\n",
        "\n",
        "`write_parquet()`: write a file in Parquet format\n",
        "\n",
        "`write_feather()`: write a file in Arrow IPC format\n",
        "\n",
        "`write_csv_arrow()`: write a file in CSV format\n",
        "\n",
        "\n",
        "We will write it to a Parquet file using `write_parquet()` function:"
      ],
      "metadata": {
        "id": "CmSdwVjbeN0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "arrow::write_parquet(df, paste0(dataFolder, \"napal_data.parquet\"))"
      ],
      "metadata": {
        "id": "LnSFJCRVePnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then use `read_parquet()` to load the data from this file. As shown below, the default behavior is to return a data frame  but when we set as_data_frame = FALSE the data are read as an Arrow Table:"
      ],
      "metadata": {
        "id": "sgIBb0tQeZjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "df.parquet<- arrow::read_parquet(paste0(dataFolder, \"napal_data.parquet\"))\n"
      ],
      "metadata": {
        "id": "Z7u6qPXPeaNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison\n",
        "\n",
        "### File Size\n",
        "\n",
        "Comparing file sizes among different file formats (data frame, Parquet, Feather, and data table) can be insightful in understanding their efficiency in storage. However, please note that the actual file size depends on various factors such as the data type, compression settings, and the nature of the data itself.\n",
        "\n",
        "Now, check disk space of these three format:"
      ],
      "metadata": {
        "id": "yveVnYYsei-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# CSV file\n",
        "file.info(paste0(dataFolder,\"nepal_df_balance.csv\"))$size/1000\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLk7lnQeelsk",
        "outputId": "f16175fa-1fe4-48b1-98d2-e4f75aae369a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] 1687.577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Feather\n",
        "file.info(paste0(dataFolder,\"napal_data.feather\"))$size/1000\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXsYaJfRepwP",
        "outputId": "5a3627e0-7ee0-4e9c-aec2-3aa9c79f15d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] 3402.08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# parquet\n",
        "file.info(paste0(dataFolder,\"napal_data.parquet\"))$size/1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1sg4CoPet9k",
        "outputId": "466a7d55-dc59-4b33-f257-8635ffd58b68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] 198.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading time"
      ],
      "metadata": {
        "id": "70kgYit9e0IX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# R-base function `read.csv()`\n",
        "system.time(read.csv(paste0(dataFolder,\"nepal_df_balance.csv\"), header= TRUE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZ-iu7Zpe6mq",
        "outputId": "ba6702a8-e548-43ad-c8ae-472df6704bd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   user  system elapsed \n",
            "  0.110   0.004   0.117 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# data.table `fread()`\n",
        "system.time(data.table::fread(paste0(dataFolder,\"nepal_df_balance.csv\"), header= TRUE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izfrjy1yfB6I",
        "outputId": "bf55b7dd-7f59-4aeb-b443-924782901b11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   user  system elapsed \n",
            "  0.013   0.001   0.021 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Feather `read_feather()`\n",
        "system.time(feather::read_feather(paste0(dataFolder, \"napal_data.feather\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sc2Ok-osfKNs",
        "outputId": "1348d6e6-6807-4fcb-af94-a95ec0963646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   user  system elapsed \n",
            "  0.007   0.002   0.014 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Arrow `read_parquet()`\n",
        "system.time( arrow::read_parquet(paste0(dataFolder, \"napal_data.parquet\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iUy9NopfRfu",
        "outputId": "b0af4e87-e795-4089-d5f8-3c9f92491b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   user  system elapsed \n",
            "  0.014   0.006   0.016 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Writing time"
      ],
      "metadata": {
        "id": "brnzKTgDfarC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# R-base function `write.csv()`\n",
        "system.time(write.csv(df,  paste0(dataFolder, \"df.csv\"), row.names=F))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LB7QEVrMezUB",
        "outputId": "d0d0577e-f897-4fd2-ee75-d138c58eec07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   user  system elapsed \n",
            "  0.347   0.002   0.374 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# data.table `fwrite()`\n",
        "system.time(data.table::fwrite(df.DT,  paste0(dataFolder, \"DT.csv\"), row.names=F, quote=TRUE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c63Z0DzaflnK",
        "outputId": "6606c207-834f-4965-f654-04fcf8d304a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   user  system elapsed \n",
            "  0.018   0.000   0.035 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Feather `write_feather()`\n",
        "system.time(feather::write_feather(df.feather, paste0(dataFolder, \"napal_data.feather\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmOEv6h2fsAg",
        "outputId": "2ff640df-5ff1-448f-be0f-22f635768f8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   user  system elapsed \n",
            "  0.004   0.004   0.033 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Arrow `write_parquet()`\n",
        "system.time(arrow::write_parquet(df.feather, paste0(dataFolder, \"napal_data.parquet\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuMvd5-3fx3j",
        "outputId": "628a3f98-38e0-4809-d7f1-ec64db62565b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   user  system elapsed \n",
            "  0.027   0.003   0.043 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Summary\n",
        "\n",
        "Dealing with big data in R requires efficient import and export methods to ensure performance and scalability. Utilizing columnar storage formats like Parquet and Feather, along with database connections and distributed computing frameworks, can help you effectively handle and analyze large datasets in R. Additionally, compression can further optimize storage and transfer of big data files.\n",
        "\n",
        "This tutorial covers efficient data export-import processes using the R packages **data.table**, **Arrow** and **Feather**, which handle large datasets with speed and ease. We explore data.table's syntax for importing and exporting data and feather's binary columnar data format for seamless data exchange between R and other programming languages. Using these packages, data scientists can handle large datasets efficiently, ensuring, storage, speed and readability in data operations.\n",
        "\n",
        "To optimize data manipulation workflows, consider exploring advanced features of data.table, and experimenting with feather's compatibility with various data science ecosystems. On the other hand, the arrow package provides a powerful platform for efficient analytic operations on data, with its standardized columnar memory format, computational libraries, and zero-copy streaming capabilities. Its interoperability, columnar data representation, and high performance make it a valuable tool for big data and parallel computing environments.\n",
        "\n",
        "Compared to other formats like **CSV** and **Feather**,  **Parquet** files have a significantly smaller size on disk, making them an excellent option for handling big data. Although **Feather** files have a faster read and write speed than Parquet, they take up more space on disk. However, the Parquet format supports compression, which helps to reduce the file sizes even further significantly. The actual size of files can depend on various factors, such as the compression codec used (e.g., Snappy, Gzip) and the nature of the data itself. With all these advantages, the Parquet format is an excellent choice for storing large datasets efficiently while keeping their storage costs low.\n",
        "\n"
      ],
      "metadata": {
        "id": "ufqaUvGBKMVe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ovc25MJTsD8"
      },
      "source": [
        "## References\n",
        "\n",
        "1.   [Feather](https://posit.co/blog/feather/)\n",
        "\n",
        "2.  [data.tabler](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html)\n",
        "\n",
        "3. [Arrow](https://arrow.apache.org/docs/r/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOBKKzdNTRqsdg00xtjeQch",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}