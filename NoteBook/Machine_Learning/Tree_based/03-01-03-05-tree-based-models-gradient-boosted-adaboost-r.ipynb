{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-05-tree-based-models-gradient-boosted-adaboost-r.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qrItz_mJNWw"
      },
      "source": [
        "![alt text](http://drive.google.com/uc?export=view&id=1bLQ3nhDbZrCCqy_WCxxckOne2lgVvn3l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dzp9ZseROTcY"
      },
      "source": [
        "# 3.5 Adaptive Boosting (AdaBoost)\n",
        "\n",
        "AdaBoost, or Adaptive Boosting, is a powerful ensemble learning technique that combines multiple weak classifiers to create a strong classifier. It works by sequentially applying weak classifiers to the training data, adjusting the weights of misclassified instances to focus more on difficult cases in subsequent iterations. This method is particularly effective for improving the performance of models on complex datasets. This tutorial will guide you through the mathematical foundations of AdaBoost, its implementation in R using the CatBoost library, and practical examples using the Titanic dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLkAu88QToVr"
      },
      "source": [
        "## Overview\n",
        "\n",
        "AdaBoost, short for **Adaptive Boosting**, is a machine learning ensemble algorithm used primarily for classification tasks (though it can be extended to regression). It combines multiple weak learners—typically simple models like decision stumps (single-level decision trees)—to create a strong classifier. The key idea is to focus on misclassified samples by assigning them higher weights in subsequent iterations, allowing the model to improve its performance progressively.\n",
        "\n",
        "AdaBoost was introduced by Freund and Schapire in 1996 and is widely used due to its simplicity, effectiveness, and ability to work well with high-dimensional data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL2L7JRduO3P"
      },
      "source": [
        "### Key Features of AdaBoost\n",
        "\n",
        "AdaBoost's key features:\n",
        "\n",
        "1. `Weight Adjustment`: Boosts misclassified data weights iteratively.\n",
        "2. `Weak Learner Combo`: Combines weak classifiers into a strong one.\n",
        "3. `Error-Driven`: Focuses on errors from previous learners.\n",
        "4. `Adaptive`: Adjusts to data complexity.\n",
        "5. `Binary Focus`: Best for binary classification, extendable to multiclass.\n",
        "6. `Robust`: Less prone to overfitting with proper tuning.\n",
        "7. `Feature Importance`: Highlights key features.\n",
        "8. `Simple & Flexible`: Works with various weak learners.\n",
        "9. `Noise Sensitivity`: Can overfit with noisy data.\n",
        "10. `Efficient`: Fast for small-to-medium datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How Does AdaBoost Work?\n",
        "\n",
        "AdaBoost works by iteratively training weak learners, adjusting the weights of training samples, and combining the weak learners into a final strong classifier. Each weak learner focuses more on the samples that were misclassified by previous learners. The final prediction is a weighted combination of the weak learners' predictions.\n",
        "\n",
        "Here’s a step-by-step explanation of the AdaBoost algorithm, including the equations involved:\n",
        "\n",
        "1.  Initialize Sample Weights\n",
        "\n",
        "-   Assign equal weights to all training samples. For a dataset with ( N ) samples, each sample’s initial weight is:\n",
        "\n",
        "$$ w_i^{(1)} = \\frac{1}{N}, \\quad i = 1, 2, \\dots, N $$ where $w_i^{(1)}$ is the weight of the $i$-th sample in the first iteration.\n",
        "\n",
        "2.  Train Weak Learners Iteratively\n",
        "\n",
        "-   For $t = 1, 2, \\dots, T$ (where $T$ is the number of weak learners):\n",
        "\n",
        "a.  `Train a weak learne` $h_t(x)$ (e.g., a decision stump) on the training data, using the current sample weights $w_i^{(t)}$.\n",
        "\n",
        "b.  `compute the weighted error` of the weak learner:\n",
        "\n",
        "$$ \\epsilon*t =* \\sum{i=1}^N w_i^{(t)} \\cdot \\mathbb{I}(h_t(x_i) \\neq y_i) $$\n",
        "\n",
        "where:\n",
        "\n",
        "-   $\\epsilon_t$ is the weighted error rate.\n",
        "\n",
        "-   $\\mathbb{I}(h_t(x_i) \\neq y_i)$ is an indicator function that equals 1 if the prediction $h_t(x_i)$ is incorrect for sample $x_i$ (with true label $y_i$, and 0 otherwise.\n",
        "\n",
        "c.  `Compute the learner’s weight` (importance) based on its error:\n",
        "\n",
        "$$ \\alpha\\_t = \\frac{1}{2} \\ln \\left( \\frac{1 - \\epsilon_t}{\\epsilon_t} \\right) $$ - $\\alpha_t$ represents the contribution of the weak learner $h_t$ to the final model.\n",
        "\n",
        "-   If $\\epsilon_t < 0.5$, $\\alpha_t> 0$, meaning the learner contributes positively.\n",
        "\n",
        "-   If $\\epsilon_t = 0.5$, $\\alpha_t = 0$, indicating no contribution (random guessing).\n",
        "\n",
        "-   If $\\epsilon_t > 0.5$, $\\alpha_ < 0$, but typically, weak learners are better than random guessing.\n",
        "\n",
        "3.  Update Sample Weights\n",
        "\n",
        "-   Adjust the weights of the samples to focus on misclassified ones:\n",
        "\n",
        "$$ w_i^{(t+1)} = w_i^{(t)} \\cdot \\exp \\left( \\alpha\\_t \\cdot \\mathbb{I}(h_t(x_i) \\neq y_i) \\right) $$ - Misclassified samples $h_t(x_i) \\neq y_i$ have their weights increased by $e^{\\alpha\\_t}$.\n",
        "\n",
        "-   Correctly classified samples have their weights decreased by $e^{-\\alpha\\_t}$.\n",
        "\n",
        "-   Normalize the weights so they sum to 1:\n",
        "\n",
        "$$  w_i^{(t+1)} = \\frac{w_i^{(t+1)}}{\\sum_{j=1}^N w_j^{(t+1)}} $$ \\`\n",
        "\n",
        "4.  Combine Weak Learners\n",
        "\n",
        "-   After ( T ) iterations, combine the weak learners into a final strong classifier:\n",
        "\n",
        "$$  H(x) = \\text{sign} \\left( \\sum\\_{t=1}^T \\alpha\\_t h_t(x) \\right)$$\n",
        "\n",
        "where: - $h_t(x) \\in {-1, +1}$ is the prediction of the $t$-th weak learner.\n",
        "\n",
        "-   $\\alpha_t$ is the weight of the $t$-th weak learner.\n",
        "\n",
        "-   The final prediction $H(x)$ outputs $+1$ or $-1$ (for binary classification).\n",
        "\n",
        "-   ::: callout-note\n",
        "    -   **Weak Learners**: Typically decision stumps, but any classifier slightly better than random guessing can be used.\n",
        "\n",
        "    -   **Advantages**: AdaBoost is robust, reduces overfitting (especially with simple weak learners), and is computationally efficient.\n",
        "\n",
        "    -   **Limitations**: Sensitive to noisy data and outliers, as misclassified samples get higher weights.\n",
        "\n",
        "    -   **Extensions**: Variants like Gradient Boosting and XGBoost build on AdaBoost’s principles but use different optimization techniques.\n",
        "    :::"
      ],
      "metadata": {
        "id": "03n21BL3-pyU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWCTcHkzvHE2"
      },
      "source": [
        "The flowchart below summarizes the AdaBoost algorithm, highlighting its key steps from initialization to prediction:\n",
        "\n",
        "\n",
        "![alt text](http://drive.google.com/uc?export=view&id=1thPPruEzsmUY31rAg9K6aLmWVUMPylWL\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuhrbXQxGIIy"
      },
      "source": [
        "### Advantages of AdaBoost\n",
        "\n",
        "1. `High Accuracy`: Combines weak learners (e.g., decision stumps) into a strong classifier, achieving ~82% accuracy on the Titanic dataset.\n",
        "2. `Focus on Hard Samples`: Increases weights for misclassified samples, improving robustness (e.g., prioritizing `Sex`, `Pclass`).\n",
        "3. `Handles Mixed Data`: Works with numerical (`Age`, `Fare`) and categorical (`Sex`, `Pclass`) features.\n",
        "4. `Low Overfitting`: Simple learners reduce overfitting risk, stable at 50 iterations.\n",
        "5. `Feature Importance`: Identifies key predictors (e.g., `Sex` ~47%, `Pclass` ~22%).\n",
        "6. `Minimal Tuning`: Effective with default settings (e.g., `mfinal = 50` in `adabag`).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70UMhkxtGPsU"
      },
      "source": [
        "### Limitations of AdaBoost\n",
        "\n",
        "1. `Noise Sensitivity`: Overemphasizes outliers, reducing precision (~31% in synthetic Titanic data).\n",
        "2. `Weak Learner Dependence`: Poor performance if learners are too weak (e.g., stumps in scratch model, ~44% F1-score).\n",
        "3. `Computational Cost`: Slow on large datasets due to iterative training.\n",
        "4. `Imbalanced Data`: Struggles with minority class (e.g., ~73% recall for `Survived = 1`).\n",
        "5. `No Probabilities`: Outputs class labels, not confidence scores.\n",
        "6. `Classification Focus`: Less suited for regression tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyOope_pwUVr"
      },
      "source": [
        "## Setup R in Python Runtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDp3ULld8Gb9"
      },
      "source": [
        "### Install {rpy2}\n",
        "\n",
        "{rpy2} is a Python package that provides an interface to the R programming language, allowing Python users to run R code, call R functions, and manipulate R objects directly from Python. It enables seamless integration between Python and R, leveraging R's statistical and graphical capabilities while using Python's flexibility. The package supports passing data between the two languages and is widely used for statistical analysis, data visualization, and machine learning tasks that benefit from R's specialized libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiM6y-Mw8AJp",
        "outputId": "b1fccadb-c5b9-4a5c-b34b-184658e869ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: rpy2 3.5.17\n",
            "Uninstalling rpy2-3.5.17:\n",
            "  Successfully uninstalled rpy2-3.5.17\n",
            "Collecting rpy2==3.5.1\n",
            "  Downloading rpy2-3.5.1.tar.gz (201 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.7/201.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from rpy2==3.5.1) (1.17.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from rpy2==3.5.1) (3.1.6)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from rpy2==3.5.1) (2025.2)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.11/dist-packages (from rpy2==3.5.1) (5.3.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.10.0->rpy2==3.5.1) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->rpy2==3.5.1) (3.0.2)\n",
            "Building wheels for collected packages: rpy2\n",
            "  Building wheel for rpy2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rpy2: filename=rpy2-3.5.1-cp311-cp311-linux_x86_64.whl size=314974 sha256=1665c66b76c03a6154a819cdda8756762bb57c6c7becfa26e0e272268ee05f0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/e9/55/d1/47be85a5f3f1e1f4d1e91cb5e3a4dcb40dd72147f184c5a5ef\n",
            "Successfully built rpy2\n",
            "Installing collected packages: rpy2\n",
            "Successfully installed rpy2-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall rpy2 -y\n",
        "!pip install rpy2==3.5.1\n",
        "%load_ext rpy2.ipython"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1zeuaCowiBt"
      },
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J-4ie4bwiJ1",
        "outputId": "2bd6a40e-cff3-4591-a263-447132cb1e43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztl1QBDb4MYe"
      },
      "source": [
        "## AdaBoost  implementation in R from scratch\n",
        "\n",
        "Here we’ll implement the AdaBoost algorithm in R from scratch (without using any external packages) to predict the `Survived` outcome in the a synthetic Titanic dataset. TThe weak learners will be decision stumps (single-level decision trees), and I’ll evaluate the model’s performance using accuracy, precision, recall, and F1-score on the training data. . Below is the complete R code, including data generation, AdaBoost implementation, model fitting, and evaluation, followed by an explanation of the key components.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP2lCzFHBUip"
      },
      "source": [
        "### Data Generation\n",
        "\n",
        "   -First we will create  a synthetic Titanic dataset with 1000 samples and features: `Pclass`, `Sex`, `Age`, `SibSp`, `Parch`, `Fare`, `Embarked`, and the binary response `Survived` (0 or 1).\n",
        "   \n",
        "   - Categorical variables (`Sex`, `Embarked`, `Pclass`) are converted to factors for proper handling in the decision stump function.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTjVIV5ZBW96",
        "outputId": "7797b78b-e56f-47b3-9388-dcda4c85b3ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Pclass    Sex  Age SibSp Parch  Fare Embarked Survived\n",
            "1      3   male 20.0     0     0 19.82        Q        1\n",
            "2      2   male 19.6     1     4 23.85        C        0\n",
            "3      3   male 29.8     0     0 34.27        S        0\n",
            "4      1 female 28.7     1     0 44.96        S        1\n",
            "5      1 female  4.5     0     0  5.36        Q        0\n",
            "6      3   male 40.4     0     0 42.41        S        0\n"
          ]
        }
      ],
      "source": [
        "%%R\n",
        "# Set seed for reproducibility\n",
        "set.seed(123)\n",
        "\n",
        "# Generate synthetic Titanic dataset\n",
        "n <- 1000  # Number of samples\n",
        "\n",
        "# Generate features\n",
        "Pclass <- sample(1:3, n, replace = TRUE, prob = c(0.2, 0.3, 0.5))\n",
        "Sex <- sample(c(\"male\", \"female\"), n, replace = TRUE, prob = c(0.6, 0.4))\n",
        "Age <- round(rnorm(n, mean = 30, sd = 10), 1)\n",
        "Age[Age < 1] <- 1  # Ensure age is positive\n",
        "SibSp <- sample(0:5, n, replace = TRUE, prob = c(0.7, 0.1, 0.1, 0.05, 0.03, 0.02))\n",
        "Parch <- sample(0:4, n, replace = TRUE, prob = c(0.8, 0.1, 0.05, 0.03, 0.02))\n",
        "Fare <- round(rgamma(n, shape = 2, scale = 15), 2)\n",
        "Embarked <- sample(c(\"C\", \"Q\", \"S\"), n, replace = TRUE, prob = c(0.3, 0.2, 0.5))\n",
        "\n",
        "# Generate survival probabilities based on features\n",
        "surv_prob <- plogis(\n",
        "  0.5 * (Sex == \"female\") +\n",
        "  0.3 * (Pclass == 1) - 0.4 * (Pclass == 3) +\n",
        "  0.02 * (Age - 30) -\n",
        "  0.05 * SibSp - 0.02 * Parch +\n",
        "  0.001 * Fare +\n",
        "  0.2 * (Embarked == \"C\") - 0.1 * (Embarked == \"Q\")\n",
        ")\n",
        "Survived <- rbinom(n, 1, surv_prob)\n",
        "\n",
        "# Combine into data frame\n",
        "titanic <- data.frame(\n",
        "  Pclass, Sex, Age, SibSp, Parch, Fare, Embarked, Survived\n",
        ")\n",
        "# Convert categorical variables to factors\n",
        "titanic$Sex <- as.factor(titanic$Sex)\n",
        "titanic$Embarked <- as.factor(titanic$Embarked)\n",
        "titanic$Pclass <- as.factor(titanic$Pclass)\n",
        "head(titanic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwIhiSZNCO9N"
      },
      "source": [
        "### Preprocess Data\n",
        "\n",
        "Convert categorical features to numeric:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYPiX59GCiTY"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "preprocess_data <- function(df) {\n",
        "  df$Sex <- as.numeric(factor(df$Sex)) - 1  # male=0, female=1\n",
        "  df$Embarked <- as.numeric(factor(df$Embarked)) - 1  # C=0, Q=1, S=2\n",
        "  return(as.matrix(df))\n",
        "}\n",
        "\n",
        "X <- preprocess_data(titanic[, names(titanic) != \"Survived\"])\n",
        "y <- titanic$Survived"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aFvX9PX6t7N"
      },
      "source": [
        "\n",
        "### Decision Stump Function (`decision_stump`)\n",
        "\n",
        "   - A decision stump is a weak learner that splits data based on a single feature.\n",
        "   \n",
        "   - For **numeric features** (`Age`, `SibSp`, `Parch`, `Fare`), it tries all unique values as thresholds and predicts +1 (survived) or -1 (not survived) based on whether the feature value is less than or equal to the threshold.\n",
        "   \n",
        "   - For **categorical features** (`Pclass`, `Sex`, `Embarked`), it tries each category level and predicts +1 if the feature equals the level, else -1.\n",
        "   \n",
        "   - The **polarity** adjusts the prediction direction if the error exceeds 0.5 (flipping the prediction reduces the error).\n",
        "   \n",
        "   - The stump with the lowest weighted error is selected."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "\n",
        "# Function to create a decision stump (weak learner)\n",
        "decision_stump <- function(data, weights, predictors, response) {\n",
        "  n <- nrow(data)\n",
        "  best_error <- Inf\n",
        "  best_feature <- NULL\n",
        "  best_threshold <- NULL\n",
        "  best_split_value <- NULL\n",
        "  best_polarity <- 1\n",
        "\n",
        "  # Iterate over predictors\n",
        "  for (feature in predictors) {\n",
        "    values <- data[[feature]]\n",
        "\n",
        "    if (is.numeric(values)) {\n",
        "      # For numeric features, try thresholds\n",
        "      thresholds <- sort(unique(values))\n",
        "      thresholds <- thresholds[-length(thresholds)]  # Exclude max value\n",
        "      for (threshold in thresholds) {\n",
        "        # Predict +1 if feature <= threshold\n",
        "        pred <- ifelse(values <= threshold, 1, -1)\n",
        "        error <- sum(weights * (pred != data[[response]]))\n",
        "        if (error > 0.5) {\n",
        "          error <- 1 - error\n",
        "          polarity <- -1\n",
        "        } else {\n",
        "          polarity <- 1\n",
        "        }\n",
        "        if (error < best_error) {\n",
        "          best_error <- error\n",
        "          best_feature <- feature\n",
        "          best_threshold <- threshold\n",
        "          best_polarity <- polarity\n",
        "        }\n",
        "      }\n",
        "    } else {\n",
        "      # For categorical features, try each level\n",
        "      levels <- unique(values)\n",
        "      for (level in levels) {\n",
        "        # Predict +1 if feature == level\n",
        "        pred <- ifelse(values == level, 1, -1)\n",
        "        error <- sum(weights * (pred != data[[response]]))\n",
        "        if (error > 0.5) {\n",
        "          error <- 1 - error\n",
        "          polarity <- -1\n",
        "        } else {\n",
        "          polarity <- 1\n",
        "        }\n",
        "        if (error < best_error) {\n",
        "          best_error <- error\n",
        "          best_feature <- feature\n",
        "          best_split_value <- level\n",
        "          best_polarity <- polarity\n",
        "          best_threshold <- NULL\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  # Return the best stump\n",
        "  list(\n",
        "    feature = best_feature,\n",
        "    threshold = best_threshold,\n",
        "    split_value = best_split_value,\n",
        "    polarity = best_polarity,\n",
        "    error = best_error\n",
        "  )\n",
        "}"
      ],
      "metadata": {
        "id": "UMkiS7b-Droi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict Stump Function (`predict_stump`)\n",
        "\n",
        "   - Applies the decision stump to the data, returning predictions (+1 or -1) based on the feature, threshold (for numeric), or split value (for categorical), and polarity."
      ],
      "metadata": {
        "id": "gpz0NZAADtRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Function to predict with a decision stump\n",
        "predict_stump <- function(stump, data) {\n",
        "  feature <- stump$feature\n",
        "  values <- data[[feature]]\n",
        "\n",
        "  if (is.null(stump$threshold)) {\n",
        "    # Categorical feature\n",
        "    pred <- ifelse(values == stump$split_value, 1, -1)\n",
        "  } else {\n",
        "    # Numeric feature\n",
        "    pred <- ifelse(values <= stump$threshold, 1, -1)\n",
        "  }\n",
        "  pred * stump$polarity\n",
        "}"
      ],
      "metadata": {
        "id": "Bx9doXYoDv33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AdaBoost Function (`adaboost`)\n",
        "\n",
        "   - Initializes sample weights as $\\frac{1}{N}$).\n",
        "   \n",
        "   - For $T = 50$ iterations:\n",
        "   \n",
        "  - Trains a decision stump using current weights.\n",
        "     \n",
        "  - Computes the weighted error $\\epsilon_t$.\n",
        "     \n",
        "  - Calculates the stump’s weight $\\alpha_t = \\frac{1}{2} \\ln \\left( \\frac{1 - \\epsilon_t}{\\epsilon_t} \\right)$.\n",
        "     \n",
        "  - Updates sample weights: $w_i \\leftarrow w_i \\cdot \\exp(-\\alpha_t \\cdot y_i \\cdot h_t(x_i))$, then normalizes them.\n",
        "     \n",
        "  - Stores the stump and its $\\alpha_t$.\n",
        "     \n",
        "  - Stops early if the error is too high $\\epsilon_t \\geq 0.5$.\n"
      ],
      "metadata": {
        "id": "lfCVvlWXD4M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# AdaBoost algorithm\n",
        "adaboost <- function(data, predictors, response, T = 10) {\n",
        "  n <- nrow(data)\n",
        "  # Initialize weights\n",
        "  weights <- rep(1/n, n)\n",
        "  stumps <- list()\n",
        "  alphas <- numeric(T)\n",
        "\n",
        "  # Convert response to {-1, +1}\n",
        "  y <- ifelse(data[[response]] == 1, 1, -1)\n",
        "\n",
        "  for (t in 1:T) {\n",
        "    # Train a decision stump\n",
        "    stump <- decision_stump(data, weights, predictors, response)\n",
        "\n",
        "    # Compute predictions\n",
        "    pred <- predict_stump(stump, data)\n",
        "\n",
        "    # Compute weighted error\n",
        "    error <- stump$error\n",
        "    if (error == 0) error <- 1e-10  # Avoid division by zero\n",
        "    if (error >= 0.5) break  # Stop if error is too high\n",
        "\n",
        "    # Compute alpha (stump weight)\n",
        "    alpha <- 0.5 * log((1 - error) / error)\n",
        "\n",
        "    # Update weights\n",
        "    weights <- weights * exp(-alpha * y * pred)\n",
        "    weights <- weights / sum(weights)  # Normalize\n",
        "\n",
        "    # Store stump and alpha\n",
        "    stumps[[t]] <- stump\n",
        "    alphas[t] <- alpha\n",
        "  }\n",
        "\n",
        "  list(stumps = stumps, alphas = alphas[1:length(stumps)])\n",
        "}"
      ],
      "metadata": {
        "id": "azlfCVpkD-rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict AdaBoost Function (`predict_adaboost`)\n",
        "\n",
        "   - Combines predictions from all stumps, weighted by their $\\alpha_t$).\n",
        "   \n",
        "   - The final prediction is $\\text{sign} \\left( \\sum \\alpha_t h_t(x) \\right)$, mapped to 0 or 1."
      ],
      "metadata": {
        "id": "z01FdgBWEBzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Function to predict with AdaBoost model\n",
        "predict_adaboost <- function(model, data) {\n",
        "  n <- nrow(data)\n",
        "  final_pred <- numeric(n)\n",
        "\n",
        "  for (t in 1:length(model$stumps)) {\n",
        "    pred <- predict_stump(model$stumps[[t]], data)\n",
        "    final_pred <- final_pred + model$alphas[t] * pred\n",
        "  }\n",
        "\n",
        "  # Return class labels (0 or 1)\n",
        "  ifelse(final_pred >= 0, 1, 0)\n",
        "}"
      ],
      "metadata": {
        "id": "9ESlrzn-ECi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Fitting and Evaluation\n",
        "\n",
        "   - Fit the AdaBoost model using the synthetic Titanic dataset.\n",
        "   \n",
        "   - Make predictions on the training data.\n",
        "   \n",
        "   - computes a confusion matrix comparing predicted and actual `Survived` values.\n",
        "   - Calculates:\n",
        "     - `Accuracy`: Proportion of correct predictions.\n",
        "     - `Precision`: True positives / (True positives + False positives).\n",
        "     - `Recall`: True positives / (True positives + False negatives).\n",
        "     - `F1-Score`: Harmonic mean of precision and recall.\n",
        "     \n"
      ],
      "metadata": {
        "id": "LYrovqRLEIZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Fit AdaBoost model\n",
        "predictors <- c(\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\")\n",
        "model <- adaboost(titanic, predictors, \"Survived\", T = 50)\n",
        "\n",
        "# Make predictions\n",
        "predictions <- predict_adaboost(model, titanic)\n",
        "\n",
        "# Evaluate model\n",
        "actual <- titanic$Survived\n",
        "conf_matrix <- table(Predicted = predictions, Actual = actual)\n",
        "accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)\n",
        "precision <- conf_matrix[2,2] / sum(conf_matrix[2,])\n",
        "recall <- conf_matrix[2,2] / sum(conf_matrix[,2])\n",
        "f1_score <- 2 * precision * recall / (precision + recall)\n",
        "\n",
        "# Print results\n",
        "cat(\"Confusion Matrix:\\n\")\n",
        "print(conf_matrix)\n",
        "cat(sprintf(\"Accuracy: %.4f\\n\", accuracy))\n",
        "cat(sprintf(\"Precision: %.4f\\n\", precision))\n",
        "cat(sprintf(\"Recall: %.4f\\n\", recall))\n",
        "cat(sprintf(\"F1-Score: %.4f\\n\", f1_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNLTXzHMEJIs",
        "outputId": "012cc3cc-7c27-4756-8993-caf6ce11086c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "         Actual\n",
            "Predicted   0   1\n",
            "        0 269 238\n",
            "        1 222 271\n",
            "Accuracy: 0.5400\n",
            "Precision: 0.5497\n",
            "Recall: 0.5324\n",
            "F1-Score: 0.5409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5Z1SpIUbgLy"
      },
      "source": [
        "## AdaBoost with R Packages\n",
        "\n",
        "{adabag} is a popular R package for implementing AdaBoost and other boosting algorithms. It provides a user-friendly interface and efficient implementations of various boosting methods, including AdaBoost with decision trees as weak learners. Below is an example of how to use the {adabag} package to perform AdaBoost on the Titanic dataset.\n",
        "\n",
        "We’ll fit and evaluate an AdaBoost model using the `adabag` package in R with the Titanic dataset from the `titanic` package, following the provided data loading and processing code. The dataset will be preprocessed as specified, split into training (80%) and test (20%) sets, and the model will be evaluated using accuracy, precision, recall, and F1-score on the test set. Below is the complete R code, followed by an explanation of the steps and results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXu-XY0mw1A8"
      },
      "source": [
        "### Check amd Install Required R Packages\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeYB57l0wz5N"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "packages <- c('tidyverse',\n",
        "              'plyr',\n",
        "              'adabag',\n",
        "              'caret',\n",
        "              'titanic',\n",
        "              'pROC',\n",
        "              'fastDummies',\n",
        "              'ggpmisc',\n",
        "              'Metrics'\n",
        "\n",
        "         )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7NH-51RYeIk"
      },
      "source": [
        "### Install Missing Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0aTMYTHAraZ"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "# Install missing packages\n",
        "new.packages <- packages[!(packages %in% installed.packages(lib='drive/My Drive/R/')[,\"Package\"])]\n",
        "if(length(new.packages)) install.packages(new.packages, lib='drive/My Drive/R/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ah43eXWcYfpA"
      },
      "source": [
        "### Verify Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9hpg7qnArfh",
        "outputId": "616f7f5c-857e-4750-e1b9-a295afe99d30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installed packages:\n",
            "  tidyverse        plyr      adabag       caret     titanic        pROC \n",
            "       TRUE        TRUE        TRUE        TRUE        TRUE        TRUE \n",
            "fastDummies     ggpmisc     Metrics \n",
            "       TRUE        TRUE        TRUE \n"
          ]
        }
      ],
      "source": [
        "%%R\n",
        "# set library path\n",
        ".libPaths('drive/My Drive/R')\n",
        "# Verify installation\n",
        "cat(\"Installed packages:\\n\")\n",
        "print(sapply(packages, requireNamespace, quietly = TRUE))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load R Packages"
      ],
      "metadata": {
        "id": "PA5QgksfIYAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# set library path\n",
        ".libPaths('drive/My Drive/R')\n",
        "# Load packages with suppressed messages\n",
        "invisible(lapply(packages, function(pkg) {\n",
        "  suppressPackageStartupMessages(library(pkg, character.only = TRUE))\n",
        "}))"
      ],
      "metadata": {
        "id": "3N6xkAH4IYO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Loaded Packages"
      ],
      "metadata": {
        "id": "LjV_E6ZCIkpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Check loaded packages\n",
        "cat(\"Successfully loaded packages:\\n\")\n",
        "print(search()[grepl(\"package:\", search())])# Check loaded packageswer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIA11u2dIk0S",
        "outputId": "192de397-2d28-4a20-bf99-2695542cbe74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded packages:\n",
            " [1] \"package:Metrics\"     \"package:ggpmisc\"     \"package:ggpp\"       \n",
            " [4] \"package:fastDummies\" \"package:pROC\"        \"package:titanic\"    \n",
            " [7] \"package:adabag\"      \"package:doParallel\"  \"package:parallel\"   \n",
            "[10] \"package:iterators\"   \"package:foreach\"     \"package:caret\"      \n",
            "[13] \"package:lattice\"     \"package:rpart\"       \"package:plyr\"       \n",
            "[16] \"package:lubridate\"   \"package:forcats\"     \"package:stringr\"    \n",
            "[19] \"package:dplyr\"       \"package:purrr\"       \"package:readr\"      \n",
            "[22] \"package:tidyr\"       \"package:tibble\"      \"package:ggplot2\"    \n",
            "[25] \"package:tidyverse\"   \"package:tools\"       \"package:stats\"      \n",
            "[28] \"package:graphics\"    \"package:grDevices\"   \"package:utils\"      \n",
            "[31] \"package:datasets\"    \"package:methods\"     \"package:base\"       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and Prepare the Data"
      ],
      "metadata": {
        "id": "vtFBtYNxEUX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Load and prepare the Titanic dataset\n",
        "data <- titanic_train[, c(\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\")]\n",
        "\n",
        "# Clean missing values\n",
        "data$Age[is.na(data$Age)] <- median(data$Age, na.rm = TRUE)\n",
        "data$Embarked[is.na(data$Embarked)] <- \"S\"\n",
        "\n",
        "# Convert categorical columns to factors\n",
        "data$Pclass <- as.factor(data$Pclass)\n",
        "data$Sex <- as.factor(data$Sex)\n",
        "data$Embarked <- as.factor(data$Embarked)\n",
        "data$Survived <- as.factor(data$Survived)  # Ensure Survived is a factor\n",
        "\n",
        "\n",
        "# Split into training and test sets\n",
        "set.seed(123)\n",
        "train_index <- caret::createDataPartition(data$Survived, p = 0.8, list = FALSE)\n",
        "train_data <- data[train_index, ]\n",
        "test_data <- data[-train_index, ]\n",
        "\n",
        "# Separate features and labels\n",
        "X_train <- train_data[, -1]\n",
        "y_train <- as.factor(train_data$Survived)  # adabag requires factor response\n",
        "X_test <- test_data[, -1]\n",
        "y_test <- as.factor(test_data$Survived)"
      ],
      "metadata": {
        "id": "0Rx9_gQuEUsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Fitting the AdaBoost Model\n",
        "\n",
        "   - The `boosting()` function from the {adabag} package is used to fit the AdaBoost model.\n",
        "   \n",
        "   - Parameters:\n",
        "     - `mfinal = 50`: Uses 50 iterations (decision trees as weak learners).\n",
        "     - `boos = TRUE`: Enables bootstrap sampling for each iteration.\n",
        "     - `coeflearn = \"Breiman\"`: Uses Breiman’s method for calculating the weight ($\\alpha_t$) of each weak learner, consistent with the classic AdaBoost algorithm ($\\alpha_t = \\frac{1}{2} \\ln \\left( \\frac{1 - \\epsilon_t}{\\epsilon_t} \\right)$).\n",
        "   - The model is trained on the training data using all specified predictors."
      ],
      "metadata": {
        "id": "W_q4eufmEcMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Fit AdaBoost model using adabag\n",
        "adaboost_model <- boosting(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,\n",
        "                          data = train_data,\n",
        "                          mfinal = 50,  # Number of iterations (trees)\n",
        "                          boos = TRUE,  # Use bootstrap sampling\n",
        "                          coeflearn = \"Breiman\")  # Use Breiman's alpha calculation\n",
        "summary(adaboost_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Co3v1w4REc0-",
        "outputId": "cf3e9733-4368-49f6-e176-3489ea2332a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Length Class   Mode     \n",
            "formula       3   formula call     \n",
            "trees        50   -none-  list     \n",
            "weights      50   -none-  numeric  \n",
            "votes      1428   -none-  numeric  \n",
            "prob       1428   -none-  numeric  \n",
            "class       714   -none-  character\n",
            "importance    7   -none-  numeric  \n",
            "terms         3   terms   call     \n",
            "call          6   -none-  call     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making Predictions and Evaluating the Model\n",
        "\n",
        "   - Predictions are made on the test set using the `predict()` function.\n",
        "   \n",
        "   - The `predictions$class` field provides the predicted class labels (0 or 1).\n",
        "   \n",
        "   - A confusion matrix is computed to compare predicted and actual `Survived` values.\n",
        "   \n",
        "   - Metrics calculated:\n",
        "     - **Accuracy**: Proportion of correct predictions.\n",
        "     - **Precision**: True positives / (True positives + False positives).\n",
        "     - **Recall**: True positives / (True positives + False negatives).\n",
        "     - **F1-Score**: Harmonic mean of precision and recall."
      ],
      "metadata": {
        "id": "82gVvlyTEiEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Predict on test set\n",
        "predictions <- predict(adaboost_model, newdata = X_test)\n",
        "\n",
        "# Extract predicted class labels\n",
        "pred_classes <- as.factor(predictions$class)\n",
        "\n",
        "# Evaluate model\n",
        "conf_matrix <- table(Predicted = pred_classes, Actual = y_test)\n",
        "accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)\n",
        "precision <- conf_matrix[2,2] / sum(conf_matrix[2,])\n",
        "recall <- conf_matrix[2,2] / sum(conf_matrix[,2])\n",
        "f1_score <- 2 * precision * recall / (precision + recall)\n",
        "\n",
        "# Print results\n",
        "cat(\"Confusion Matrix:\\n\")\n",
        "print(conf_matrix)\n",
        "cat(sprintf(\"Accuracy: %.4f\\n\", accuracy))\n",
        "cat(sprintf(\"Precision: %.4f\\n\", precision))\n",
        "cat(sprintf(\"Recall: %.4f\\n\", recall))\n",
        "cat(sprintf(\"F1-Score: %.4f\\n\", f1_score))\n",
        "\n",
        "# Variable importance\n",
        "cat(\"\\nVariable Importance:\\n\")\n",
        "print(adaboost_model$importance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0z8wdm4WEiv0",
        "outputId": "a7818be9-5521-4333-edb4-83dafa29d3bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "         Actual\n",
            "Predicted  0  1\n",
            "        0 94 19\n",
            "        1 15 49\n",
            "Accuracy: 0.8079\n",
            "Precision: 0.7656\n",
            "Recall: 0.7206\n",
            "F1-Score: 0.7424\n",
            "\n",
            "Variable Importance:\n",
            "      Age  Embarked      Fare     Parch    Pclass       Sex     SibSp \n",
            "32.674180  3.222505 33.496485  3.148909  7.966494 14.587410  4.904018 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variable Importance\n",
        "\n",
        "   - The `adabag_model$importance` provides the relative importance of each predictor, based on how much each feature reduces the weighted error across iterations."
      ],
      "metadata": {
        "id": "guLHUMaWEnrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Extract variable importance and create a data frame\n",
        "var_importance <- data.frame(\n",
        "  Predictor = names(adaboost_model$importance),\n",
        "  Importance = as.numeric(adaboost_model$importance)\n",
        ")\n",
        "# Order predictors by importance (descending)\n",
        "var_importance <- var_importance[order(var_importance$Importance, decreasing = TRUE), ]\n",
        "var_importance$Predictor <- factor(var_importance$Predictor, levels = var_importance$Predictor)\n",
        "\n",
        "# Plot variable importance with ggplot2\n",
        "ggplot(var_importance, aes(x = Predictor, y = Importance)) +\n",
        "  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n",
        "  coord_flip() +  # Horizontal bars\n",
        "  labs(title = \"Variable Importance in AdaBoost Model\",\n",
        "       x = \"Predictors\",\n",
        "       y = \"Relative Importance (%)\") +\n",
        "  theme_minimal() +\n",
        "  theme(axis.text.y = element_text(size = 10),\n",
        "        axis.text.x = element_text(size = 10),\n",
        "        axis.title = element_text(size = 12),\n",
        "        plot.title = element_text(size = 14, hjust = 0.5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "oWPw-wbWEqjF",
        "outputId": "94a1286c-edd0-4e96-8bef-98798c3206d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-validation"
      ],
      "metadata": {
        "id": "b5c0FtqgEvfV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Perform 5-fold cross-validation with `boosting.cv()`"
      ],
      "metadata": {
        "id": "DoLuhLRREzmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Perform 5-fold cross-validation with boosting.cv\n",
        "set.seed(123)\n",
        "cv_results <- boosting.cv(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,\n",
        "                         data = data,\n",
        "                         v = 5,\n",
        "                         mfinal = 50,\n",
        "                         boos = TRUE,\n",
        "                         coeflearn = \"Breiman\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJtj-TVyEwPb",
        "outputId": "1a5c1253-ec63-42e7-e0a8-701c413c629b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i:  1 Mon Jul  7 18:26:45 2025 \n",
            "i:  2 Mon Jul  7 18:26:50 2025 \n",
            "i:  3 Mon Jul  7 18:26:56 2025 \n",
            "i:  4 Mon Jul  7 18:27:00 2025 \n",
            "i:  5 Mon Jul  7 18:27:05 2025 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extract predictions and actual labels"
      ],
      "metadata": {
        "id": "nNfI7w60E5B6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Extract predictions and actual labels\n",
        "predictions <- cv_results$class\n",
        "actual <- data$Survived\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix <- table(Predicted = predictions, Actual = actual)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)\n",
        "precision <- conf_matrix[2,2] / sum(conf_matrix[2,])\n",
        "recall <- conf_matrix[2,2] / sum(conf_matrix[,2])\n",
        "f1_score <- 2 * precision * recall / (precision + recall)\n",
        "\n",
        "# Print results\n",
        "cat(\"Confusion Matrix (Aggregated Across Folds):\\n\")\n",
        "print(conf_matrix)\n",
        "cat(sprintf(\"Accuracy: %.4f\\n\", accuracy))\n",
        "cat(sprintf(\"Precision: %.4f\\n\", precision))\n",
        "cat(sprintf(\"Recall: %.4f\\n\", recall))\n",
        "cat(sprintf(\"F1-Score: %.4f\\n\", f1_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuHwN3AcE7F1",
        "outputId": "9c650977-e264-41f8-ffb9-fed778c3fe45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix (Aggregated Across Folds):\n",
            "         Actual\n",
            "Predicted   0   1\n",
            "        0 478  94\n",
            "        1  71 248\n",
            "Accuracy: 0.8148\n",
            "Precision: 0.7774\n",
            "Recall: 0.7251\n",
            "F1-Score: 0.7504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb0kdVIuGlK7"
      },
      "source": [
        "\n",
        "## Summary and Conclusion\n",
        "\n",
        "\n",
        "AdaBoost is a robust and effective algorithm for classification tasks like predicting survival in the Titanic dataset, offering high accuracy and interpretability through feature importance. However, its sensitivity to noise, computational cost, and limitations with imbalanced data or probabilistic outputs require careful consideration. Thsi tutorial demonstrated how to implement AdaBoost from scratch and using the {adabag} package in R, providing a comprehensive understanding of its mechanics and practical applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACNQyq1seLc7"
      },
      "source": [
        "## References\n",
        "\n",
        "\n",
        "1. **Freund, Y., & Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1), 119–139.**\n",
        "   - Introduces the original AdaBoost algorithm, detailing its theoretical foundation and iterative weighting mechanism for combining weak learners.\n",
        "\n",
        "2. **Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning (2nd ed.). Springer.**\n",
        "   - Chapter 10 provides a comprehensive overview of boosting, including AdaBoost’s mechanics, strengths, and limitations in classification tasks.\n",
        "\n",
        "3. **Carrasco-Ochoa, J. A., et al. (Eds.). (2011). Machine Learning with R: The adabag Package. CRAN Repository.**\n",
        "   - Documentation for the `adabag` R package, explaining the `boosting` and `boosting.cv` functions used for AdaBoost.M1 and cross-validation, as applied in the Titanic dataset."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO60bWs6clfTcrHIR+VpXMr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}