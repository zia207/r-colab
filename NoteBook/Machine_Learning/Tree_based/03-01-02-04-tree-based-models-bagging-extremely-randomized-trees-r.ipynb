{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMAyTBQANcJFo2a4wQQGoqs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-04-tree-based-models-bagging-extremely-randomized-trees-r.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![alt text](http://drive.google.com/uc?export=view&id=1bLQ3nhDbZrCCqy_WCxxckOne2lgVvn3l)"
      ],
      "metadata": {
        "id": "zYZbTX0qQrZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.4 Extremely Randomized Trees (Extra Trees)\n",
        "\n",
        "Extremely Randomized Trees (Extra Trees) is a machine learning ensemble method that builds multiple decision trees and combines their predictions to improve accuracy and robustness. It is an extension of the Random Forest algorithm, introduced by Pierre Geurts et al. in 2006, and is designed to further reduce variance and computational complexity by introducing additional randomization."
      ],
      "metadata": {
        "id": "RGBfrL2GQoII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "Extra Trees is a powerful ensemble learning method that builds multiple decision trees and combines their predictions. It is particularly effective for high-dimensional datasets and can handle both classification and regression tasks. The key idea behind Extra Trees is to introduce more randomness into the tree-building process, which helps to reduce overfitting and improve generalization.\n",
        "\n",
        "Key Features:\n",
        "\n",
        "1. **Random Feature Selection**: Picks random subset of features at each node.\n",
        "2. **Random Split Thresholds**: Chooses split points randomly, not optimally.\n",
        "3. **Ensemble Aggregation**: Combines tree predictions (voting for classification, averaging for regression).\n",
        "4. **No Bootstrap (Optional)**: Uses full dataset per tree, unlike Random Forest.\n",
        "5. **Low Variance**: High randomization reduces tree correlation.\n",
        "6. **Fast Training**: Random splits make it quicker than Random Forest.\n",
        "7. **Noise Robust**: Handles noisy data well.\n",
        "\n",
        "**Pros**: Fast, robust, less overfitting.  \n",
        "**Cons**: Slightly less accurate than Random Forest, less interpretable.  \n",
        "**Uses**: Classification, regression on noisy or high-dimensional data.\n"
      ],
      "metadata": {
        "id": "7ZqlGV65Qme0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How Extra Trees Works\n",
        "\n",
        "Extra Trees builds an ensemble of decision trees using a highly randomized approach. The key steps in the Extra Trees algorithm are as follows:\n",
        "\n",
        "1. Ensemble of Decision Trees\n",
        "\n",
        "-   Extra Trees builds a collection of decision trees, where each tree is trained on a subset of the data (or the entire dataset, depending on configuration).\n",
        "\n",
        "-   The final prediction is made by aggregating the outputs of all trees:\n",
        "\n",
        "  -   `Classification`: Majority voting across all trees.\n",
        "\n",
        "  -   `Regression`: Averaging the predictions of all trees.\n",
        "\n",
        "    -   \n",
        "\n",
        "   Lets denote:\n",
        "\n",
        "  -   $T$: Number of trees in the ensemble.\n",
        "\n",
        "  -   $D$: Training dataset with $n$ samples and $m$ features, $D = \\{(x_i, y_i)\\}_{i=1}^n$, where $x_i$ is a feature vector and $y$ is the target (class label for classification or continuous value for regression).\n",
        "\n",
        "   -   Each tree $t \\in \\{1, 2, \\dots, T\\}$ produces a prediction \\$h_t(x)% for an input ( x ).\n",
        "\n",
        "-   For **classification**:\n",
        "\n",
        "   -   The final prediction is the majority vote:\n",
        "\n",
        "$$ \\hat{y}(x) = \\text{mode} \\{ h_1(x), h_2(x), \\dots, h_T(x) \\} $$\n",
        "\n",
        "where \\#\\text{mode}\\$ selects the most frequent class across all trees.\n",
        "\n",
        "-   For `regression`:\n",
        "\n",
        "   -   The final prediction is the average:\n",
        "\n",
        "$$ \\hat{y}(x) = \\frac{1}{T} \\sum_{t=1}^T h_t(x) $$\n",
        "\n",
        "where $h_t(x)$ is the output of the ( t )-th tree.\n",
        "\n",
        "-   The ensemble reduces variance by combining predictions from multiple trees, which are intentionally diversified through randomization (explained in later steps).\n",
        "\n",
        "2. Randomized Feature Selection\n",
        "\n",
        "-   At each node of a decision tree, Extra Trees selects a random subset of features (similar to Random Forests).\n",
        "\n",
        "-   Unlike Random Forests, which evaluate all possible splits for these features to find the best one (e.g., based on Gini impurity or information gain), Extra Trees picks a split randomly without optimizing.\n",
        "\n",
        "-   At each node:\n",
        "\n",
        "-   Select a random subset of $k$ features from the total $m$ features (typically $k = \\sqrt{m}$ for classification or $k = m/3$ for regression, though this can be tuned).\n",
        "\n",
        "-   For each feature in this subset, a random split point is chosen (see Step 3).\n",
        "    \n",
        "-   Let $F = \\{f_1, f_2, \\dots, f_m\\}$ be the set of all features.\n",
        "\n",
        "-   A random subset $F_{\\text{node}} \\subseteq F$ is chosen, where $|F_{\\text{node}}| = k$.\n",
        "\n",
        "-   No optimization is performed (unlike Random Forests, which minimize a criterion like Gini impurity):\n",
        "\n",
        "$$ \\text{Gini}(S) = 1 - \\sum_{c=1}^C p_c^2 $$\n",
        "\n",
        "or\n",
        "\n",
        "$$ \\text{Information Gain}(S, f) = H(S) - \\sum_{v \\in \\text{values}(f)} \\frac{|S_v|}{|S|} H(S_v) $$\n",
        "\n",
        "where $H(S)$ is the entropy of set $S$, and $S_v$ is the subset of samples where feature $f$ takes value $v$. Extra Trees skips these calculations.\n",
        "\n",
        "-   By avoiding the search for the optimal split, Extra Trees reduces computational cost, making it faster than Random Forests, especially for large datasets.\n",
        "\n",
        "3. Randomized Splitting\n",
        "\n",
        "-   For each feature in the random subset $F_{\\text{node}}$, Extra Trees generates a random threshold within the feature’s range.\n",
        "\n",
        "-   The split is chosen randomly among these thresholds without evaluating their quality (e.g., no comparison of Gini impurity or variance reduction).\n",
        "\n",
        "-   For a feature $f_j \\in F_{\\text{node}}$:\n",
        "\n",
        "  -   Compute the range of values for $f_j$ in the current node’s data: $[\\text{min}(f_j), \\text{max}(f_j)]$.\n",
        "\n",
        "  -   Select a random threshold $\\theta_j$) uniformly from this range:\n",
        "\n",
        "$$ \\theta_j \\sim \\text{Uniform}(\\text{min}(f_j), \\text{max}(f_j)) $$\n",
        "\n",
        "-   The node splits the data into two subsets:\n",
        "\n",
        "  -   Left: $\\{ x \\in S \\mid f_j(x) \\leq \\theta_j \\}$\n",
        "\n",
        "   -   Right: $\\{ x \\in S \\mid f_j(x) > \\theta_j \\}$\n",
        "\n",
        "    -   Among the $k$ features in $F_{\\text{node}}$, Extra Trees randomly selects one of the proposed splits (or sometimes uses a simple heuristic, like picking the first one, depending on the implementation).\n",
        "\n",
        "\\`- The split condition for a node is:\n",
        "\n",
        "$$ \\text{Split: } f_j(x) \\leq \\theta_j $$\n",
        "\n",
        "-   No optimization criterion is computed, unlike Random Forests, which would evaluate:\n",
        "\n",
        "$$  \\text{Best split} = \\arg\\min_{f_j, \\theta_j} \\text{Criterion}(S_{\\text{left}}, S_{\\text{right}}) $$\n",
        "\n",
        "where the criterion could be Gini impurity, entropy, or variance.\n",
        "\n",
        "-   The random threshold selection introduces extra randomness, which helps decorrelate the trees further and reduces overfitting while maintaining predictive power.\n",
        "\n",
        "4. Bootstrap Sampling (Optional)\n",
        "\n",
        "-   Extra Trees can use bootstrap sampling (random sampling with replacement) to create different training subsets for each tree, similar to Random Forests.\n",
        "\n",
        "-   Alternatively, Extra Trees often uses the entire training dataset for each tree (no bootstrapping), which is a key distinction from Random Forests and reduces variance further.\n",
        "\n",
        "-   If bootstrap sampling is enabled:\n",
        "\n",
        "  -   For each tree $t$, sample $n$ instances from $D$) with replacement to create a subset $D_t$.\n",
        "\n",
        "  -   The probability that a sample $(x_i, y_i)$ is included in $D_t$ is:\n",
        "\n",
        "$$  P(\\text{included}) = 1 - \\left(1 - \\frac{1}{n}\\right)^n \\approx 1 - e^{-1} \\approx 0.632 $$\n",
        "\n",
        "-   Thus, about 63.2% of the data is used per tree, with some samples repeated.\n",
        "\n",
        "-   If bootstrap sampling is disabled (common in Extra Trees):\n",
        "\n",
        "    -   Each tree is trained on the full dataset $D$, i.e., $D_t = D$.\n",
        "\n",
        "-   Using the full dataset (no bootstrapping) reduces variance because each tree sees all the data, but the randomization in feature selection and splitting ensures diversity among trees.\n",
        "\n",
        "5. Aggregation\n",
        "\n",
        "-   Once all $T$ trees are built, their predictions are combined to produce the final output.\n",
        "\n",
        "-   This step is identical to Random Forests but relies on the highly randomized trees built in the previous steps.\n",
        "\n",
        "-   For a new input $x$:\n",
        "\n",
        "   -   Each tree $t$ produces a prediction $h_t(x)$.\n",
        "\n",
        "   -   The final prediction is computed as:\n",
        "\n",
        "- `Classification`:\n",
        "\n",
        "$$ \\hat{y}(x) = \\text{mode} \\{ h_1(x), h_2(x), \\dots, h_T(x) \\} $$\n",
        "\n",
        "where the mode is the most frequent class label.\n",
        "\n",
        "         \n",
        "- `Regression`:\n",
        "\n",
        "\n",
        "$$  \\hat{y}(x) = \\frac{1}{T} \\sum_{t=1}^T h_t(x) $$\n",
        "\n",
        "where the average of all tree predictions is taken.\n",
        "\n",
        "-   The aggregation leverages the law of large numbers to produce a robust prediction, as the errors of individual trees (due to randomness) tend to cancel out.\n",
        "\n"
      ],
      "metadata": {
        "id": "YHSyBY0O3Tj8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a flowchart summarizing the Extra Trees algorithm:\n",
        "\n",
        "![alt text](http://drive.google.com/uc?export=view&id=1AOZtY5BD50W1qRERKfG-OBRakgl6GZqd)\n"
      ],
      "metadata": {
        "id": "MfeqJ2MyFVfU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Differences from Random Forests\n",
        "\n",
        "-   `More Randomness`: Extra Trees randomizes both feature splits and threshold selection, while Random Forests optimize splits based on a criterion (e.g., Gini or entropy).\n",
        "-   `Faster Training`: Random split selection avoids computationally expensive optimization, making Extra Trees faster to train.\n",
        "-   `Bias-Variance Tradeoff`: Extra Trees increases bias slightly (due to random splits) but reduces variance, which can lead to better generalization on noisy datasets.\n",
        "-   `Overfitting`: Extra Trees is less prone to overfitting than Random Forests, especially when the dataset is small or noisy."
      ],
      "metadata": {
        "id": "5kbEVEgICPd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advantages\n",
        "\n",
        "-   `Speed`: Faster training due to random split selection.\n",
        "-   `Robustness`: Handles noisy data well and reduces overfitting.\n",
        "-   `Simplicity`: Fewer hyperparameters to tune compared to other ensemble methods.\n",
        "-   `Versatility`: Works for both classification and regression tasks."
      ],
      "metadata": {
        "id": "zoVtYA7e3_qr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Disadvantages\n",
        "\n",
        "-   `Less Interpretable`: Like Random Forests, the ensemble nature makes it harder to interpret individual trees.\n",
        "-   `Slightly Higher Bias`: Random splits may lead to less optimal individual trees compared to optimized splits in Random Forests.\n",
        "-   `Memory Usage`: Building many trees can be memory-intensive for large datasets."
      ],
      "metadata": {
        "id": "0BkKGa9k4BdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### When to Use Extra Trees\n",
        "\n",
        "-   When you need a fast, robust ensemble method for classification or regression.\n",
        "-   When dealing with noisy or high-dimensional datasets.\n",
        "-   When computational resources are limited, and training speed is a priority.\n",
        "-   As an alternative to Random Forests when overfitting is a concern."
      ],
      "metadata": {
        "id": "gg_wMc694I5n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup R in Python Runtype"
      ],
      "metadata": {
        "id": "Bn4w4oqMCagF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install {rpy2}\n",
        "\n",
        "{rpy2} is a Python package that provides an interface to the R programming language, allowing Python users to run R code, call R functions, and manipulate R objects directly from Python. It enables seamless integration between Python and R, leveraging R's statistical and graphical capabilities while using Python's flexibility. The package supports passing data between the two languages and is widely used for statistical analysis, data visualization, and machine learning tasks that benefit from R's specialized libraries."
      ],
      "metadata": {
        "id": "yerTCtKKCmik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall rpy2 -y\n",
        "!pip install rpy2==3.5.1\n",
        "%load_ext rpy2.ipython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqeCyNf0Crlc",
        "outputId": "19b1cb67-b9b7-4d95-b059-1e7315939f3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: rpy2 3.5.17\n",
            "Uninstalling rpy2-3.5.17:\n",
            "  Successfully uninstalled rpy2-3.5.17\n",
            "Collecting rpy2==3.5.1\n",
            "  Downloading rpy2-3.5.1.tar.gz (201 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.7/201.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from rpy2==3.5.1) (1.17.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from rpy2==3.5.1) (3.1.6)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from rpy2==3.5.1) (2025.2)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.11/dist-packages (from rpy2==3.5.1) (5.3.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.10.0->rpy2==3.5.1) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->rpy2==3.5.1) (3.0.2)\n",
            "Building wheels for collected packages: rpy2\n",
            "  Building wheel for rpy2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rpy2: filename=rpy2-3.5.1-cp311-cp311-linux_x86_64.whl size=314977 sha256=f8f240d6368b73bc994799169bea20c4dc1cb0c0bc60c1bc6b0bc334dfb68083\n",
            "  Stored in directory: /root/.cache/pip/wheels/e9/55/d1/47be85a5f3f1e1f4d1e91cb5e3a4dcb40dd72147f184c5a5ef\n",
            "Successfully built rpy2\n",
            "Installing collected packages: rpy2\n",
            "Successfully installed rpy2-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Google Drive"
      ],
      "metadata": {
        "id": "vOzGUgYCCyqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lngQQKKC2lc",
        "outputId": "f5a6e9af-018c-4dfe-cb4f-938485383663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Trees from Scratch\n",
        "\n",
        "Extra Trees can be implemented from scratch in R without relying on external packages. Below is a complete implementation for both classification and regression tasks using the Iris and BostonHousing datasets, respectively.\n"
      ],
      "metadata": {
        "id": "xLU0BT_LCVBr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification\n",
        "\n",
        "This R code implements an **Extra Trees Classifier** on the Iris dataset using base R, without external packages. Here's a concise explanation:\n",
        "\n",
        "1. **Dataset**: Uses the Iris dataset (`iris`), with features (sepal/petal measurements) and target (`Species`).\n",
        "\n",
        "2. **Bootstrap Sampling**:\n",
        "   - `bootstrap_sample`: Creates a random sample with replacement from the dataset (used if `use_bootstrap = TRUE`).\n",
        "\n",
        "3. **Feature Selection**:\n",
        "   - `select_features`: Randomly selects `k` features (default: square root of total features) for each node.\n",
        "\n",
        "4. **Tree Building** (`build_tree`):\n",
        "   - Builds a single decision tree with:\n",
        "     - Random feature subset at each node.\n",
        "     - Random split thresholds (using `runif` for random value within feature range).\n",
        "     - Stops if max depth (`max_depth = 5`) or node size (`min_size = 5`) is reached.\n",
        "     - Returns a leaf node with majority class if stopping criteria are met or no valid split is found.\n",
        "     - Recursively builds left/right subtrees for valid splits.\n",
        "\n",
        "5. **Prediction**:\n",
        "   - `predict_tree`: Predicts class for a single row by traversing a tree based on feature thresholds.\n",
        "   - `predict_extra_trees`: Combines predictions from all trees using majority voting.\n",
        "\n",
        "6. **Main Function** (`extra_trees_classification`):\n",
        "   - Builds `n_trees` (default: 100) decision trees.\n",
        "   - Uses bootstrap sampling or full dataset based on `use_bootstrap`.\n",
        "\n",
        "7. **Training/Testing**:\n",
        "   - Splits Iris data into 70% training, 30% testing.\n",
        "   - Trains model with 100 trees and bootstrap sampling.\n",
        "   - Predicts on test set and computes accuracy.\n",
        "\n",
        "8. **Evaluation**:\n",
        "   - Outputs accuracy (proportion of correct predictions).\n",
        "   - Prints a confusion matrix comparing predicted vs. actual classes.\n",
        "\n",
        "**Key Extra Trees Traits**:\n",
        "- Random feature selection and split thresholds reduce variance and computation.\n",
        "- Ensemble of trees improves robustness.\n",
        "- Optional bootstrap sampling (here, enabled)."
      ],
      "metadata": {
        "id": "_W-exb0KKCFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Extra Trees Classification on Iris Dataset\n",
        "# No external packages, base R only\n",
        "\n",
        "# Load Iris dataset (assumed available in R environment)\n",
        "data(iris)\n",
        "iris_data <- iris\n",
        "\n",
        "# Function to create a bootstrap sample\n",
        "bootstrap_sample <- function(data) {\n",
        "  n <- nrow(data)\n",
        "  indices <- sample(1:n, n, replace = TRUE)\n",
        "  return(data[indices, ])\n",
        "}\n",
        "\n",
        "# Function to select random features\n",
        "select_features <- function(n_features, k) {\n",
        "  sample(1:n_features, k, replace = FALSE)\n",
        "}\n",
        "\n",
        "# Function to build a single decision tree\n",
        "build_tree <- function(data, max_depth = 5, min_size = 5) {\n",
        "  build_node <- function(data, depth) {\n",
        "    # Stop if max depth reached or node too small\n",
        "    if (depth >= max_depth || nrow(data) <= min_size) {\n",
        "      # Return leaf node with majority class\n",
        "      classes <- table(data$Species)\n",
        "      return(list(leaf = TRUE, prediction = names(which.max(classes))))\n",
        "    }\n",
        "\n",
        "    # Select random features (sqrt of total features)\n",
        "    n_features <- ncol(data) - 1  # Exclude Species column\n",
        "    k <- floor(sqrt(n_features))\n",
        "    features <- select_features(n_features, k)\n",
        "\n",
        "    # Find best random split\n",
        "    best_feature <- NULL\n",
        "    best_threshold <- NULL\n",
        "    best_split <- NULL\n",
        "\n",
        "    for (f in features) {\n",
        "      # Get random threshold within feature range\n",
        "      values <- data[[f]]\n",
        "      threshold <- runif(1, min(values), max(values))\n",
        "\n",
        "      # Split data\n",
        "      left <- data[data[[f]] <= threshold, ]\n",
        "      right <- data[data[[f]] > threshold, ]\n",
        "\n",
        "      # Ensure non-empty splits\n",
        "      if (nrow(left) > 0 && nrow(right) > 0) {\n",
        "        best_feature <- f\n",
        "        best_threshold <- threshold\n",
        "        best_split <- list(left = left, right = right)\n",
        "      }\n",
        "    }\n",
        "\n",
        "    # If no valid split, return leaf\n",
        "    if (is.null(best_feature)) {\n",
        "      classes <- table(data$Species)\n",
        "      return(list(leaf = TRUE, prediction = names(which.max(classes))))\n",
        "    }\n",
        "\n",
        "    # Recursively build left and right subtrees\n",
        "    left_tree <- build_node(best_split$left, depth + 1)\n",
        "    right_tree <- build_node(best_split$right, depth + 1)\n",
        "\n",
        "    return(list(\n",
        "      leaf = FALSE,\n",
        "      feature = best_feature,\n",
        "      threshold = best_threshold,\n",
        "      left = left_tree,\n",
        "      right = right_tree\n",
        "    ))\n",
        "  }\n",
        "\n",
        "  build_node(data, 0)\n",
        "}\n",
        "\n",
        "# Function to predict with a single tree\n",
        "predict_tree <- function(tree, x) {\n",
        "  if (tree$leaf) {\n",
        "    return(tree$prediction)\n",
        "  }\n",
        "\n",
        "  if (x[tree$feature] <= tree$threshold) {\n",
        "    return(predict_tree(tree$left, x))\n",
        "  } else {\n",
        "    return(predict_tree(tree$right, x))\n",
        "  }\n",
        "}\n",
        "\n",
        "# Function to predict with the ensemble\n",
        "predict_extra_trees <- function(trees, data) {\n",
        "  predictions <- matrix(NA, nrow(data), length(trees))\n",
        "  for (i in 1:length(trees)) {\n",
        "    predictions[, i] <- apply(data, 1, function(x) predict_tree(trees[[i]], x))\n",
        "  }\n",
        "\n",
        "  # Majority voting\n",
        "  apply(predictions, 1, function(row) {\n",
        "    tbl <- table(row)\n",
        "    names(which.max(tbl))\n",
        "  })\n",
        "}\n",
        "\n",
        "# Main Extra Trees function\n",
        "extra_trees_classification <- function(data, n_trees = 100, use_bootstrap = TRUE) {\n",
        "  trees <- list()\n",
        "  for (i in 1:n_trees) {\n",
        "    # Use bootstrap sample or full dataset\n",
        "    sample_data <- if (use_bootstrap) bootstrap_sample(data) else data\n",
        "    trees[[i]] <- build_tree(sample_data)\n",
        "  }\n",
        "  return(trees)\n",
        "}\n",
        "\n",
        "# Split Iris data into training and testing sets\n",
        "set.seed(123)\n",
        "train_idx <- sample(1:nrow(iris_data), 0.7 * nrow(iris_data))\n",
        "train_data <- iris_data[train_idx, ]\n",
        "test_data <- iris_data[-train_idx, ]\n",
        "\n",
        "# Train Extra Trees model\n",
        "trees <- extra_trees_classification(train_data, n_trees = 100, use_bootstrap = TRUE)\n",
        "\n",
        "# Make predictions\n",
        "predictions <- predict_extra_trees(trees, test_data)\n",
        "\n",
        "# Evaluate accuracy\n",
        "actual <- test_data$Species\n",
        "accuracy <- mean(predictions == actual)\n",
        "cat(\"Accuracy:\", round(accuracy, 3), \"\\n\")\n",
        "\n",
        "# Output confusion matrix\n",
        "conf_matrix <- table(Predicted = predictions, Actual = actual)\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "id": "bnJ7L5JfKEFZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14e976d9-5270-4efb-ea8d-ffbbecf0951d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.978 \n",
            "            Actual\n",
            "Predicted    setosa versicolor virginica\n",
            "  setosa         14          0         0\n",
            "  versicolor      0         17         0\n",
            "  virginica       0          1        13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regression\n",
        "\n",
        "This R code implements an **Extra Trees Regressor** on the Boston Housing dataset using base R. Here's a brief explanation:\n",
        "\n",
        "1. **Dataset**: Uses Boston Housing (`BostonHousing`) to predict `medv` (house prices). Converts `chas` (factor) to numeric.\n",
        "\n",
        "2. **Bootstrap Sampling**:\n",
        "   - `bootstrap_sample`: Generates a random sample with replacement (if `use_bootstrap = TRUE`).\n",
        "\n",
        "3. **Feature Selection**:\n",
        "   - `select_features`: Randomly selects `k` numeric features (default: `n_features / 3` for regression).\n",
        "\n",
        "4. **Tree Building** (`build_tree`):\n",
        "   - Constructs a decision tree:\n",
        "     - Randomly selects features and a random split threshold.\n",
        "     - Stops at `max_depth = 5` or `min_size = 5`.\n",
        "     - Returns leaf node with mean `medv` if stopping criteria met or no valid split.\n",
        "     - Recursively builds subtrees for valid splits.\n",
        "\n",
        "5. **Prediction**:\n",
        "   - `predict_tree`: Predicts a value for a row by traversing a tree.\n",
        "   - `predict_extra_trees`: Averages predictions across all trees.\n",
        "\n",
        "6. **Main Function** (`extra_trees_regression`):\n",
        "   - Builds `n_trees = 100` trees using bootstrap sampling or full dataset.\n",
        "\n",
        "7. **Training/Testing**:\n",
        "   - Splits data into 70% training, 30% testing.\n",
        "   - Trains model and predicts on test set.\n",
        "\n",
        "8. **Evaluation**:\n",
        "   - Computes RMSE (Root Mean Squared Error) to measure prediction error.\n",
        "\n",
        "**Extra Trees Traits**:\n",
        "- Random feature selection and split thresholds reduce variance.\n",
        "- Ensemble averaging improves robustness.\n",
        "- Bootstrap sampling enabled."
      ],
      "metadata": {
        "id": "hm5QODWQKJVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Extra Trees Regression on BostonHousing Dataset\n",
        "# No external packages, base R only\n",
        "\n",
        "# Load BostonHousing dataset (assumed available in R environment)\n",
        "data(Boston, package = \"MASS\")\n",
        "boston_data <- Boston\n",
        "\n",
        "# Ensure all features are numeric (convert factors if necessary)\n",
        "# Note: 'chas' is a factor; we’ll exclude it from feature selection\n",
        "boston_data$chas <- as.numeric(as.character(boston_data$chas))  # Convert chas to numeric (0 or 1)\n",
        "\n",
        "# Function to create a bootstrap sample\n",
        "bootstrap_sample <- function(data) {\n",
        "  n <- nrow(data)\n",
        "  indices <- sample(1:n, n, replace = TRUE)\n",
        "  return(data[indices, ])\n",
        "}\n",
        "\n",
        "# Function to select random features (only numeric columns)\n",
        "select_features <- function(n_features, k, numeric_cols) {\n",
        "  sample(numeric_cols, k, replace = FALSE)\n",
        "}\n",
        "\n",
        "# Function to build a single decision tree\n",
        "build_tree <- function(data, max_depth = 5, min_size = 5) {\n",
        "  build_node <- function(data, depth) {\n",
        "    # Stop if max depth reached or node too small\n",
        "    if (depth >= max_depth || nrow(data) <= min_size) {\n",
        "      return(list(leaf = TRUE, prediction = mean(data$medv)))\n",
        "    }\n",
        "\n",
        "    # Identify numeric columns (exclude medv and non-numeric columns)\n",
        "    numeric_cols <- which(sapply(data, is.numeric) & names(data) != \"medv\")\n",
        "    n_features <- length(numeric_cols)\n",
        "    k <- floor(n_features / 3)  # Use m/3 for regression\n",
        "    if (k < 1) k <- 1  # Ensure at least one feature\n",
        "\n",
        "    # Select random features\n",
        "    features <- select_features(n_features, k, numeric_cols)\n",
        "\n",
        "    # Find best random split\n",
        "    best_feature <- NULL\n",
        "    best_threshold <- NULL\n",
        "    best_split <- NULL\n",
        "\n",
        "    for (f in features) {\n",
        "      # Get random threshold within feature range\n",
        "      values <- data[[f]]\n",
        "      if (length(unique(values)) > 1) {  # Ensure feature has variability\n",
        "        threshold <- runif(1, min(values), max(values))\n",
        "\n",
        "        # Split data\n",
        "        left <- data[data[[f]] <= threshold, ]\n",
        "        right <- data[data[[f]] > threshold, ]\n",
        "\n",
        "        # Ensure non-empty splits\n",
        "        if (nrow(left) > 0 && nrow(right) > 0) {\n",
        "          best_feature <- f\n",
        "          best_threshold <- threshold\n",
        "          best_split <- list(left = left, right = right)\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "\n",
        "    # If no valid split, return leaf\n",
        "    if (is.null(best_feature)) {\n",
        "      return(list(leaf = TRUE, prediction = mean(data$medv)))\n",
        "    }\n",
        "\n",
        "    # Recursively build left and right subtrees\n",
        "    left_tree <- build_node(best_split$left, depth + 1)\n",
        "    right_tree <- build_node(best_split$right, depth + 1)\n",
        "\n",
        "    return(list(\n",
        "      leaf = FALSE,\n",
        "      feature = best_feature,\n",
        "      threshold = best_threshold,\n",
        "      left = left_tree,\n",
        "      right = right_tree\n",
        "    ))\n",
        "  }\n",
        "\n",
        "  build_node(data, 0)\n",
        "}\n",
        "\n",
        "# Function to predict with a single tree\n",
        "predict_tree <- function(tree, x) {\n",
        "  if (tree$leaf) {\n",
        "    return(tree$prediction)\n",
        "  }\n",
        "\n",
        "  if (x[tree$feature] <= tree$threshold) {\n",
        "    return(predict_tree(tree$left, x))\n",
        "  } else {\n",
        "    return(predict_tree(tree$right, x))\n",
        "  }\n",
        "}\n",
        "\n",
        "# Function to predict with the ensemble\n",
        "predict_extra_trees <- function(trees, data) {\n",
        "  predictions <- matrix(NA, nrow(data), length(trees))\n",
        "  for (i in 1:length(trees)) {\n",
        "    predictions[, i] <- apply(data, 1, function(x) predict_tree(trees[[i]], x))\n",
        "  }\n",
        "\n",
        "  # Average predictions\n",
        "  rowMeans(predictions)\n",
        "}\n",
        "\n",
        "# Main Extra Trees function\n",
        "extra_trees_regression <- function(data, n_trees = 100, use_bootstrap = TRUE) {\n",
        "  trees <- list()\n",
        "  for (i in 1:n_trees) {\n",
        "    # Use bootstrap sample or full dataset\n",
        "    sample_data <- if (use_bootstrap) bootstrap_sample(data) else data\n",
        "    trees[[i]] <- build_tree(sample_data)\n",
        "  }\n",
        "  return(trees)\n",
        "}\n",
        "\n",
        "# Split BostonHousing data into training and testing sets\n",
        "set.seed(123)\n",
        "train_idx <- sample(1:nrow(boston_data), 0.7 * nrow(boston_data))\n",
        "train_data <- boston_data[train_idx, ]\n",
        "test_data <- boston_data[-train_idx, ]\n",
        "\n",
        "# Train Extra Trees model\n",
        "trees <- extra_trees_regression(train_data, n_trees = 100, use_bootstrap = TRUE)\n",
        "\n",
        "# Make predictions\n",
        "predictions <- predict_extra_trees(trees, test_data)\n",
        "\n",
        "# Evaluate performance (Mean Squared Error)\n",
        "actual <- test_data$medv\n",
        "mse <- mean((predictions - actual)^2)\n",
        "rmse <- sqrt(mse)\n",
        "cat(\"RMSE:\", round(rmse, 3), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDQate6zLGeq",
        "outputId": "0824f586-9e67-45f3-aae3-8ec33c1338b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 6.403 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Trees with R\n",
        "\n",
        "The Extra Trees algorithm can be implemented in R using the {extraTrees} package. Below is a step-by-step guide for installing and using the package for regression tasks. However, please note that the {extraTrees} package is no longer available on the CRAN repository, as it was removed on June 14, 2022, due to unresolved issues identified during checks, as documented in the CRAN archive. You can still download and install the archived version of the {extraTrees} package (version 1.0.5, the last available) from the CRAN archive.\n",
        "\n",
        "While using Google Colab, I encountered a Java runtime error when trying to run the {extraTrees} library, which prevented me from utilizing its functionality. To work around this issue, I developed my own function that mimics the functionality of the `extraTrees()` function, allowing me to fit an extra tree model."
      ],
      "metadata": {
        "id": "bh7jEhGCLDZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and Check Required Libraries"
      ],
      "metadata": {
        "id": "RlLmnBj77AMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Download the archived package\n",
        "url <- \"https://cran.r-project.org/src/contrib/Archive/extraTrees/extraTrees_1.0.5.tar.gz\"\n",
        "download.file(url, destfile = \"extraTrees_1.0.5.tar.gz\")\n"
      ],
      "metadata": {
        "id": "iyJQ4X5iCkhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Install the package\n",
        "install.packages(\"./extraTrees_1.0.5.tar.gz\", repos = NULL, type = \"source\", lib='drive/My Drive/R/')"
      ],
      "metadata": {
        "id": "er5FShwgDq0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "packages <- c('tidyverse',\n",
        "              'plyr',\n",
        "              'mlbench',\n",
        "              'Metrics'\n",
        "         )"
      ],
      "metadata": {
        "id": "9rXvjfapLYED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Missing Packages"
      ],
      "metadata": {
        "id": "MDwlBWYYLc1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Install missing packages\n",
        "new.packages <- packages[!(packages %in% installed.packages(lib='drive/My Drive/R/')[,\"Package\"])]\n",
        "if(length(new.packages)) install.packages(new.packages, lib='drive/My Drive/R/')"
      ],
      "metadata": {
        "id": "PWzWe7xULb2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verify Installation"
      ],
      "metadata": {
        "id": "HM3Bl4-PLfFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# set library path\n",
        ".libPaths('drive/My Drive/R')\n",
        "# Verify installation\n",
        "cat(\"Installed packages:\\n\")\n",
        "print(sapply(packages, requireNamespace, quietly = TRUE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHr9zUJvLfNl",
        "outputId": "664a1896-0d8e-4dff-bd65-63519d8f8c56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installed packages:\n",
            "tidyverse      plyr   mlbench   Metrics \n",
            "     TRUE      TRUE      TRUE      TRUE \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load R Packages"
      ],
      "metadata": {
        "id": "2cPLFz1GLlRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# set library path\n",
        ".libPaths('drive/My Drive/R')\n",
        "# Load packages with suppressed messages\n",
        "invisible(lapply(packages, function(pkg) {\n",
        "  suppressPackageStartupMessages(library(pkg, character.only = TRUE))\n",
        "}))"
      ],
      "metadata": {
        "id": "cQl-1GIuLmCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Loaded Packages"
      ],
      "metadata": {
        "id": "AH4vKW2yL1OF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Check loaded packages\n",
        "cat(\"Successfully loaded packages:\\n\")\n",
        "print(search()[grepl(\"package:\", search())])# Check loaded packageswer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Du3QbdUeL1Yw",
        "outputId": "737009fd-3b51-463d-a2dc-f8ed9740eb36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded packages:\n",
            " [1] \"package:Metrics\"   \"package:mlbench\"   \"package:plyr\"     \n",
            " [4] \"package:lubridate\" \"package:forcats\"   \"package:stringr\"  \n",
            " [7] \"package:dplyr\"     \"package:purrr\"     \"package:readr\"    \n",
            "[10] \"package:tidyr\"     \"package:tibble\"    \"package:ggplot2\"  \n",
            "[13] \"package:tidyverse\" \"package:tools\"     \"package:stats\"    \n",
            "[16] \"package:graphics\"  \"package:grDevices\" \"package:utils\"    \n",
            "[19] \"package:datasets\"  \"package:methods\"   \"package:base\"     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification Example\n",
        "\n",
        "This section demonstrates how to use Conditional Random Forest (cforest) for classification tasks in R, specifically using the `{party}` package. We will build a cforest model on the Health Insurance dataset, which contains information about individuals' choices of insurance products based on various features."
      ],
      "metadata": {
        "id": "aS-CaRU1MCaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data and Data Processing\n",
        "\n",
        "For classification, we will use the {party} packages to build a cforest model on [Health Iinsurance](http://peopleanalytics-regression-book.org/data/health_insurance.csv) data. The dataset contains information about individuals' choices of insurance products based on various features.\n",
        "\n",
        "We will use `read_csv()` function of {readr} package to import data as a **tidy** data.\n"
      ],
      "metadata": {
        "id": "euKDzTA_OEl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Load data\n",
        "mf <- readr::read_csv(\"https://github.com/zia207/r-colab/raw/main/Data/Machine_Learning/health_insurance.csv\")\n",
        "\n",
        "# Convert to factors\n",
        "mf$product <- as.factor(mf$product)\n",
        "mf$gender <- as.factor(mf$gender)\n",
        "\n",
        "# Split data into train and test (stratified by product and gender)\n",
        "seeds <- 11076\n",
        "tr_prop <- 0.70\n",
        "set.seed(seeds)\n",
        "train <- ddply(mf, .(product, gender),\n",
        "               function(., seed) { set.seed(seed); .[sample(1:nrow(.), trunc(nrow(.) * tr_prop)), ] }, seed = 101)\n",
        "test <- ddply(mf, .(product, gender),\n",
        "              function(., seed) { set.seed(seed); .[-sample(1:nrow(.), trunc(nrow(.) * tr_prop)), ] }, seed = 101)\n",
        "\n",
        "# Prepare features and target\n",
        "train_features <- train[, c(\"age\", \"household\", \"position_level\", \"absent\", \"gender\")]\n",
        "train_target <- train$product\n",
        "test_features <- test[, c(\"age\", \"household\", \"position_level\", \"absent\", \"gender\")]\n",
        "test_target <- test$product\n",
        "\n",
        "# Convert training features to numeric matrix\n",
        "train_features$gender <- as.factor(train_features$gender)\n",
        "train_features_numeric <- train_features\n",
        "train_features_numeric$gender <- as.numeric(train_features$gender) - 1  # Encode: Female=0, Male=1\n",
        "train_features_numeric$age <- as.numeric(train_features_numeric$age)\n",
        "train_features_numeric$household <- as.numeric(train_features_numeric$household)\n",
        "train_features_numeric$position_level <- as.numeric(train_features_numeric$position_level)\n",
        "train_features_numeric$absent <- as.numeric(train_features_numeric$absent)\n",
        "train_features_matrix <- as.matrix(train_features_numeric)\n",
        "\n",
        "# Convert test features to numeric matrix\n",
        "test_features$gender <- as.factor(test_features$gender)\n",
        "test_features_numeric <- test_features\n",
        "test_features_numeric$gender <- as.numeric(test_features$gender) - 1\n",
        "test_features_numeric$age <- as.numeric(test_features_numeric$age)\n",
        "test_features_numeric$household <- as.numeric(test_features_numeric$household)\n",
        "test_features_numeric$position_level <- as.numeric(test_features_numeric$position_level)\n",
        "test_features_numeric$absent <- as.numeric(test_features_numeric$absent)\n",
        "test_features_matrix <- as.matrix(test_features_numeric)\n",
        "\n",
        "# Ensure train_target is a factor\n",
        "train_target <- as.factor(train_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiC3ipq-OHim",
        "outputId": "07499e3b-a2f6-4466-8dbf-a3a96e3094dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows: 1448 Columns: 6\n",
            "── Column specification ────────────────────────────────────────────────────────\n",
            "Delimiter: \",\"\n",
            "chr (2): product, gender\n",
            "dbl (4): age, household, position_level, absent\n",
            "\n",
            "ℹ Use `spec()` to retrieve the full column specification for this data.\n",
            "ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extra Trees function\n",
        "\n",
        "The `extraTrees` function implements an Extra Trees classifier in base R for a numeric feature matrix `x` and factor target `y`.\n",
        "\n",
        "**Key Features**:\n",
        "- **Inputs**: `x` (numeric matrix), `y` (factor), `ntree` (500 trees), `mtry` (√features), `nodesize` (min 5 samples), `numThreads` (ignored, set to 1).\n",
        "- **Validation**: Checks `x` is numeric, `y` is a factor, dimensions match, and `mtry`/`nodesize` are valid.\n",
        "- **Bootstrap Sampling**: Creates random samples with replacement for each tree.\n",
        "- **Tree Building**:\n",
        "  - Selects `mtry` random features per node.\n",
        "  - Uses random split thresholds (not optimized).\n",
        "  - Stops at `max_depth=10` or `nodesize`.\n",
        "  - Returns majority class for leaf nodes.\n",
        "- **Output**: Returns a list of `ntree` trees and parameters.\n",
        "\n",
        "**Process**: Builds `ntree` decision trees with random feature selection and splits, using bootstrap samples, for classification via majority voting."
      ],
      "metadata": {
        "id": "JaFT5tP5OrTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Extra Trees function\n",
        "extraTrees <- function(x, y, ntree = 500, mtry = floor(sqrt(ncol(x))), nodesize = 5, numThreads = 1) {\n",
        "  # Input validation\n",
        "  if (!is.matrix(x) || !is.numeric(x)) stop(\"x must be a numeric matrix\")\n",
        "  if (!is.factor(y)) stop(\"y must be a factor for classification\")\n",
        "  if (nrow(x) != length(y)) stop(\"Number of rows in x must match length of y\")\n",
        "  if (mtry < 1 || mtry > ncol(x)) stop(\"mtry must be between 1 and number of features\")\n",
        "  if (nodesize < 1) stop(\"nodesize must be positive\")\n",
        "\n",
        "  # Function to create a bootstrap sample\n",
        "  bootstrap_sample <- function(data, target) {\n",
        "    n <- nrow(data)\n",
        "    indices <- sample(1:n, n, replace = TRUE)\n",
        "    return(list(data = data[indices, , drop = FALSE], target = target[indices]))\n",
        "  }\n",
        "\n",
        "  # Function to select random features\n",
        "  select_features <- function(n_features, k) {\n",
        "    sample(1:n_features, k, replace = FALSE)\n",
        "  }\n",
        "\n",
        "  # Function to build a single decision tree\n",
        "  build_tree <- function(data, target, max_depth = 10, min_size = nodesize) {\n",
        "    build_node <- function(data, target, depth) {\n",
        "      # Stop if max depth reached or node too small\n",
        "      if (depth >= max_depth || nrow(data) <= min_size) {\n",
        "        classes <- table(target)\n",
        "        return(list(leaf = TRUE, prediction = names(which.max(classes))))\n",
        "      }\n",
        "\n",
        "      # Select random features\n",
        "      n_features <- ncol(data)\n",
        "      k <- min(mtry, n_features)\n",
        "      features <- select_features(n_features, k)\n",
        "\n",
        "      # Find a valid random split\n",
        "      best_feature <- NULL\n",
        "      best_threshold <- NULL\n",
        "      best_split <- NULL\n",
        "\n",
        "      for (f in features) {\n",
        "        values <- data[, f]\n",
        "        if (length(unique(values)) > 1) {\n",
        "          threshold <- runif(1, min(values), max(values))\n",
        "          left_idx <- data[, f] <= threshold\n",
        "          right_idx <- data[, f] > threshold\n",
        "\n",
        "          if (sum(left_idx) > 0 && sum(right_idx) > 0) {\n",
        "            best_feature <- f\n",
        "            best_threshold <- threshold\n",
        "            best_split <- list(\n",
        "              left_data = data[left_idx, , drop = FALSE],\n",
        "              left_target = target[left_idx],\n",
        "              right_data = data[right_idx, , drop = FALSE],\n",
        "              right_target = target[right_idx]\n",
        "            )\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "\n",
        "      # If no valid split, return leaf\n",
        "      if (is.null(best_feature)) {\n",
        "        classes <- table(target)\n",
        "        return(list(leaf = TRUE, prediction = names(which.max(classes))))\n",
        "      }\n",
        "\n",
        "      # Recursively build subtrees\n",
        "      left_tree <- build_node(best_split$left_data, best_split$left_target, depth + 1)\n",
        "      right_tree <- build_node(best_split$right_data, best_split$right_target, depth + 1)\n",
        "\n",
        "      return(list(\n",
        "        leaf = FALSE,\n",
        "        feature = best_feature,\n",
        "        threshold = best_threshold,\n",
        "        left = left_tree,\n",
        "        right = right_tree\n",
        "      ))\n",
        "    }\n",
        "\n",
        "    build_node(data, target, 0)\n",
        "  }\n",
        "\n",
        "  # Build ntree trees\n",
        "  trees <- list()\n",
        "  for (i in 1:ntree) {\n",
        "    sample <- bootstrap_sample(x, y)\n",
        "    trees[[i]] <- build_tree(sample$data, sample$target)\n",
        "  }\n",
        "\n",
        "  # Return model with trees and parameters\n",
        "  return(list(trees = trees, mtry = mtry, nodesize = nodesize))\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6vVnAayOuQ6",
        "outputId": "18fc92c8-9302-4deb-8168-09a902627763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.824 \n",
            "         Actual\n",
            "Predicted   A   B   C\n",
            "        A 128  10  20\n",
            "        B  20 123  21\n",
            "        C   1   5 110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prediction Function\n",
        "\n",
        "The `predict_extraTrees` function predicts class labels for a numeric feature matrix `x` using an Extra Trees model.\n",
        "\n",
        "**Key Features**:\n",
        "- **Inputs**: `model` (list from `extraTrees`), `x` (numeric matrix).\n",
        "- **Validation**: Ensures `x` is a numeric matrix.\n",
        "- **Prediction**:\n",
        "  - `predict_tree`: Traverses a single tree, returning the leaf’s class if reached, or recursively follows left/right branches based on feature thresholds.\n",
        "  - Applies `predict_tree` to each row of `x` for all trees, collecting predictions.\n",
        "- **Output**: Returns class labels via majority voting across all trees’ predictions.\n",
        "\n",
        "**Process**: Predicts by evaluating each test row through all trees and selecting the most common class."
      ],
      "metadata": {
        "id": "orBWQrs6L1QG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "predict_extraTrees <- function(model, x) {\n",
        "  # Input validation\n",
        "  if (!is.matrix(x) || !is.numeric(x)) stop(\"x must be a numeric matrix\")\n",
        "\n",
        "  # Predict with a single tree\n",
        "  predict_tree <- function(tree, x) {\n",
        "    if (tree$leaf) {\n",
        "      return(tree$prediction)\n",
        "    }\n",
        "    if (x[tree$feature] <= tree$threshold) {\n",
        "      return(predict_tree(tree$left, x))\n",
        "    } else {\n",
        "      return(predict_tree(tree$right, x))\n",
        "    }\n",
        "  }\n",
        "\n",
        "  # Get predictions from all trees\n",
        "  predictions <- matrix(NA, nrow(x), length(model$trees))\n",
        "  for (i in 1:length(model$trees)) {\n",
        "    predictions[, i] <- apply(x, 1, function(row) predict_tree(model$trees[[i]], row))\n",
        "  }\n",
        "\n",
        "  # Majority voting\n",
        "  apply(predictions, 1, function(row) {\n",
        "    tbl <- table(row)\n",
        "    names(which.max(tbl))\n",
        "  })\n",
        "}"
      ],
      "metadata": {
        "id": "QXqjga-JL1dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit Extra Trees Model\n",
        "\n",
        "The main function for fitting the Extra Trees model is `extraTrees()`. The function takes the following arguments:\n",
        "\n",
        "-   The `ntree` parameter sets the number of trees in the Extra Trees ensemble.\n",
        "\n",
        "-   The `mtry` parameter specifies the number of features to consider at each split. For Extra Trees, this is typically set to the square root of the total number of features.\n",
        "\n",
        "-   The `nodesize` parameter sets the minimum number of samples required to split a node.\n",
        "\n",
        "-   The `numThreads` parameter specifies the number of threads to use for parallel processing."
      ],
      "metadata": {
        "id": "pEQt68dVMHuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Train Extra Trees model\n",
        "seeds=123\n",
        "set.seed(seeds)\n",
        "et_model <- extraTrees(\n",
        "  x = train_features_matrix,\n",
        "  y = train_target,\n",
        "  ntree = 500,\n",
        "  mtry = floor(sqrt(ncol(train_features_matrix))),  # 2 features\n",
        "  nodesize = 5,\n",
        "  numThreads = 1  # Set to 1 since base R doesn't support threading\n",
        ")"
      ],
      "metadata": {
        "id": "4g2pShDvPTSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prediction and Evaluation"
      ],
      "metadata": {
        "id": "aGdnDz8-O2QF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Make predictions on test set\n",
        "predictions <- predict_extraTrees(et_model, test_features_matrix)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy <- mean(predictions == test_target)\n",
        "cat(\"Accuracy:\", round(accuracy, 3), \"\\n\")\n",
        "\n",
        "# Output confusion matrix\n",
        "conf_matrix <- table(Predicted = predictions, Actual = test_target)\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNGjn3RLO85q",
        "outputId": "a4c93c03-2bb7-4610-9d32-a92aaddd4ba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.822 \n",
            "         Actual\n",
            "Predicted   A   B   C\n",
            "        A 128  12  18\n",
            "        B  18 120  21\n",
            "        C   3   6 112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feature Importance\n",
        "\n",
        "Since feature importance is not available, we can compute feature importance manually using permutation importance. This method measures the decrease in model performance when a feature’s values are randomly shuffled, indicating its importance."
      ],
      "metadata": {
        "id": "LbYzS8pYPGVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Function to compute permutation importance\n",
        "compute_permutation_importance <- function(model, x, y, feature_names) {\n",
        "  # Baseline accuracy\n",
        "  pred_baseline <- predict_extraTrees(model, x)\n",
        "  baseline_accuracy <- mean(pred_baseline == y)\n",
        "\n",
        "  # Initialize importance vector\n",
        "  importance <- numeric(length(feature_names))\n",
        "  names(importance) <- feature_names\n",
        "\n",
        "  # Permute each feature and measure accuracy drop\n",
        "  for (i in 1:length(feature_names)) {\n",
        "    x_permuted <- x\n",
        "    x_permuted[, i] <- sample(x_permuted[, i])  # Shuffle feature i\n",
        "    pred_permuted <- predict_extraTrees(model, x_permuted)\n",
        "    permuted_accuracy <- mean(pred_permuted == y)\n",
        "    importance[i] <- baseline_accuracy - permuted_accuracy\n",
        "  }\n",
        "\n",
        "  return(importance)\n",
        "}"
      ],
      "metadata": {
        "id": "-oL1iZIVMob8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Compute importance\n",
        "feature_names <- c(\"age\", \"household\", \"position_level\", \"absent\", \"gender\")\n",
        "importance <- compute_permutation_importance(\n",
        "  model = et_model,\n",
        "  x = train_features_matrix,\n",
        "  y = train_target,\n",
        "  feature_names = feature_names\n",
        ")\n",
        "\n",
        "# Create data frame\n",
        "var_imp_et <- data.frame(\n",
        "  Variable = feature_names,\n",
        "  Importance = importance\n",
        ")"
      ],
      "metadata": {
        "id": "bDGICYF_Mwcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "ggplot(var_imp_et, aes(x = reorder(Variable, Importance), y = Importance)) +\n",
        "  geom_bar(stat = \"identity\", fill = \"steelblue\") +  # Add color\n",
        "  coord_flip() +\n",
        "  labs(title = \"Variable Importance for Extra Trees Classification\",\n",
        "       x = \"Features\",\n",
        "       y = \"Accuracy Drop\") +\n",
        "  theme_minimal() +\n",
        "  theme(\n",
        "    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n",
        "    axis.text = element_text(size = 12),\n",
        "    axis.title = element_text(size = 12)\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "6qqOhaYLP9nA",
        "outputId": "1f181295-faa9-4511-b050-4bc6f340e044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regression Example\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SN2wTqH2PTqt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataa and Data Processing"
      ],
      "metadata": {
        "id": "WatdvRREVuS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Load and preprocess data\n",
        "mf <- read_csv(\"https://github.com/zia207/r-colab/raw/main/Data/Machine_Learning/gp_soil_data.csv\")\n",
        "df <- mf %>% select(SOC, DEM, Slope, Aspect, TPI, KFactor, SiltClay, MAT, MAP, NDVI, NLCD, FRG)\n",
        "df$NLCD <- as.factor(df$NLCD)\n",
        "df$FRG <- as.factor(df$FRG)\n",
        "\n",
        "# Check for missing values\n",
        "if (anyNA(df)) {\n",
        "  cat(\"Missing values detected. Removing rows with NA.\\n\")\n",
        "  df <- na.omit(df)\n",
        "}\n",
        "\n",
        "# Split data (70% train, stratified by NLCD and FRG)\n",
        "seeds <- 11076\n",
        "set.seed(seeds)\n",
        "train <- ddply(df, .(NLCD, FRG), function(., seed) {\n",
        "  set.seed(seed)\n",
        "  if (nrow(.) > 0) .[sample(nrow(.), trunc(nrow(.) * 0.7)), ] else .\n",
        "}, seed = 101)\n",
        "test <- ddply(df, .(NLCD, FRG), function(., seed) {\n",
        "  set.seed(seed)\n",
        "  if (nrow(.) > 0) .[-sample(nrow(.), trunc(nrow(.) * 0.7)), ] else .\n",
        "}, seed = 101)\n",
        "\n",
        "# Ensure train and test have data\n",
        "if (nrow(train) == 0 || nrow(test) == 0) stop(\"Train or test set is empty after splitting.\")\n",
        "\n",
        "# Scale numerical features\n",
        "numerical_cols <- c(\"DEM\", \"Slope\", \"Aspect\", \"TPI\", \"KFactor\", \"SiltClay\", \"MAT\", \"MAP\", \"NDVI\")\n",
        "train[numerical_cols] <- scale(train[numerical_cols])\n",
        "test[numerical_cols] <- scale(test[numerical_cols])\n",
        "\n",
        "# Encode categorical variables (NLCD, FRG) into dummy variables\n",
        "combined <- rbind(cbind(train, set = \"train\"), cbind(test, set = \"test\"))\n",
        "# Ensure all levels are consistent\n",
        "combined$NLCD <- factor(combined$NLCD, levels = levels(df$NLCD))\n",
        "combined$FRG <- factor(combined$FRG, levels = levels(df$FRG))\n",
        "dummy_matrix <- model.matrix(~ NLCD + FRG - 1, data = combined)\n",
        "train_dummy <- dummy_matrix[combined$set == \"train\", ]\n",
        "test_dummy <- dummy_matrix[combined$set == \"test\", ]\n",
        "\n",
        "# Combine numerical and dummy variables\n",
        "x_train <- as.matrix(cbind(train[numerical_cols], train_dummy))\n",
        "x_test <- as.matrix(cbind(test[numerical_cols], test_dummy))\n",
        "y_train <- train$SOC\n",
        "y_test <- test$SOC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiauoScUVuct",
        "outputId": "e5780902-1616-47c3-bba6-cba6b00a4f82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows: 467 Columns: 19\n",
            "── Column specification ────────────────────────────────────────────────────────\n",
            "Delimiter: \",\"\n",
            "chr  (4): STATE, COUNTY, NLCD, FRG\n",
            "dbl (15): ID, FIPS, STATE_ID, Longitude, Latitude, SOC, DEM, Aspect, Slope, ...\n",
            "\n",
            "ℹ Use `spec()` to retrieve the full column specification for this data.\n",
            "ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extra Trees function (for regression)"
      ],
      "metadata": {
        "id": "zqooJblKPaDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# 1) Extra Trees function (for regression)\n",
        "extraTrees <- function(x, y, ntree = 100, mtry = floor(sqrt(ncol(x))), nodesize = 5) {\n",
        "  # Validate inputs\n",
        "  if (anyNA(x) || anyNA(y)) stop(\"Input data contains missing values.\")\n",
        "  if (nrow(x) != length(y)) stop(\"Rows in x must match length of y.\")\n",
        "\n",
        "  bootstrap_sample <- function(data, target) {\n",
        "    indices <- sample(nrow(data), replace = TRUE)\n",
        "    list(data = data[indices, ], target = target[indices])\n",
        "  }\n",
        "\n",
        "  select_features <- function(n_features, k) sample(n_features, k)\n",
        "\n",
        "  feature_importance <- numeric(ncol(x))\n",
        "  names(feature_importance) <- colnames(x)\n",
        "\n",
        "  build_tree <- function(data, target) {\n",
        "    build_node <- function(data, target, depth) {\n",
        "      # Check for valid data\n",
        "      if (!is.matrix(data) || nrow(data) == 0 || anyNA(data) || length(target) == 0) {\n",
        "        return(list(leaf = TRUE, prediction = mean(target, na.rm = TRUE)))\n",
        "      }\n",
        "      if (depth > 10 || nrow(data) <= nodesize) {\n",
        "        return(list(leaf = TRUE, prediction = mean(target, na.rm = TRUE)))\n",
        "      }\n",
        "\n",
        "      features <- select_features(ncol(data), mtry)\n",
        "      best_feature <- NULL\n",
        "      best_threshold <- NULL\n",
        "      best_split <- NULL\n",
        "      best_mse <- Inf\n",
        "\n",
        "      for (f in features) {\n",
        "        values <- data[, f]\n",
        "        if (length(unique(values)) > 1) {\n",
        "          threshold <- runif(1, min(values), max(values))\n",
        "          left <- data[, f] <= threshold\n",
        "          if (sum(left) > 0 && sum(!left) > 0) {\n",
        "            left_target <- target[left]\n",
        "            right_target <- target[!left]\n",
        "            mse <- mean((left_target - mean(left_target, na.rm = TRUE))^2, na.rm = TRUE) +\n",
        "                   mean((right_target - mean(right_target, na.rm = TRUE))^2, na.rm = TRUE)\n",
        "            if (mse < best_mse && !is.na(mse)) {\n",
        "              best_mse <- mse\n",
        "              best_feature <- f\n",
        "              best_threshold <- threshold\n",
        "              best_split <- list(left_data = data[left, ], left_target = left_target,\n",
        "                                 right_data = data[!left, ], right_target = right_target)\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "\n",
        "      if (is.null(best_feature)) {\n",
        "        return(list(leaf = TRUE, prediction = mean(target, na.rm = TRUE)))\n",
        "      }\n",
        "\n",
        "      feature_importance[best_feature] <<- feature_importance[best_feature] + 1\n",
        "      left_tree <- build_node(best_split$left_data, best_split$left_target, depth + 1)\n",
        "      right_tree <- build_node(best_split$right_data, best_split$right_target, depth + 1)\n",
        "\n",
        "      list(leaf = FALSE, feature = best_feature, threshold = best_threshold,\n",
        "           left = left_tree, right = right_tree)\n",
        "    }\n",
        "    build_node(data, target, 0)\n",
        "  }\n",
        "\n",
        "  trees <- lapply(1:ntree, function(i) {\n",
        "    sample <- bootstrap_sample(x, y)\n",
        "    build_tree(sample$data, sample$target)\n",
        "  })\n",
        "\n",
        "  list(trees = trees, feature_importance = feature_importance)\n",
        "}"
      ],
      "metadata": {
        "id": "Iq9VcQt7Pbm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prediction Function"
      ],
      "metadata": {
        "id": "oJioPHvKWJVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# 2) Prediction function\n",
        "predict_extraTrees <- function(model, x) {\n",
        "  predict_tree <- function(tree, row) {\n",
        "    if (tree$leaf) return(tree$prediction)\n",
        "    if (row[tree$feature] <= tree$threshold) predict_tree(tree$left, row)\n",
        "    else predict_tree(tree$right, row)\n",
        "  }\n",
        "\n",
        "  predictions <- sapply(model$trees, function(tree) apply(x, 1, function(row) predict_tree(tree, row)))\n",
        "  rowMeans(predictions, na.rm = TRUE)\n",
        "}"
      ],
      "metadata": {
        "id": "t3eLJqGnWJoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit the Extra Tree Model"
      ],
      "metadata": {
        "id": "wbVv7GD1PgnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# 3) Train the model\n",
        "set.seed(seeds)\n",
        "et_model <- extraTrees(x_train,\n",
        "                       y_train,\n",
        "                       ntree = 100,\n",
        "                       mtry = floor(sqrt(ncol(x_train))),\n",
        "                       nodesize = 5)"
      ],
      "metadata": {
        "id": "N059JmapPhQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prediction and Evaluation"
      ],
      "metadata": {
        "id": "oTOC7MItPyP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# 4) Prediction and evaluation\n",
        "y_pred <- predict_extraTrees(et_model, x_test)\n",
        "rmse <- sqrt(mean((y_pred - y_test)^2, na.rm = TRUE))\n",
        "r2 <- cor(y_test, y_pred, use = \"complete.obs\")^2\n",
        "cat(\"Test RMSE:\", round(rmse, 3), \"\\n\")\n",
        "cat(\"Test R²:\", round(r2, 3), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9ycjSCdPwiM",
        "outputId": "7c158de1-d1d6-4bb4-f4f0-268e34101b37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test RMSE: 4.246 \n",
            "Test R²: 0.448 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Tuning\n",
        "\n",
        "Hyperparameter tuning is an essential step in optimizing the performance of Extra Trees models. The key hyperparameters to tune include:\n",
        "\n",
        "-   `ntree`: Number of trees in the ensemble. More trees can improve performance but increase computation time.\n",
        "-   `mtry`: Number of features to consider at each split. A smaller value can reduce overfitting, while a larger value can increase model complexity.\n",
        "-   `nodesize`: Minimum number of samples required to split a node. Larger values can prevent overfitting.\n"
      ],
      "metadata": {
        "id": "AqAqV4HmTBvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# 3) Custom 5-fold cross-validation function\n",
        "create_folds <- function(y, k = 5) {\n",
        "  n <- length(y)\n",
        "  indices <- sample(n)\n",
        "  fold_sizes <- rep(floor(n / k), k)\n",
        "  fold_sizes[1:(n %% k)] <- fold_sizes[1:(n %% k)] + 1\n",
        "  folds <- list()\n",
        "  start <- 1\n",
        "  for (i in 1:k) {\n",
        "    end <- start + fold_sizes[i] - 1\n",
        "    folds[[i]] <- indices[start:end]\n",
        "    start <- end + 1\n",
        "  }\n",
        "  folds\n",
        "}\n",
        "\n",
        "# 5) Train final model with best parameters\n",
        "set.seed(seeds)\n",
        "et_model <- extraTrees(x_train, y_train,\n",
        "                       ntree = best_params$ntree,\n",
        "                       mtry = best_params$mtry,\n",
        "                       nodesize = best_params$nodesize)\n",
        "\n",
        "# 6) Prediction and evaluation on test set\n",
        "y_pred <- predict_extraTrees(et_model, x_test)\n",
        "rmse <- compute_rmse(y_test, y_pred)\n",
        "r2 <- cor(y_test, y_pred, use = \"complete.obs\")^2\n",
        "cat(\"\\nTest RMSE:\", round(rmse, 3), \"\\n\")\n",
        "cat(\"Test R²:\", round(r2, 3), \"\\n\")"
      ],
      "metadata": {
        "id": "f5DsGtZaRJEy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d05ecd89-333d-4ed6-a3ea-c3b5a75bace1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params: ntree=50, mtry=4, nodesize=1 | CV RMSE=3.926, CV R²=0.388\n",
            "Params: ntree=100, mtry=4, nodesize=1 | CV RMSE=3.925, CV R²=0.395\n",
            "Params: ntree=500, mtry=4, nodesize=1 | CV RMSE=3.883, CV R²=0.407\n",
            "Params: ntree=50, mtry=6, nodesize=1 | CV RMSE=4.027, CV R²=0.353\n",
            "Params: ntree=100, mtry=6, nodesize=1 | CV RMSE=3.987, CV R²=0.378\n",
            "Params: ntree=500, mtry=6, nodesize=1 | CV RMSE=3.975, CV R²=0.379\n",
            "Params: ntree=50, mtry=9, nodesize=1 | CV RMSE=4.080, CV R²=0.351\n",
            "Params: ntree=100, mtry=9, nodesize=1 | CV RMSE=4.099, CV R²=0.340\n",
            "Params: ntree=500, mtry=9, nodesize=1 | CV RMSE=4.088, CV R²=0.349\n",
            "Params: ntree=50, mtry=4, nodesize=5 | CV RMSE=3.915, CV R²=0.390\n",
            "Params: ntree=100, mtry=4, nodesize=5 | CV RMSE=3.912, CV R²=0.399\n",
            "Params: ntree=500, mtry=4, nodesize=5 | CV RMSE=3.892, CV R²=0.407\n",
            "Params: ntree=50, mtry=6, nodesize=5 | CV RMSE=4.024, CV R²=0.359\n",
            "Params: ntree=100, mtry=6, nodesize=5 | CV RMSE=3.953, CV R²=0.386\n",
            "Params: ntree=500, mtry=6, nodesize=5 | CV RMSE=3.983, CV R²=0.377\n",
            "Params: ntree=50, mtry=9, nodesize=5 | CV RMSE=4.137, CV R²=0.332\n",
            "Params: ntree=100, mtry=9, nodesize=5 | CV RMSE=4.116, CV R²=0.335\n",
            "Params: ntree=500, mtry=9, nodesize=5 | CV RMSE=4.088, CV R²=0.348\n",
            "Params: ntree=50, mtry=4, nodesize=10 | CV RMSE=3.934, CV R²=0.386\n",
            "Params: ntree=100, mtry=4, nodesize=10 | CV RMSE=3.901, CV R²=0.403\n",
            "Params: ntree=500, mtry=4, nodesize=10 | CV RMSE=3.930, CV R²=0.395\n",
            "Params: ntree=50, mtry=6, nodesize=10 | CV RMSE=4.048, CV R²=0.354\n",
            "Params: ntree=100, mtry=6, nodesize=10 | CV RMSE=4.013, CV R²=0.364\n",
            "Params: ntree=500, mtry=6, nodesize=10 | CV RMSE=4.012, CV R²=0.366\n",
            "Params: ntree=50, mtry=9, nodesize=10 | CV RMSE=4.178, CV R²=0.302\n",
            "Params: ntree=100, mtry=9, nodesize=10 | CV RMSE=4.093, CV R²=0.345\n",
            "Params: ntree=500, mtry=9, nodesize=10 | CV RMSE=4.115, CV R²=0.340\n",
            "\n",
            "Best Parameters:\n",
            "  ntree mtry nodesize\n",
            "3   500    4        1\n",
            "Best CV RMSE: 3.883\n",
            "Best CV R²: 0.407\n",
            "\n",
            "Test RMSE: 4.174 \n",
            "Test R²: 0.466 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyperparameter tuning with grid search"
      ],
      "metadata": {
        "id": "E7kQdjNkeTJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Function to compute RMSE\n",
        "compute_rmse <- function(y_true, y_pred) {\n",
        "  sqrt(mean((y_true - y_pred)^2, na.rm = TRUE))\n",
        "}\n",
        "\n",
        "# 4) Hyperparameter tuning with grid search\n",
        "set.seed(seeds)\n",
        "n_features <- ncol(x_train)\n",
        "param_grid <- expand.grid(\n",
        "  ntree = c(50, 100, 500),\n",
        "  mtry = c(floor(sqrt(n_features)), floor(n_features/3), floor(n_features/2)),\n",
        "  nodesize = c(1, 5, 10)\n",
        ")\n"
      ],
      "metadata": {
        "id": "UXxcv-p3eTSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Perform 3-fold cross-validation"
      ],
      "metadata": {
        "id": "s5WRD_Gxebx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Perform 3-fold cross-validation\n",
        "k <- 3\n",
        "folds <- create_folds(y_train, k)results <- data.frame(param_grid, RMSE = NA, R2 = NA)\n",
        "\n",
        "for (i in 1:nrow(param_grid)) {\n",
        "  ntree <- param_grid$ntree[i]\n",
        "  mtry <- param_grid$mtry[i]\n",
        "  nodesize <- param_grid$nodesize[i]\n",
        "\n",
        "  cv_rmse <- numeric(k)\n",
        "  cv_r2 <- numeric(k)\n",
        "\n",
        "  for (j in 1:k) {\n",
        "    val_idx <- folds[[j]]\n",
        "    train_idx <- setdiff(1:length(y_train), val_idx)\n",
        "\n",
        "    x_train_cv <- x_train[train_idx, ]\n",
        "    y_train_cv <- y_train[train_idx]\n",
        "    x_val_cv <- x_train[val_idx, ]\n",
        "    y_val_cv <- y_train[val_idx]\n",
        "\n",
        "    model_cv <- extraTrees(x_train_cv, y_train_cv, ntree = ntree, mtry = mtry, nodesize = nodesize)\n",
        "    y_pred_cv <- predict_extraTrees(model_cv, x_val_cv)\n",
        "\n",
        "    cv_rmse[j] <- compute_rmse(y_val_cv, y_pred_cv)\n",
        "    cv_r2[j] <- cor(y_val_cv, y_pred_cv, use = \"complete.obs\")^2\n",
        "  }\n",
        "\n",
        "  results$RMSE[i] <- mean(cv_rmse, na.rm = TRUE)\n",
        "  results$R2[i] <- mean(cv_r2, na.rm = TRUE)\n",
        "  cat(sprintf(\"Params: ntree=%d, mtry=%d, nodesize=%d | CV RMSE=%.3f, CV R²=%.3f\\n\",\n",
        "              ntree, mtry, nodesize, results$RMSE[i], results$R2[i]))\n",
        "}\n"
      ],
      "metadata": {
        "id": "Ea7Hgqkheb84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Select best parameters"
      ],
      "metadata": {
        "id": "RFRCvacrezps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Select best parameters\n",
        "best_idx <- which.min(results$RMSE)\n",
        "best_params <- param_grid[best_idx, ]\n",
        "cat(\"\\nBest Parameters:\\n\")\n",
        "print(best_params)\n",
        "cat(sprintf(\"Best CV RMSE: %.3f\\nBest CV R²: %.3f\\n\", results$RMSE[best_idx], results$R2[best_idx]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0YZFgtNez0p",
        "outputId": "2924d2ba-a65a-48f3-c858-8221cd1a8d33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best Parameters:\n",
            "  ntree mtry nodesize\n",
            "3   500    4        1\n",
            "Best CV RMSE: 3.883\n",
            "Best CV R²: 0.407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train final model with best parameters"
      ],
      "metadata": {
        "id": "vLDcz9LbfBqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Train final model with best parameters\n",
        "set.seed(seeds)\n",
        "et_model_final <- extraTrees(x_train, y_train,\n",
        "                       ntree = best_params$ntree,\n",
        "                       mtry = best_params$mtry,\n",
        "                       nodesize = best_params$nodesize)\n",
        "\n"
      ],
      "metadata": {
        "id": "-UOhhn32fB1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prediction and Evaluation"
      ],
      "metadata": {
        "id": "9w_DIFMAfLIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Prediction and evaluation on test set\n",
        "y_pred <- predict_extraTrees(et_model_final, x_test)\n",
        "rmse <- compute_rmse(y_test, y_pred)\n",
        "r2 <- cor(y_test, y_pred, use = \"complete.obs\")^2\n",
        "cat(\"\\nTest RMSE:\", round(rmse, 3), \"\\n\")\n",
        "cat(\"Test R²:\", round(r2, 3), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyniMS25fLSJ",
        "outputId": "792ddfe3-3cd7-47ee-beee-e30c415ba5bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test RMSE: 4.174 \n",
            "Test R²: 0.466 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary and Conclusion\n",
        "\n",
        "Extra Trees is a powerful and efficient ensemble method that can be applied to various machine learning tasks, offering a balance between speed and robustness. It is particularly useful for high-dimensional datasets and can handle both classification and regression problems effectively. In this tutorial , we explored the implementation of Extra Trees in R using the {extraTrees} package, demonstrating its application on both classification and regression tasks. We also discussed the importance of feature selection and model evaluation metrics, providing a comprehensive overview of how to leverage Extra Trees for predictive modeling.  At the end, we computed feature importance using permutation importance, which helps identify the most influential features in the model. This information can guide further analysis and feature engineering efforts."
      ],
      "metadata": {
        "id": "_j759RVMQMut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "\n",
        "1.  Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction* (2nd ed.). Springer.\\\n",
        "\n",
        "2.  Geurts, P., Ernst, D., & Wehenkel, L. (2006). Extremely randomized trees. *Machine Learning*, 63(1), 3–42. https://doi.org/10.1007/s10994-006-6226-1\n",
        "\n",
        "3.  CRAN. (2023). *extraTrees: Extremely Randomized Trees (ExtraTrees) for Regression and Classification*. R package version 0.8-12. https://cran.r-project.org/web/packages/extraTrees/index.html\n",
        "\n",
        "4.  Sim, J. (2016). *Extremely Randomized Trees with R*. R-bloggers. https://www.r-bloggers.com/2016/04/extremely-randomized-trees-with-r/"
      ],
      "metadata": {
        "id": "SaZTnZx3QPGI"
      }
    }
  ]
}