{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPFhYsdCF89EiXQtuSPb/Z3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-04-tree-based-models-decision-tree-c45-r.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![alt text](http://drive.google.com/uc?export=view&id=1bLQ3nhDbZrCCqy_WCxxckOne2lgVvn3l)"
      ],
      "metadata": {
        "id": "Wb-c3rXaCanB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.4 C4.5 Model\n",
        "\n",
        "The **C4.5 model** is a decision tree algorithm used for classification tasks in machine learning, developed by Ross Quinlan as an improvement over his earlier ID3 algorithm. It builds a decision tree from a dataset by recursively splitting the input space into regions based on feature values and making a decision based on the majority class in that region. C4.5 is widely used due to its ability to handle both categorical and continuous data, deal with missing values, and produce interpretable models."
      ],
      "metadata": {
        "id": "JjdcmaWoCcd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Overview of C4.5\n",
        "\n",
        "C4.5 is a supervised learning algorithm that constructs a decision tree based on the concept of information gain. It uses the `gain ratio` to select the best feature for splitting the data at each node, which helps to reduce bias towards features with many values. C4.5 can handle both categorical and continuous features, manage missing values, and apply pruning techniques to avoid overfitting. The algorithm is particularly effective for classification tasks, where the goal is to predict a categorical outcome based on input features. C4.5 has been widely used in various domains, including medical diagnosis, credit scoring, and customer segmentation."
      ],
      "metadata": {
        "id": "zVHn9mmz-qAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Features of C4.5\n",
        "\n",
        "1.  `Handling Continuous Attributes`: C4.5 can process continuous data by selecting optimal thresholds to split numerical features into discrete intervals.\n",
        "2.  `Handling Missing Values`: It manages missing data by using probabilistic methods to assign instances to branches.\n",
        "3.  `Gain Ratio`: Instead of using information gain (as in ID3), C4.5 uses the gain ratio to mitigate bias toward attributes with many values.\n",
        "4.  `Pruning`: Post-pruning is applied to reduce overfitting by removing branches that do not significantly improve accuracy.\n",
        "5.  `Rule Derivation`: C4.5 can convert decision trees into sets of if-then rules for easier interpretation.\n",
        "6.  `Handling Imbalanced Data`: It adjusts for class imbalances through weighting mechanisms."
      ],
      "metadata": {
        "id": "AUaSdX6A-t9T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How C4.5 Works\n",
        "\n",
        "C4.5 constructs a decision tree by recursively selecting the best attribute to split the data, based on the gain ratio criterion, and continues until a stopping condition is met (e.g., all instances belong to one class or a predefined depth is reached). After building the tree, it prunes unnecessary branches to improve generalization.\n",
        "\n",
        "1.  Calculate Information Entropy\n",
        "\n",
        "  Entropy measures the impurity of a dataset. For a dataset $S$ with $c$ classes, entropy is:\n",
        "\n",
        "$$ \\text{Entropy}(S) = -\\sum_{i=1}^c p_i \\log_2(p_i) $$\n",
        "\n",
        "where $p_i$ is the proportion of instances belonging to class $i$.\n",
        "\n",
        "2.  Calculate Information Gain\n",
        "\n",
        "  Information gain measures the reduction in entropy after splitting on an attribute $A$:\n",
        "\n",
        "$$ \\text{Gain}(S, A) = \\text{Entropy}(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\text{Entropy}(S_v) $$\n",
        "\n",
        "where $S_v$ is the subset of $S$ for which attribute $A$ has value $v$, and $|S_v| / |S|$ is the proportion of instances with value $v$.\n",
        "\n",
        "3.  Calculate Split Information\n",
        "\n",
        "  Split information accounts for the number and size of branches created by splitting on attribute $A$:\n",
        "\n",
        "$$ \\text{SplitInfo}(S, A) = -\\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\log_2\\left(\\frac{|S_v|}{|S|}\\right) $$\n",
        "\n",
        "4.  Calculate Gain Ratio\n",
        "\n",
        "  The gain ratio normalizes information gain by split information to reduce bias:\n",
        "\n",
        "$$ \\text{GainRatio}(S, A) = \\frac{\\text{Gain}(S, A)}{\\text{SplitInfo}(S, A)} $$\n",
        "\n",
        "The attribute with the highest gain ratio is selected for splitting.\n",
        "\n",
        "5.  Handle Continuous Attributes\n",
        "\n",
        "  For continuous attributes, C4.5 evaluates possible thresholds by sorting values and testing splits between consecutive values. The threshold yielding the highest gain ratio is chosen.\n",
        "\n",
        "6.  Handle Missing Values\n",
        "\n",
        "   When an instance has a missing value for the splitting attribute, C4.5 assigns it to all branches with probabilities proportional to the number of instances in each branch.\n",
        "\n",
        "7.  Recursive Splitting\n",
        "\n",
        "  The dataset is split based on the selected attribute, and the process repeats for each subset until a stopping criterion is met (e.g., all instances in a node belong to one class, or a minimum node size is reached).\n",
        "\n",
        "8.  Pruning\n",
        "\n",
        "  After building the tree, C4.5 applies reduced error pruning or cost-complexity pruning to remove branches that do not improve classification accuracy on a validation set. This reduces overfitting.\n",
        "\n",
        "9.  Convert to Rules (Optional)\n",
        "\n",
        "  The tree can be converted into a set of if-then rules by tracing paths from the root to each leaf."
      ],
      "metadata": {
        "id": "NkMU_j3Y-vXv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advantages of C4.5\n",
        "\n",
        "-   `Interpretable`: The decision tree is easy to understand and visualize.\n",
        "-   `Handles Mixed Data`: Works with both categorical and continuous features.\n",
        "-   `Robust to Missing Data`: Can handle incomplete datasets.\n",
        "-   `Reduces Overfitting`: Pruning improves generalization.\n",
        "-   `Efficient`: Scales reasonably well for moderately sized datasets."
      ],
      "metadata": {
        "id": "0VugaT0i-6CA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Limitations\n",
        "\n",
        "-   `Bias Toward Splits`: The gain ratio can still favor certain splits, leading to suboptimal trees.\n",
        "-   `Overfitting Risk`: Without proper pruning, large trees can overfit noisy data.\n",
        "-   `Scalability`: May struggle with very large datasets or high-dimensional data compared to modern algorithms like gradient boosting.\n",
        "-   `Instability`: Small changes in the data can lead to significantly different trees."
      ],
      "metadata": {
        "id": "sLItoPE9-8uZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applications\n",
        "\n",
        "C4.5 is used in various domains, such as:\n",
        "\n",
        "-   `Medical Diagnosis`: Classifying patients based on symptoms and test results.\n",
        "-   `Credit Scoring`: Determining loan eligibility based on applicant data.\n",
        "-   `Customer Segmentation`: Categorizing customers for marketing purposes."
      ],
      "metadata": {
        "id": "odAZXcMn-_la"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup R in Python Runtype"
      ],
      "metadata": {
        "id": "DOSCCKDbDXLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install rpy2"
      ],
      "metadata": {
        "id": "IqA0SLZlDcYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall rpy2 -y\n",
        "!pip install rpy2==3.5.1\n",
        "%load_ext rpy2.ipython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RD-b5kwIDYDx",
        "outputId": "cfddfbcb-99dd-4df2-afbb-c8703c07efa3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: rpy2 3.5.1\n",
            "Uninstalling rpy2-3.5.1:\n",
            "  Successfully uninstalled rpy2-3.5.1\n",
            "Collecting rpy2==3.5.1\n",
            "  Using cached rpy2-3.5.1-cp311-cp311-linux_x86_64.whl\n",
            "Requirement already satisfied: cffi>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from rpy2==3.5.1) (1.17.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from rpy2==3.5.1) (3.1.6)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from rpy2==3.5.1) (2025.2)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.11/dist-packages (from rpy2==3.5.1) (5.3.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.10.0->rpy2==3.5.1) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->rpy2==3.5.1) (3.0.2)\n",
            "Installing collected packages: rpy2\n",
            "Successfully installed rpy2-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Google Drive"
      ],
      "metadata": {
        "id": "KqyWFjgkDgYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujT_LCWgDgl2",
        "outputId": "c7c1b742-afca-4b41-879d-10870e4db0fc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C4.5 Model from Scratch in R\n",
        "\n",
        "Implementing the C4.5 decision tree algorithm from scratch in R without using external packages requires coding the core components: calculating entropy, information gain, and gain ratio; handling categorical and continuous features; building the tree recursively; and implementing pruning. Below is a simplified implementation that focuses on the core logic of C4.5, including handling categorical features and basic pruning. For simplicity, this implementation assumes no missing values and focuses on categorical features, but it includes comments on extending to continuous features."
      ],
      "metadata": {
        "id": "556afMju_Hhw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to Calculate Entropy and Information Gain\n",
        "\n",
        "\n",
        "1. `entropy()` : Computes entropy using -$\\sum p_i \\log_2(p_i)$$, where $p_i$ is class proportion. Adds `1e-10` to avoid $\\log_2(0)$.\n",
        "\n",
        "2. `info_gain()`:\n",
        "\n",
        "   - Calculates dataset entropy.\n",
        "     - For each unique feature value:\n",
        "       - Computes subset proportion and entropy.\n",
        "       - Sums weighted entropy and split info: -$\\sum \\text{proportion} \\log_2(\\text{proportion})$.\n",
        "     - Computes gain: $\\text{total_entropy} - \\text{weighted_entropy}$.\n",
        "     - Returns gain ratio: $\\text{gain} / \\text{split_info}$ (0 if `split_info` is 0)."
      ],
      "metadata": {
        "id": "d4zqkOcn_Kw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Function to calculate entropy of a dataset\n",
        "entropy <- function(class_vector) {\n",
        "  probs <- table(class_vector) / length(class_vector)\n",
        "  -sum(probs * log2(probs + 1e-10))  # Add small constant to avoid log(0)\n",
        "}\n",
        "\n",
        "# Function to calculate information gain\n",
        "info_gain <- function(data, feature, class_col) {\n",
        "  total_entropy <- entropy(data[[class_col]])\n",
        "  feature_values <- unique(data[[feature]])\n",
        "\n",
        "  weighted_entropy <- 0\n",
        "  split_info <- 0\n",
        "  total_instances <- nrow(data)\n",
        "\n",
        "  for (val in feature_values) {\n",
        "    subset <- data[data[[feature]] == val, ]\n",
        "    proportion <- nrow(subset) / total_instances\n",
        "    weighted_entropy <- weighted_entropy + proportion * entropy(subset[[class_col]])\n",
        "    split_info <- split_info - proportion * log2(proportion + 1e-10)\n",
        "  }\n",
        "\n",
        "  gain <- total_entropy - weighted_entropy\n",
        "  if (split_info == 0) return(0)  # Avoid division by zero\n",
        "  gain_ratio <- gain / split_info\n",
        "  return(gain_ratio)\n",
        "}"
      ],
      "metadata": {
        "id": "lNAEHYWp_Od9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fuctions to Build the Decision Tree\n",
        "\n",
        "Builds a decision tree using gain ratio, predicts classes, and prunes to reduce overfitting.\n",
        "\n",
        "1. `best_feature`: Computes gain ratio for each feature, returns the best feature and its gain.\n",
        "\n",
        "2. `build_tree`:  Returns majority class if data < `min_split` or single-class. Selects best feature, splits data, and recursively builds subtrees.\n",
        "\n",
        "3. `predict_tree`: Returns leaf class or majority class if no child node. Recursively predicts using child nodes.\n",
        "\n",
        "4. `prune_tree`: ecursively prunes children. Replaces subtree with majority class if accuracy improves or equals."
      ],
      "metadata": {
        "id": "HQ4znj88_RqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Function to find the best feature to split on\n",
        "best_feature <- function(data, features, class_col) {\n",
        "  gains <- sapply(features, function(f) info_gain(data, f, class_col))\n",
        "  best_idx <- which.max(gains)\n",
        "  list(feature = features[best_idx], gain = gains[best_idx])\n",
        "}\n",
        "\n",
        "# Function to build the decision tree recursively\n",
        "build_tree <- function(data, features, class_col, min_split = 2) {\n",
        "  # Base cases\n",
        "  if (nrow(data) < min_split) {\n",
        "    return(list(class = names(sort(table(data[[class_col]]), decreasing = TRUE))[1]))\n",
        "  }\n",
        "  if (length(unique(data[[class_col]])) == 1) {\n",
        "    return(list(class = data[[class_col]][1]))\n",
        "  }\n",
        "\n",
        "  # Find best feature to split on\n",
        "  best <- best_feature(data, features, class_col)\n",
        "  if (best$gain == 0) {\n",
        "    return(list(class = names(sort(table(data[[class_col]]), decreasing = TRUE))[1]))\n",
        "  }\n",
        "\n",
        "  # Create tree node\n",
        "  tree <- list(feature = best$feature, children = list())\n",
        "\n",
        "  # Split data and recurse\n",
        "  feature_values <- unique(data[[best$feature]])\n",
        "  for (val in feature_values) {\n",
        "    subset <- data[data[[best$feature]] == val, ]\n",
        "    if (nrow(subset) > 0) {\n",
        "      tree$children[[val]] <- build_tree(subset, features, class_col, min_split)\n",
        "    }\n",
        "  }\n",
        "\n",
        "  return(tree)\n",
        "}\n",
        "\n",
        "# Function to predict using the decision tree\n",
        "predict_tree <- function(tree, instance) {\n",
        "  if (!is.null(tree$class)) {\n",
        "    return(tree$class)\n",
        "  }\n",
        "\n",
        "  feature_value <- instance[[tree$feature]]\n",
        "  if (is.null(tree$children[[feature_value]])) {\n",
        "    return(names(sort(table(instance[[tree$feature]]), decreasing = TRUE))[1])\n",
        "  }\n",
        "\n",
        "  return(predict_tree(tree$children[[feature_value]], instance))\n",
        "}\n",
        "\n",
        "# Simple pruning function (replace subtree with majority class if it improves accuracy)\n",
        "prune_tree <- function(tree, data, class_col) {\n",
        "  if (is.null(tree$children)) {\n",
        "    return(tree)\n",
        "  }\n",
        "\n",
        "  # Recursively prune children\n",
        "  for (val in names(tree$children)) {\n",
        "    tree$children[[val]] <- prune_tree(tree$children[[val]], data[data[[tree$feature]] == val, ], class_col)\n",
        "  }\n",
        "\n",
        "  # Check if pruning improves accuracy\n",
        "  majority_class <- names(sort(table(data[[class_col]]), decreasing = TRUE))[1]\n",
        "  predictions_before <- apply(data, 1, function(row) predict_tree(tree, row))\n",
        "  accuracy_before <- mean(predictions_before == data[[class_col]])\n",
        "\n",
        "  leaf_predictions <- rep(majority_class, nrow(data))\n",
        "  accuracy_after <- mean(leaf_predictions == data[[class_col]])\n",
        "\n",
        "  if (accuracy_after >= accuracy_before) {\n",
        "    return(list(class = majority_class))\n",
        "  }\n",
        "\n",
        "  return(tree)\n",
        "}"
      ],
      "metadata": {
        "id": "X8bjR_Zd_Zi9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Usage\n"
      ],
      "metadata": {
        "id": "CYAPLJG7_ccy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Example usage\n",
        "# Sample dataset (Weather data: Play Tennis)\n",
        "data <- data.frame(\n",
        "  Outlook = c(\"Sunny\", \"Sunny\", \"Overcast\", \"Rain\", \"Rain\", \"Rain\", \"Overcast\", \"Sunny\", \"Sunny\", \"Rain\", \"Sunny\", \"Overcast\", \"Overcast\", \"Rain\"),\n",
        "  Temperature = c(\"Hot\", \"Hot\", \"Hot\", \"Mild\", \"Cool\", \"Cool\", \"Cool\", \"Mild\", \"Cool\", \"Mild\", \"Mild\", \"Mild\", \"Hot\", \"Mild\"),\n",
        "  Humidity = c(\"High\", \"High\", \"High\", \"High\", \"Normal\", \"Normal\", \"Normal\", \"High\", \"Normal\", \"Normal\", \"Normal\", \"High\", \"Normal\", \"High\"),\n",
        "  Wind = c(\"Weak\", \"Strong\", \"Weak\", \"Weak\", \"Weak\", \"Strong\", \"Strong\", \"Weak\", \"Weak\", \"Weak\", \"Strong\", \"Strong\", \"Weak\", \"Strong\"),\n",
        "  Play = c(\"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\")\n",
        ")\n",
        "\n",
        "# Define features and class column\n",
        "features <- c(\"Outlook\", \"Temperature\", \"Humidity\", \"Wind\")\n",
        "class_col <- \"Play\"\n",
        "\n",
        "# Build and prune tree\n",
        "tree <- build_tree(data, features, class_col)\n",
        "pruned_tree <- prune_tree(tree, data, class_col)\n",
        "\n",
        "# Print tree structure\n",
        "print_tree <- function(tree, indent = \"\") {\n",
        "  if (!is.null(tree$class)) {\n",
        "    cat(indent, \"Class:\", tree$class, \"\\n\")\n",
        "  } else {\n",
        "    cat(indent, \"Feature:\", tree$feature, \"\\n\")\n",
        "    for (val in names(tree$children)) {\n",
        "      cat(indent, \"  Value:\", val, \"\\n\")\n",
        "      print_tree(tree$children[[val]], paste0(indent, \"    \"))\n",
        "    }\n",
        "  }\n",
        "}\n",
        "cat(\"Pruned Decision Tree:\\n\")\n",
        "print_tree(pruned_tree)\n",
        "\n",
        "# Make a prediction\n",
        "new_instance <- data.frame(\n",
        "  Outlook = \"Sunny\",\n",
        "  Temperature = \"Hot\",\n",
        "  Humidity = \"High\",\n",
        "  Wind = \"Weak\"\n",
        ")\n",
        "prediction <- predict_tree(pruned_tree, new_instance)\n",
        "cat(\"\\nPrediction for new instance:\", prediction, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usqzNnRa_gIN",
        "outputId": "32f1ff41-1f20-4b50-acb5-6e67323d9483"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned Decision Tree:\n",
            " Feature: Outlook \n",
            "   Value: Sunny \n",
            "     Feature: Humidity \n",
            "       Value: High \n",
            "         Class: No \n",
            "       Value: Normal \n",
            "         Class: Yes \n",
            "   Value: Overcast \n",
            "     Class: Yes \n",
            "   Value: Rain \n",
            "     Feature: Wind \n",
            "       Value: Weak \n",
            "         Class: Yes \n",
            "       Value: Strong \n",
            "         Class: No \n",
            "\n",
            "Prediction for new instance: No \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C4.5 Model in R\n",
        "\n",
        "To implement the C4.5 and C5.0 decision tree algorithms in R using an existing package, the most suitable package is {RWeka}, which provides an interface to the Weka machine learning library and includes the **J48** algorithm, Weka's implementation of C4.5. Below, Iâ€™ll guide you step-by-step through the process of installing the package, preparing a dataset, training a C4.5 model, evaluating it, and making predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "hKCkWWbSCpfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Google Colab, I encountered a Java runtime error while attempting to run the {RWeka} library, which prevented me from utilizing its functionality. To work around this issue, I developed my own function that mimics the functionality of the `J48()` function, allowing for the generation of both unpruned and pruned C4.5 decision trees. Additionally, for the purpose of performing cross-validation on my decision tree models, I implemented a function called `evaluate_Weka_classifier()`, which assesses the performance of the classifier using various metrics to ensure robust validation of the models."
      ],
      "metadata": {
        "id": "hAIfgGAAVhJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check amd Install Other Required R Packages\n",
        "\n",
        "Following R packages are required to run this notebook. If any of these packages are not installed, you can install them using the code below:"
      ],
      "metadata": {
        "id": "ptRSvSvyCzD1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "w-nv_puDCUoe"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "packages <- c('tidyverse',\n",
        "              'plyr',\n",
        "              'rpart',\n",
        "              'rpart.plot',\n",
        "              'partykit',\n",
        "              'mlbench',\n",
        "              'ggparty',\n",
        "              'nnet',\n",
        "              'sandwich'\n",
        "         )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Install Missing Packages"
      ],
      "metadata": {
        "id": "G1a1YHy9C5RV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Install missing packages\n",
        "new.packages <- packages[!(packages %in% installed.packages(lib='drive/My Drive/R/')[,\"Package\"])]\n",
        "if(length(new.packages)) install.packages(new.packages, lib='drive/My Drive/R/')"
      ],
      "metadata": {
        "id": "LquHXSirC5ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Verify Installation"
      ],
      "metadata": {
        "id": "sOLZAxTmC_Z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# set library path\n",
        ".libPaths('drive/My Drive/R')\n",
        "# Verify installation\n",
        "cat(\"Installed packages:\\n\")\n",
        "print(sapply(packages, requireNamespace, quietly = TRUE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YskwGEcDC_in",
        "outputId": "c37ea72c-1c0e-498a-afdb-7b748cf3f561"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installed packages:\n",
            " tidyverse       plyr      rpart rpart.plot   partykit    mlbench    ggparty \n",
            "      TRUE       TRUE       TRUE       TRUE       TRUE       TRUE       TRUE \n",
            "      nnet   sandwich \n",
            "      TRUE       TRUE \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load R Packages"
      ],
      "metadata": {
        "id": "k2vFs39qDFVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# set library path\n",
        ".libPaths('drive/My Drive/R')\n",
        "# Load packages with suppressed messages\n",
        "invisible(lapply(packages, function(pkg) {\n",
        "  suppressPackageStartupMessages(library(pkg, character.only = TRUE))\n",
        "}))\n"
      ],
      "metadata": {
        "id": "i_6as7QMDFgE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check Loaded Packages"
      ],
      "metadata": {
        "id": "4uwQzcFUDLsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Check loaded packages\n",
        "cat(\"Successfully loaded packages:\\n\")\n",
        "print(search()[grepl(\"package:\", search())])# Check loaded packageswer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaCSw0l9DL2F",
        "outputId": "b9d77de8-a035-4dc6-d82e-0eb994a965bf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded packages:\n",
            "[1] \"package:tools\"     \"package:stats\"     \"package:graphics\" \n",
            "[4] \"package:grDevices\" \"package:utils\"     \"package:datasets\" \n",
            "[7] \"package:methods\"   \"package:base\"     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Create `J48()` function"
      ],
      "metadata": {
        "id": "vkIrf9AyBFvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Create J48() function\n",
        "entropy <- function(class_vector) {\n",
        "  probs <- table(class_vector) / length(class_vector)\n",
        "  -sum(probs * log2(probs + 1e-10))\n",
        "}\n",
        "\n",
        "info_gain <- function(data, feature, class_col) {\n",
        "  total_entropy <- entropy(data[[class_col]])\n",
        "  feature_values <- unique(data[[feature]])\n",
        "  weighted_entropy <- 0\n",
        "  split_info <- 0\n",
        "  total_instances <- nrow(data)\n",
        "  for (val in feature_values) {\n",
        "    subset <- data[data[[feature]] == val, ]\n",
        "    proportion <- nrow(subset) / total_instances\n",
        "    weighted_entropy <- weighted_entropy + proportion * entropy(subset[[class_col]])\n",
        "    split_info <- split_info - proportion * log2(proportion + 1e-10)\n",
        "  }\n",
        "  gain <- total_entropy - weighted_entropy\n",
        "  if (split_info == 0) return(0)\n",
        "  gain_ratio <- gain / split_info\n",
        "  return(gain_ratio)\n",
        "}\n",
        "\n",
        "best_feature <- function(data, features, class_col) {\n",
        "  gains <- sapply(features, function(f) info_gain(data, f, class_col))\n",
        "  best_idx <- which.max(gains)\n",
        "  list(feature = features[best_idx], gain = gains[best_idx])\n",
        "}\n",
        "\n",
        "build_tree <- function(data, features, class_col, min_split = 2, confidence = 0.25) {\n",
        "  if (nrow(data) < min_split) {\n",
        "    return(list(class = names(sort(table(data[[class_col]]), decreasing = TRUE))[1]))\n",
        "  }\n",
        "  if (length(unique(data[[class_col]])) == 1) {\n",
        "    return(list(class = data[[class_col]][1]))\n",
        "  }\n",
        "  best <- best_feature(data, features, class_col)\n",
        "  if (best$gain == 0) {\n",
        "    return(list(class = names(sort(table(data[[class_col]]), decreasing = TRUE))[1]))\n",
        "  }\n",
        "  tree <- list(feature = best$feature, children = list())\n",
        "  feature_values <- unique(data[[best$feature]])\n",
        "  for (val in feature_values) {\n",
        "    subset <- data[data[[best$feature]] == val, ]\n",
        "    if (nrow(subset) > 0) {\n",
        "      tree$children[[val]] <- build_tree(subset, features, class_col, min_split, confidence)\n",
        "    }\n",
        "  }\n",
        "  return(tree)\n",
        "}\n",
        "\n",
        "predict_tree <- function(tree, instance) {\n",
        "  if (!is.null(tree$class)) {\n",
        "    return(tree$class)\n",
        "  }\n",
        "  feature_value <- instance[[tree$feature]]\n",
        "  if (is.null(tree$children[[feature_value]])) {\n",
        "    return(names(sort(table(instance[[tree$feature]]), decreasing = TRUE))[1])\n",
        "  }\n",
        "  return(predict_tree(tree$children[[feature_value]], instance))\n",
        "}\n",
        "\n",
        "prune_tree <- function(tree, data, class_col, confidence = 0.25) {\n",
        "  if (is.null(tree$children)) {\n",
        "    return(tree)\n",
        "  }\n",
        "  for (val in names(tree$children)) {\n",
        "    tree$children[[val]] <- prune_tree(tree$children[[val]], data[data[[tree$feature]] == val, ], class_col, confidence)\n",
        "  }\n",
        "  majority_class <- names(sort(table(data[[class_col]]), decreasing = TRUE))[1]\n",
        "  predictions_before <- apply(data, 1, function(row) predict_tree(tree, row))\n",
        "  accuracy_before <- mean(predictions_before == data[[class_col]])\n",
        "  leaf_predictions <- rep(majority_class, nrow(data))\n",
        "  accuracy_after <- mean(leaf_predictions == data[[class_col]])\n",
        "  # Apply confidence factor for pruning decision\n",
        "  if (accuracy_after >= accuracy_before * (1 - confidence)) {\n",
        "    return(list(class = majority_class))\n",
        "  }\n",
        "  return(tree)\n",
        "}\n",
        "\n",
        "J48 <- function(formula, data, control = list(C = 0.25, M = 2)) {\n",
        "  terms <- terms(formula, data = data)\n",
        "  response <- all.vars(formula)[1]\n",
        "  predictors <- attr(terms, \"term.labels\")\n",
        "  min_split <- control$M\n",
        "  confidence <- control$C\n",
        "  tree <- build_tree(data, predictors, response, min_split, confidence)\n",
        "  pruned_tree <- prune_tree(tree, data, response, confidence)\n",
        "  model <- list(tree = pruned_tree, formula = formula, call = match.call(), class_col = response, predictors = predictors, data = data)\n",
        "  class(model) <- \"J48\"\n",
        "  return(model)\n",
        "}\n",
        "\n",
        "predict.J48 <- function(object, newdata) {\n",
        "  apply(newdata, 1, function(row) predict_tree(object$tree, row))\n",
        "}"
      ],
      "metadata": {
        "id": "SPGr32lYO1fx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create `evaluate_J48_classifier()` function"
      ],
      "metadata": {
        "id": "PTX_8T9QBRdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Create evaluate_J48_classifier() function\n",
        "evaluate_J48_classifier <- function(model, numFolds = 10, seed = NULL) {\n",
        "  data <- model$data\n",
        "  formula <- model$formula\n",
        "  class_col <- model$class_col\n",
        "  if (!is.null(seed)) set.seed(seed)\n",
        "  n <- nrow(data)\n",
        "  fold_size <- floor(n / numFolds)\n",
        "  indices <- sample(1:n)\n",
        "  folds <- split(indices, rep(1:numFolds, each = fold_size, length.out = n))\n",
        "  all_predictions <- rep(NA, n)\n",
        "  all_actual <- data[[class_col]]\n",
        "  for (i in 1:numFolds) {\n",
        "    test_indices <- folds[[i]]\n",
        "    train_indices <- unlist(folds[-i])\n",
        "    train_data <- data[train_indices, ]\n",
        "    test_data <- data[test_indices, ]\n",
        "    fold_model <- J48(formula, train_data, control = list(C = model$call$control$C, M = model$call$control$M))\n",
        "    predictions <- predict(fold_model, test_data)\n",
        "    all_predictions[test_indices] <- predictions\n",
        "  }\n",
        "  conf_matrix <- table(Predicted = all_predictions, Actual = all_actual)\n",
        "  accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)\n",
        "  result <- list(confusionMatrix = conf_matrix, accuracy = accuracy, details = paste(numFolds, \"-fold cross-validation with seed =\", seed))\n",
        "  class(result) <- \"J48_evaluation\"\n",
        "  return(result)\n",
        "}\n",
        "\n",
        "print.J48_evaluation <- function(x) {\n",
        "  cat(\"=== J48 Cross-Validation Evaluation ===\\n\")\n",
        "  cat(\"Details:\", x$details, \"\\n\")\n",
        "  cat(\"Confusion Matrix:\\n\")\n",
        "  print(x$confusionMatrix)\n",
        "  cat(\"\\nAccuracy:\", round(x$accuracy * 100, 2), \"%\\n\")\n",
        "}"
      ],
      "metadata": {
        "id": "h4JAQHyFZkIi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Dataset"
      ],
      "metadata": {
        "id": "yNp4JmJBBY3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# create synthetic data\n",
        "data <- data.frame(\n",
        "  Outlook = c(\"Sunny\", \"Sunny\", \"Overcast\", \"Rain\", \"Rain\", \"Rain\", \"Overcast\", \"Sunny\", \"Sunny\", \"Rain\", \"Sunny\", \"Overcast\", \"Overcast\", \"Rain\"),\n",
        "  Temperature = c(\"Hot\", \"Hot\", \"Hot\", \"Mild\", \"Cool\", \"Cool\", \"Cool\", \"Mild\", \"Cool\", \"Mild\", \"Mild\", \"Mild\", \"Hot\", \"Mild\"),\n",
        "  Humidity = c(\"High\", \"High\", \"High\", \"High\", \"Normal\", \"Normal\", \"Normal\", \"High\", \"Normal\", \"Normal\", \"Normal\", \"High\", \"Normal\", \"High\"),\n",
        "  Wind = c(\"Weak\", \"Strong\", \"Weak\", \"Weak\", \"Weak\", \"Strong\", \"Strong\", \"Weak\", \"Weak\", \"Weak\", \"Strong\", \"Strong\", \"Weak\", \"Strong\"),\n",
        "  Play = c(\"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\")\n",
        "\n",
        ")\n",
        "\n",
        "# Convert categorical variables to factors\n",
        "data$Outlook <- as.factor(data$Outlook)\n",
        "data$Temperature <- as.factor(data$Temperature)\n",
        "data$Humidity <- as.factor(data$Humidity)\n",
        "data$Wind <- as.factor(data$Wind)\n",
        "data$Play <- as.factor(data$Play)\n",
        "\n",
        "# View the dataset\n",
        "head(data)"
      ],
      "metadata": {
        "id": "Z_fkw7p6Z9fU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eacae40-1e4c-44a5-dc1c-419775552d81"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Outlook Temperature Humidity   Wind Play\n",
            "1    Sunny         Hot     High   Weak   No\n",
            "2    Sunny         Hot     High Strong   No\n",
            "3 Overcast         Hot     High   Weak  Yes\n",
            "4     Rain        Mild     High   Weak  Yes\n",
            "5     Rain        Cool   Normal   Weak  Yes\n",
            "6     Rain        Cool   Normal Strong   No\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train CR4.5 Model"
      ],
      "metadata": {
        "id": "w_xj_f0iBeeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Train the model with J48\n",
        "model <- J48(Play ~ Outlook + Temperature + Humidity + Wind, data = data)\n",
        "model"
      ],
      "metadata": {
        "id": "CoeB0sSdaCxG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6513354c-ea5d-4ccb-8915-536e7e12da29"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$tree\n",
            "$tree$feature\n",
            "[1] \"Outlook\"\n",
            "\n",
            "$tree$children\n",
            "$tree$children$Sunny\n",
            "$tree$children$Sunny$feature\n",
            "[1] \"Humidity\"\n",
            "\n",
            "$tree$children$Sunny$children\n",
            "$tree$children$Sunny$children$High\n",
            "$tree$children$Sunny$children$High$class\n",
            "[1] No\n",
            "Levels: No Yes\n",
            "\n",
            "\n",
            "$tree$children$Sunny$children$Normal\n",
            "$tree$children$Sunny$children$Normal$class\n",
            "[1] Yes\n",
            "Levels: No Yes\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "$tree$children$Overcast\n",
            "$tree$children$Overcast$class\n",
            "[1] Yes\n",
            "Levels: No Yes\n",
            "\n",
            "\n",
            "$tree$children$Rain\n",
            "$tree$children$Rain$feature\n",
            "[1] \"Wind\"\n",
            "\n",
            "$tree$children$Rain$children\n",
            "$tree$children$Rain$children$Weak\n",
            "$tree$children$Rain$children$Weak$class\n",
            "[1] Yes\n",
            "Levels: No Yes\n",
            "\n",
            "\n",
            "$tree$children$Rain$children$Strong\n",
            "$tree$children$Rain$children$Strong$class\n",
            "[1] No\n",
            "Levels: No Yes\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "$formula\n",
            "Play ~ Outlook + Temperature + Humidity + Wind\n",
            "\n",
            "$call\n",
            "J48(formula = Play ~ Outlook + Temperature + Humidity + Wind, \n",
            "    data = data)\n",
            "\n",
            "$class_col\n",
            "[1] \"Play\"\n",
            "\n",
            "$predictors\n",
            "[1] \"Outlook\"     \"Temperature\" \"Humidity\"    \"Wind\"       \n",
            "\n",
            "$data\n",
            "    Outlook Temperature Humidity   Wind Play\n",
            "1     Sunny         Hot     High   Weak   No\n",
            "2     Sunny         Hot     High Strong   No\n",
            "3  Overcast         Hot     High   Weak  Yes\n",
            "4      Rain        Mild     High   Weak  Yes\n",
            "5      Rain        Cool   Normal   Weak  Yes\n",
            "6      Rain        Cool   Normal Strong   No\n",
            "7  Overcast        Cool   Normal Strong  Yes\n",
            "8     Sunny        Mild     High   Weak   No\n",
            "9     Sunny        Cool   Normal   Weak  Yes\n",
            "10     Rain        Mild   Normal   Weak  Yes\n",
            "11    Sunny        Mild   Normal Strong  Yes\n",
            "12 Overcast        Mild     High Strong  Yes\n",
            "13 Overcast         Hot   Normal   Weak  Yes\n",
            "14     Rain        Mild     High Strong   No\n",
            "\n",
            "attr(,\"class\")\n",
            "[1] \"J48\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction and Evaluation"
      ],
      "metadata": {
        "id": "VAAtXWGnBqqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Evaluate the Model\n",
        "# Make predictions on the training data\n",
        "predictions <- predict(model, newdata = data)\n",
        "\n",
        "# Create a confusion matrix\n",
        "conf_matrix <- table(Predicted = predictions, Actual = data$Play)\n",
        "print(conf_matrix)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)\n",
        "cat(\"Accuracy:\", accuracy, \"\\n\")"
      ],
      "metadata": {
        "id": "ITgwbLRXaZtm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "520b4024-d166-4660-b649-f66dc32d98d3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Actual\n",
            "Predicted No Yes\n",
            "      No   5   0\n",
            "      Yes  0   9\n",
            "Accuracy: 1 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Make Predictions on New Data\n",
        "# Create a new instance\n",
        "new_instance <- data.frame(\n",
        "  Outlook = \"Sunny\",\n",
        "  Temperature = \"Hot\",\n",
        "  Humidity = \"High\",\n",
        "  Wind = \"Weak\"\n",
        ")\n",
        "\n",
        "# Ensure factors have the same levels as the training data\n",
        "new_instance$Outlook <- factor(new_instance$Outlook, levels = levels(data$Outlook))\n",
        "new_instance$Temperature <- factor(new_instance$Temperature, levels = levels(data$Temperature))\n",
        "new_instance$Humidity <- factor(new_instance$Humidity, levels = levels(data$Humidity))\n",
        "new_instance$Wind <- factor(new_instance$Wind, levels = levels(data$Wind))\n",
        "\n",
        "# Make prediction\n",
        "prediction <- predict(model, newdata = new_instance)\n",
        "cat(\"Prediction for new instance:\", as.character(prediction), \"\\n\")"
      ],
      "metadata": {
        "id": "oxhTZVpHas_g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5b5fd7f-1ab6-4039-fa5a-fed534385974"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for new instance: No \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Tune Model Parameters\n",
        "model_tuned <- J48(Play~ ., data = data, control = list(C = 0.05, M = 3))\n",
        "eval_tuned <- evaluate_J48_classifier(model_tuned, numFolds = 10, seed = 1)\n",
        "cat(\"\\nTuned Model Evaluation:\\n\")\n",
        "print(eval_tuned)"
      ],
      "metadata": {
        "id": "YpMIojGcbTwo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10dfc622-f811-4e6e-dd2e-cf3acc33d022"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tuned Model Evaluation:\n",
            "=== J48 Cross-Validation Evaluation ===\n",
            "Details: 10 -fold cross-validation with seed = 1 \n",
            "Confusion Matrix:\n",
            "         Actual\n",
            "Predicted No Yes\n",
            "      No   0   2\n",
            "      Yes  5   7\n",
            "\n",
            "Accuracy: 50 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "#Cross-validation with evaluate_J48_classifier\n",
        "# Redefine the evaluate_J48_classifier function to handle empty folds\n",
        "evaluate_J48_classifier <- function(model, numFolds, seed) {\n",
        "  require(caret)\n",
        "  set.seed(seed)\n",
        "  folds <- createFolds(model$data$Play, k = numFolds)\n",
        "  evaluations <- list()\n",
        "\n",
        "  for (i in 1:numFolds) {\n",
        "    train_data <- model$data[-folds[[i]], ]\n",
        "    test_data <- model$data[folds[[i]], ]\n",
        "\n",
        "    # Check if train_data and test_data are not empty before evaluating\n",
        "    if (nrow(train_data) > 0 && nrow(test_data) > 0) {\n",
        "      model_fold <- J48(Play ~ ., data = train_data)\n",
        "      predictions <- predict(model_fold, newdata = test_data)\n",
        "      conf_matrix <- table(Predicted = predictions, Actual = test_data$Play)\n",
        "      accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)\n",
        "      evaluations[[i]] <- list(conf_matrix = conf_matrix, accuracy = accuracy)\n",
        "    } else {\n",
        "      # Handle empty fold by skipping\n",
        "      warning(paste(\"Fold\", i, \"has empty train or test data, skipping evaluation for this fold.\"))\n",
        "      evaluations[[i]] <- NULL\n",
        "    }\n",
        "  }\n",
        "  # You might want to process or summarize the evaluations list here\n",
        "  return(evaluations)\n",
        "}\n",
        "\n",
        "eval <- evaluate_J48_classifier(model, numFolds = 10, seed = 1)\n",
        "print(eval)"
      ],
      "metadata": {
        "id": "thxpd8gJEmHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Viasualize the Decision Tree"
      ],
      "metadata": {
        "id": "iuZR374UBkzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Visualize the Decision Tree with partykit\n",
        "as.party.J48 <- function(model) {\n",
        "  tree <- model$tree\n",
        "  party_tree <- function(node, id = 1) {\n",
        "    if (is.null(node$children)) {\n",
        "      return(partynode(id = as.integer(id), info = node$class))\n",
        "    }\n",
        "    kids <- list()\n",
        "    for (val in names(node$children)) {\n",
        "      kids[[val]] <- party_tree(node$children[[val]], paste(id, val, sep = \".\"))\n",
        "    }\n",
        "    split <- partysplit(varid = which(model$predictors == node$feature),\n",
        "                        breaks = NULL, index = seq_along(kids),\n",
        "                        info = node$feature)\n",
        "    return(partynode(id = as.integer(id), split = split, kids = kids))\n",
        "  }\n",
        "  return(party(party_tree(tree), data = model$data))\n",
        "}\n",
        "plot(as.party.J48(model))"
      ],
      "metadata": {
        "id": "DruuolAwaTOF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "06e70cd4-62f7-4b0f-863b-76d2bd2aea6c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary and Conclusion\n",
        "\n",
        "C4.5 is a robust and interpretable decision tree algorithm that improves on ID3 by using the gain ratio, handling continuous features, managing missing values, and applying pruning. While it has been largely superseded by ensemble methods in high-performance applications, it remains a foundational algorithm in machine learning, valued for its simplicity and clarity. In R, the `RWeka` package provides an easy way to implement C4.5 through the `J48` function, allowing users to build, visualize, and evaluate decision trees effectively. This tutorial show a brief overview of the C4.5 algorithm, its implementation in R, and how to use it for classification tasks.\n"
      ],
      "metadata": {
        "id": "p7HCJ0xeFL6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "1.  R. Quinlan (1993). C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers, San Mateo, CA.\n",
        "\n",
        "2.  Witten, I. H., Frank, E., & Hall, M. A. (2016). Data Mining: Practical Machine Learning Tools and Techniques (4th ed.). Morgan Kaufmann.\n",
        "\n",
        "3.  Hornik, K., Buchta, C., & Zeileis, A. (2009). Open-Source Machine Learning: R Meets Weka. Computational Statistics, 24(2), 225â€“232\n",
        "\n",
        "4.  Kuhn, M., & Quinlan, R. (2023). C50: C5.0 Decision Trees and Rule-Based Models. CRAN. Official C50 package documentation: CRAN C50."
      ],
      "metadata": {
        "id": "KXHd8rAHFR2v"
      }
    }
  ]
}