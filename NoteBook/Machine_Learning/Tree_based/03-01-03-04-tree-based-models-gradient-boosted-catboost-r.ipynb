{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-04-tree-based-models-gradient-boosted-catboost-r.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qrItz_mJNWw"
      },
      "source": [
        "![alt text](http://drive.google.com/uc?export=view&id=1bLQ3nhDbZrCCqy_WCxxckOne2lgVvn3l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dzp9ZseROTcY"
      },
      "source": [
        "# 3.4 Categorical Boosting (CatBoost)\n",
        "\n",
        "CatBoost (Categorical Boosting) is an open-source machine learning algorithm developed by Yandex, designed for gradient boosting on decision trees. It excels in handling categorical features efficiently and is particularly effective for tabular data tasks, such as classification, regression, and ranking. CatBoost is known for its high performance, ease of use, and ability to work well with default parameters, reducing the need for extensive hyperparameter tuning. This tutorial provides a detailed overview of CatBoost, including its mathematical foundations, key features, and practical implementation steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLkAu88QToVr"
      },
      "source": [
        "## Overview\n",
        "\n",
        "CatBoost is a gradient boosting library that is particularly effective for categorical features. It handles categorical variables automatically, which can save time and improve model performance. It is designed to work with large datasets and can be used for both classification and regression tasks. It is known for its speed and accuracy, making it a popular choice for machine learning practitioners. It is available in Python, R, and other programming languages. It is particularly useful for datasets with a large number of categorical features, as it can handle them without the need for extensive preprocessing. It is also designed to be robust against overfitting, making it a reliable choice for many machine learning tasks. CatBoost uses a technique called \"ordered boosting\" to improve the accuracy of the model and reduce overfitting. It also includes features such as support for missing values, automatic hyperparameter tuning, and the ability to handle large datasets efficiently. It is widely used in industry and has been shown to achieve state-of-the-art results on many benchmark datasets. CatBoost is particularly effective for tasks such as click-through rate prediction, fraud detection, and recommendation systems. It is also known for its ease of use, with a simple API that allows users to quickly build and deploy models. Overall, CatBoost is a powerful tool for machine learning practitioners looking to build accurate and efficient models with categorical data.\\`\n",
        "\n",
        "CatBoost (Categorical Boosting) is an open-source machine learning algorithm developed by Yandex, designed for gradient boosting on decision trees. It excels in handling categorical features efficiently and is particularly effective for tabular data tasks, such as classification, regression, and ranking. CatBoost is known for its high performance, ease of use, and ability to work well with default parameters, reducing the need for extensive hyperparameter tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL2L7JRduO3P"
      },
      "source": [
        "### Key Features of CatBoost\n",
        "\n",
        "Key features of CatBoost include:\n",
        "\n",
        "-   Native handling of categorical features: Automatically processes categorical variables without requiring extensive preprocessing (e.g., one-hot encoding).\n",
        "\n",
        "-   Robust to overfitting: Incorporates techniques like ordered boosting to prevent overfitting.\n",
        "\n",
        "-   Support for GPU acceleration: Enables faster training on large datasets.\n",
        "\n",
        "-   High performance: Often outperforms other gradient boosting libraries like XGBoost and LightGBM in terms of speed and accuracy on datasets with categorical features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClfC-kf6IgzO"
      },
      "source": [
        "### How CatBoost Works\n",
        "\n",
        "CatBoost builds on the principles of gradient boosting, where an ensemble of weak learners (typically decision trees) is combined to create a strong predictive model. It iteratively builds trees to minimize a loss function, with each tree correcting the errors of the previous ones. Below is an explanation of how CatBoost works, focusing on its unique aspects:\n",
        "\n",
        "1.  Initialize the Model\n",
        "\n",
        "CatBoost starts by initializing a model with a constant prediction for all data points, typically the mean of the target variable (for regression) or the log-odds (for classification).\n",
        "\n",
        "For `regression`:\n",
        "\n",
        "$$ F_0(x) = \\arg\\min_{\\gamma} \\sum_{i=1}^n L(y_i, \\gamma) $$\n",
        "\n",
        "Where:\n",
        "\n",
        "-   $F_0(x)$: Initial model prediction (constant for all inputs \\$x \\$).\n",
        "\n",
        "-   $y_i$: True target value for the $i$-th data point.\n",
        "\n",
        "-   $L(y_i, \\gamma)$): Loss function (e.g., mean squared error, $L(y_i, \\gamma) = (y_i - \\gamma)^2$.\n",
        "\n",
        "Thsi is often simplifies to:\n",
        "\n",
        "$$ F_0(x) = \\frac{1}{n} \\sum_{i=1}^n y_i $$\n",
        "\n",
        "For `classification` (e.g., binary classification with log loss):\n",
        "\n",
        "$$ F_0(x) = \\log\\left(\\frac{\\sum_{i:y_i=1} 1}{\\sum_{i:y_i=0} 1}\\right) $$\n",
        "\n",
        "This represents the initial log-odds based on the class distribution.\n",
        "\n",
        "2.  Handle Categorical Features (Ordered Target Encoding)\n",
        "\n",
        "CatBoost processes categorical features by converting them into numerical values using **ordered target encoding**, which computes statistics based on the target variable while avoiding data leakage.\n",
        "\n",
        "-   For a categorical feature $x_k$ with categories $\\{c_1, c_2, \\dots, c_m\\}$, CatBoost calculates a target statistic (e.g., mean target value) for each category.\n",
        "\n",
        "-   To prevent leakage, it uses only prior data points in a random permutation of the dataset.\n",
        "\n",
        "For a categorical feature value $c$ at data point $i$:\n",
        "\n",
        "$$ x_{i,k}^{\\text{encoded}} = \\frac{\\sum_{j \\in P_i, x_{j,k}=c} y_j + \\alpha \\cdot \\bar{y}}{\\sum_{j \\in P_i, x_{j,k}=c} 1 + \\alpha} $$\n",
        "\n",
        "Where:\n",
        "\n",
        "-   $x_{i,k}^{\\text{encoded}}$: Encoded value of the categorical feature $x_k$ for the $i$-th data point.\n",
        "\n",
        "-   $P_i$: Set of data points before $i$ in the random permutation.\n",
        "\n",
        "-   $y_j$: Target value for the $j$-th data point.\n",
        "\n",
        "-   $\\bar{y}$: Global mean of the target (prior).\n",
        "\n",
        "-   $\\alpha$: Smoothing parameter to balance the category-specific mean and the global mean (prevents overfitting for rare categories).\n",
        "\n",
        "This encoding ensures that the model uses only past data to compute statistics, mimicking the inference-time scenario and reducing bias.\n",
        "\n",
        "3.  Compute Residuals (Gradients)\n",
        "\n",
        "CatBoost uses gradient boosting, where each iteration builds a new tree to fit the residuals (or gradients) of the loss function from the previous iteration. At each iteration \\$m, CatBoost computes the gradient of the loss function with respect to the current model's predictions to determine the direction for the next tree.\n",
        "\n",
        "Gradient for the $i$-th data point):\n",
        "\n",
        "$$ g_i^{(m)} = -\\frac{\\partial L(y_i, F_{m-1}(x_i))}{\\partial F_{m-1}(x_i)} $$\n",
        "\n",
        "Where:\n",
        "\n",
        "-   $g_i^{(m)}$: Gradient for the $i$-th data point at iteration $m$.\n",
        "\n",
        "-   $F_{m-1}(x_i)$: Current model prediction for the $i$-th data point after $m-1$ iterations.\n",
        "\n",
        "-   $L(y_i, F_{m-1}(x_i))$: Loss function (e.g., for regression, $L = (y_i - F_{m-1}(x_i))^2$; for binary classification, $L = -[y_i \\log(\\sigma(F_{m-1}(x_i))) + (1-y_i) \\log(1-\\sigma(F_{m-1}(x_i)))]$, where $sigma$ is the sigmoid function).\n",
        "\n",
        "For mean squared error in regression:\n",
        "\n",
        "$$ g_i^{(m)} = y_i - F_{m-1}(x_i) $$\n",
        "\n",
        "For log loss in binary classification:\n",
        "\n",
        "$$ g_i^{(m)} = \\sigma(F_{m-1}(x_i)) - y_i $$\n",
        "\n",
        "Where $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
        "\n",
        "4.  Ordered Boosting\n",
        "\n",
        "To reduce overfitting, CatBoost uses **ordered boosting**, where gradients for each data point are computed using a model trained on prior data points in a random permutation. This avoids using the same data for gradient computation and tree building.\n",
        "\n",
        "-   The dataset is split into a random permutation, and for each data point $i$, a separate model is trained on data points before $i$ in the permutation.\n",
        "\n",
        "-   The gradient $g_i^{(m)}$ is computed using this separate model.\n",
        "\n",
        "Gradient for ordered boosting:\n",
        "\n",
        "Where:\n",
        "\n",
        "-   $F_{m-1}^{(-i)}(x_i)$: Prediction for the $i$-th data point using a model trained on data points before $i$ in the permutation.\n",
        "\n",
        "This ensures that the gradient is unbiased, as it does not use the current data point's information, reducing overfitting.\n",
        "\n",
        "5. Build a Decision Tree\n",
        "\n",
        "CatBoost constructs a decision tree to fit the gradients $g_i^{(m)}$. It uses **symmetric trees** (balanced trees with the same split condition at each level) for efficiency and consistency.\n",
        "\n",
        "Tree output for a leaf:\n",
        "\n",
        "$$ h_m(x) = \\arg\\min_{\\gamma} \\sum_{i \\in R_j} L(y_i, F_{m-1}(x_i) + \\gamma) $$\n",
        "\n",
        "Where:\n",
        "\n",
        "-   $h_m(x)$: Output of the $m$-th tree (a piecewise constant function based on the leaf $R_j$ that $x$ falls into).\n",
        "\n",
        "-   $R_j$: $j$-th leaf region of the tree.\n",
        "\n",
        "-   $\\gamma$: Optimal leaf value, computed to minimize the loss.\n",
        "\n",
        "For mean squared error:\n",
        "\n",
        "$$ h_m(x_i) = \\frac{\\sum_{i \\in R_j} g_i^{(m)}}{\\sum_{i \\in R_j} 1} $$\n",
        "\n",
        "This is the average gradient in the leaf region.\n",
        "\n",
        "CatBoost uses histogram-based splitting for numerical features and the encoded values from Step 2 for categorical features to determine the best splits.\n",
        "\n",
        "6.  Update the Model\n",
        "\n",
        "The new tree is added to the ensemble, scaled by a learning rate to control the contribution of each tree.\n",
        "\n",
        "$$ F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x) $$\n",
        "\n",
        "Where:\n",
        "\n",
        "-   $F_m(x)$: Updated model prediction after $m$ iterations.\n",
        "\n",
        "-   $\\eta$: Learning rate (typically small, e.g., 0.01–0.3).\n",
        "\n",
        "-   $h_m(x)$: Output of the $m$-th tree.\n",
        "\n",
        "7.  Repeat Until Convergence\n",
        "\n",
        "Steps 3–6 are repeated for a specified number of iterations ($M$ ) or until early stopping is triggered (based on validation performance). The final model is:\n",
        "\n",
        "$$ F(x) = F_0(x) + \\eta \\sum_{m=1}^M h_m(x) $$\n",
        "\n",
        "7.  Prediction\n",
        "\n",
        "For a new data point $x$, CatBoost applies the same categorical encoding (using statistics computed during training) and passes the input through the ensemble of trees.\n",
        "\n",
        "$$ \\hat{y} = F(x) = F_0(x) + \\eta \\sum_{m=1}^M h_m(x) $$\n",
        "\n",
        "For classification, the output is passed through a sigmoid function (binary) or softmax (multiclass) to obtain probabilities:\n",
        "\n",
        "$$ p(x) = \\sigma(F(x)) \\quad \\text{or} \\quad p_k(x) = \\frac{e^{F_k(x)}}{\\sum_{j} e^{F_j(x)}} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWCTcHkzvHE2"
      },
      "source": [
        "The flowchart below summarizes the CatBoost algorithm, highlighting its key steps from initialization to prediction:\n",
        "\n",
        "\n",
        "![alt text](http://drive.google.com/uc?export=view&id=1N0gmbjTlKUf5O26bpDwR7VGDndEMMVbi\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuhrbXQxGIIy"
      },
      "source": [
        "### Advantages of CatBoost\n",
        "\n",
        "- `Native Categorical Handling`: Processes categorical features without manual encoding.\n",
        "- `Robust to Overfitting`: Uses ordered boosting and symmetric trees.\n",
        "- `High Performance`: Often outperforms XGBoost/LightGBM on categorical data.\n",
        "- `Minimal Tuning`: Good default settings reduce hyperparameter effort.\n",
        "- `Feature Interpretability`: Supports feature importance and SHAP values.\n",
        "- `Handles Missing Values`: No imputation needed.\n",
        "- `Scalable`: Efficient for large datasets with GPU support.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70UMhkxtGPsU"
      },
      "source": [
        "### Limitations of CatBoost\n",
        "\n",
        "- `Computationally Intensive`: Slower on large datasets without GPU.\n",
        "- `High Memory Usage`: Can be memory-heavy for many categories.\n",
        "- `Smaller Community`: Fewer resources than XGBoost/LightGBM.\n",
        "- `Complex Customization`: Fine-tuning can be challenging.\n",
        "- `Noise Sensitivity`: Requires preprocessing for noisy data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyOope_pwUVr"
      },
      "source": [
        "## Setup R in Python Runtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDp3ULld8Gb9"
      },
      "source": [
        "### Install {rpy2}\n",
        "\n",
        "{rpy2} is a Python package that provides an interface to the R programming language, allowing Python users to run R code, call R functions, and manipulate R objects directly from Python. It enables seamless integration between Python and R, leveraging R's statistical and graphical capabilities while using Python's flexibility. The package supports passing data between the two languages and is widely used for statistical analysis, data visualization, and machine learning tasks that benefit from R's specialized libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiM6y-Mw8AJp",
        "outputId": "d8828697-5807-4686-f949-50266178bfc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: rpy2 3.5.1\n",
            "Uninstalling rpy2-3.5.1:\n",
            "  Successfully uninstalled rpy2-3.5.1\n",
            "Collecting rpy2==3.5.1\n",
            "  Using cached rpy2-3.5.1-cp311-cp311-linux_x86_64.whl\n",
            "Requirement already satisfied: cffi>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from rpy2==3.5.1) (1.17.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from rpy2==3.5.1) (3.1.6)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from rpy2==3.5.1) (2025.2)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.11/dist-packages (from rpy2==3.5.1) (5.3.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.10.0->rpy2==3.5.1) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->rpy2==3.5.1) (3.0.2)\n",
            "Installing collected packages: rpy2\n",
            "Successfully installed rpy2-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall rpy2 -y\n",
        "!pip install rpy2==3.5.1\n",
        "%load_ext rpy2.ipython"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1zeuaCowiBt"
      },
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J-4ie4bwiJ1",
        "outputId": "c40f4333-8e0d-4776-8666-a86adad24b67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztl1QBDb4MYe"
      },
      "source": [
        "\n",
        "## CatBoost Implementation in R from Scratch\n",
        "\n",
        "To fit a CatBoost-like model from scratch in R without external packages, we'll implement a simplified gradient boosting algorithm using decision stumps (depth-1 trees) with Newton-Raphson updates. The synthetic Titanic dataset includes key features like passenger class, sex, age, siblings/spouses, parents/children, fare, and embarkation port. The target is survival status (binary).\n",
        "\n",
        "**Decision stumps** are the simplest form of decision trees—they make a single binary split (depth = 1) based on one feature. They act as \"weak learners\" in boosting algorithms.\n",
        "\n",
        "**Newton-Raphson** is an optimization method that uses second-order derivatives (Hessian) for faster convergence. In gradient boosting, it calculates optimal leaf values by minimizing the loss function more efficiently than first-order gradients alone.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP2lCzFHBUip"
      },
      "source": [
        "### Generate Synthetic Titanic like Dataset\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTjVIV5ZBW96",
        "outputId": "9b1e262d-8cb4-429d-d90a-a4f6ce3b8558"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Pclass    Sex  Age SibSp Parch  Fare Embarked Survived\n",
            "1      3   male 20.0     0     0 19.82        Q        1\n",
            "2      2   male 19.6     1     4 23.85        C        0\n",
            "3      3   male 29.8     0     0 34.27        S        0\n",
            "4      1 female 28.7     1     0 44.96        S        1\n",
            "5      1 female  4.5     0     0  5.36        Q        0\n",
            "6      3   male 40.4     0     0 42.41        S        0\n"
          ]
        }
      ],
      "source": [
        "%%R\n",
        "set.seed(123)\n",
        "n <- 1000  # Number of samples\n",
        "\n",
        "# Generate features\n",
        "Pclass <- sample(1:3, n, replace = TRUE, prob = c(0.2, 0.3, 0.5))\n",
        "Sex <- sample(c(\"male\", \"female\"), n, replace = TRUE, prob = c(0.6, 0.4))\n",
        "Age <- round(rnorm(n, mean = 30, sd = 10), 1)\n",
        "Age[Age < 1] <- 1  # Ensure age is positive\n",
        "SibSp <- sample(0:5, n, replace = TRUE, prob = c(0.7, 0.1, 0.1, 0.05, 0.03, 0.02))\n",
        "Parch <- sample(0:4, n, replace = TRUE, prob = c(0.8, 0.1, 0.05, 0.03, 0.02))\n",
        "Fare <- round(rgamma(n, shape = 2, scale = 15), 2)\n",
        "Embarked <- sample(c(\"C\", \"Q\", \"S\"), n, replace = TRUE, prob = c(0.3, 0.2, 0.5))\n",
        "\n",
        "# Generate survival probabilities based on features\n",
        "surv_prob <- plogis(\n",
        "  0.5 * (Sex == \"female\") +\n",
        "  0.3 * (Pclass == 1) - 0.4 * (Pclass == 3) +\n",
        "  0.02 * (Age - 30) -\n",
        "  0.05 * SibSp - 0.02 * Parch +\n",
        "  0.001 * Fare +\n",
        "  0.2 * (Embarked == \"C\") - 0.1 * (Embarked == \"Q\")\n",
        ")\n",
        "Survived <- rbinom(n, 1, surv_prob)\n",
        "\n",
        "# Combine into data frame\n",
        "titanic <- data.frame(\n",
        "  Pclass, Sex, Age, SibSp, Parch, Fare, Embarked, Survived\n",
        ")\n",
        "head(titanic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwIhiSZNCO9N"
      },
      "source": [
        "### Preprocess Data\n",
        "\n",
        "Convert categorical features to numeric:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYPiX59GCiTY"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "preprocess_data <- function(df) {\n",
        "  df$Sex <- as.numeric(factor(df$Sex)) - 1  # male=0, female=1\n",
        "  df$Embarked <- as.numeric(factor(df$Embarked)) - 1  # C=0, Q=1, S=2\n",
        "  return(as.matrix(df))\n",
        "}\n",
        "\n",
        "X <- preprocess_data(titanic[, names(titanic) != \"Survived\"])\n",
        "y <- titanic$Survived"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aFvX9PX6t7N"
      },
      "source": [
        "\n",
        "### Implement Gradient Boosting with Stumps\n",
        "\n",
        "-   Uses `logistic loss` for binary classification.\n",
        "\n",
        "-   Builds `decision stumps` (depth-1 trees) at each iteration.\n",
        "\n",
        "-   Applies `Newton-Raphson updates` for leaf values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foG5zY1Kcbe9"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "# Sigmoid function for probabilities\n",
        "sigmoid <- function(z) 1 / (1 + exp(-z))\n",
        "\n",
        "# Train the model\n",
        "gradient_boosting <- function(\n",
        "  X, y, n_trees = 50, learning_rate = 0.1, depth = 1\n",
        ") {\n",
        "  n <- nrow(X)\n",
        "  p <- ncol(X)\n",
        "  F0 <- log(mean(y) / (1 - mean(y)))  # Initial log-odds\n",
        "  Fx <- rep(F0, n)  # Initialize predictions\n",
        "  trees <- list()  # Store tree structures\n",
        "\n",
        "  for (tree_idx in 1:n_trees) {\n",
        "    # Compute current probabilities and gradients\n",
        "    p_curr <- sigmoid(Fx)\n",
        "    residual <- y - p_curr\n",
        "    weights <- p_curr * (1 - p_curr)\n",
        "\n",
        "    # Find best split across all features\n",
        "    best_gain <- -Inf\n",
        "    best_feature <- NULL\n",
        "    best_split <- NULL\n",
        "    best_left <- best_right <- NULL\n",
        "\n",
        "    for (j in 1:p) {\n",
        "      values <- unique(X[, j])\n",
        "      if (length(values) > 10) {\n",
        "        q <- quantile(X[, j], probs = seq(0.1, 0.9, 0.1))\n",
        "      } else {\n",
        "        q <- values\n",
        "      }\n",
        "\n",
        "      for (split_val in q) {\n",
        "        left_idx <- X[, j] <= split_val\n",
        "        right_idx <- !left_idx\n",
        "\n",
        "        # Skip if split creates very small leaves\n",
        "        if (sum(left_idx) < 5 || sum(right_idx) < 5) next\n",
        "\n",
        "        # Compute gradient sums and weights for leaves\n",
        "        G_left <- sum(residual[left_idx])\n",
        "        G_right <- sum(residual[!left_idx])\n",
        "        H_left <- sum(weights[left_idx])\n",
        "        H_right <- sum(weights[!left_idx])\n",
        "\n",
        "        # Calculate gain (simplified)\n",
        "        gain <- (G_left^2 / (H_left + 1e-6) + G_right^2 / (H_right + 1e-6))\n",
        "        if (gain > best_gain) {\n",
        "          best_gain <- gain\n",
        "          best_feature <- j\n",
        "          best_split <- split_val\n",
        "          best_left <- left_idx\n",
        "          best_right <- right_idx\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "\n",
        "    # Compute leaf values using Newton-Raphson\n",
        "    leaf_left <- sum(residual[best_left]) / sum(weights[best_left] + 1e-6)\n",
        "    leaf_right <- sum(residual[best_right]) / sum(weights[best_right] + 1e-6)\n",
        "\n",
        "    # Update predictions\n",
        "    Fx[best_left] <- Fx[best_left] + learning_rate * leaf_left\n",
        "    Fx[best_right] <- Fx[best_right] + learning_rate * leaf_right\n",
        "\n",
        "    # Save tree information\n",
        "    trees[[tree_idx]] <- list(\n",
        "      feature = best_feature,\n",
        "      split = best_split,\n",
        "      left_value = learning_rate * leaf_left,\n",
        "      right_value = learning_rate * leaf_right\n",
        "    )\n",
        "  }\n",
        "\n",
        "  return(list(trees = trees, init = F0))\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPJv1wf3xLmp"
      },
      "source": [
        "### Train the Model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSuBBDLKDHML"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "model <- gradient_boosting(X, y, n_trees = 50, learning_rate = 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzQKJ-urbCtn"
      },
      "source": [
        "### Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8mxvvBkbGBc"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "predict_boost <- function(model, X) {\n",
        "  Fx <- rep(model$init, nrow(X))\n",
        "  for (tree in model$trees) {\n",
        "    left_idx <- X[, tree$feature] <= tree$split\n",
        "    Fx[left_idx] <- Fx[left_idx] + tree$left_value\n",
        "    Fx[!left_idx] <- Fx[!left_idx] + tree$right_value\n",
        "  }\n",
        "  return(sigmoid(Fx))\n",
        "}\n",
        "\n",
        "# Predict on training data\n",
        "preds <- predict_boost(model, X)\n",
        "pred_class <- ifelse(preds > 0.5, 1, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okobJFvfbTzD"
      },
      "source": [
        "### Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eClvfwtHbUoD",
        "outputId": "9e7fc009-28e3-4710-b5b7-456d1f7d7829"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.607 \n"
          ]
        }
      ],
      "source": [
        "%%R\n",
        "accuracy <- mean(pred_class == y)\n",
        "cat(\"Training Accuracy:\", round(accuracy, 3), \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5Z1SpIUbgLy"
      },
      "source": [
        "## CatBoost with R Packages\n",
        "\n",
        "The **`catboost`** package is an R interface to **CatBoost**, a high-performance, open-source gradient boosting library developed by Yandex. It is designed to handle **categorical features natively**, without requiring manual preprocessing like one-hot encoding. CatBoost excels in classification, regression, and ranking tasks, and is optimized for both speed and accuracy. Its key strengths include **efficient handling of categorical data**, **robust default parameters**, and **built-in support for missing values**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXu-XY0mw1A8"
      },
      "source": [
        "### Check and Install Required R Packages\n",
        "\n",
        "Install the `{catboost}` package from GitHub if not already installed. The package is available for both Linux and Windows systems. You have to install it from source as it is not available on CRAN.\n",
        "\n",
        "Following R packages are required to run this notebook. If any of these packages are not installed, you can install them using the code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1S711KlGL7lU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c1b0463-e8bf-4d09-bc59-593b546e820c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: Downloading GitHub repo catboost/catboost@HEAD\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "── R CMD build ─────────────────────────────────────────────────────────────────\n",
            "* checking for file ‘/tmp/RtmpXMn0Zp/remotes2b2413642e74/catboost-catboost-5b5c9f3/catboost/R-package/DESCRIPTION’ ... OK\n",
            "* preparing ‘catboost’:\n",
            "* checking DESCRIPTION meta-information ... OK\n",
            "* cleaning src\n",
            "* checking for LF line-endings in source and make files and shell scripts\n",
            "* checking for empty or unneeded directories\n",
            "* building ‘catboost_1.2.8.tar.gz’\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%R\n",
        "# Install CatBoost from GitHub source\n",
        "devtools::install_github('catboost/catboost', subdir = 'catboost/R-package', force = TRUE,lib='drive/My Drive/R/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeYB57l0wz5N"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "packages <- c('tidyverse',\n",
        "              'plyr',\n",
        "              'titanic',\n",
        "              'catboost',\n",
        "              'caret',\n",
        "              'pROC',\n",
        "              'fastDummies',\n",
        "              'ggpmisc',\n",
        "              'Metrics'\n",
        "\n",
        "         )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7NH-51RYeIk"
      },
      "source": [
        "### Install Missing Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0aTMYTHAraZ"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "# Install missing packages\n",
        "new.packages <- packages[!(packages %in% installed.packages(lib='drive/My Drive/R/')[,\"Package\"])]\n",
        "if(length(new.packages)) install.packages(new.packages, lib='drive/My Drive/R/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ah43eXWcYfpA"
      },
      "source": [
        "### Verify Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9hpg7qnArfh",
        "outputId": "47aec7b0-cf9d-485f-bdef-71d0a1192409"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installed packages:\n",
            "  tidyverse        plyr     titanic    catboost       caret        pROC \n",
            "       TRUE        TRUE        TRUE       FALSE        TRUE        TRUE \n",
            "fastDummies     ggpmisc     Metrics \n",
            "       TRUE        TRUE        TRUE \n"
          ]
        }
      ],
      "source": [
        "%%R\n",
        "# set library path\n",
        ".libPaths('drive/My Drive/R')\n",
        "# Verify installation\n",
        "cat(\"Installed packages:\\n\")\n",
        "print(sapply(packages, requireNamespace, quietly = TRUE))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run catBoost Model in Python"
      ],
      "metadata": {
        "id": "Da6FTarG6Qmr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV7R29xfyWQz"
      },
      "source": [
        "Since I am  unable to run the {catboost} package in R on Google Colab, the challenge is that the provided code relies on `catboost` for model training and prediction, which cannot be executed without it. To address this, I’ll adapt the code to run in Google Colab using Python with equivalent libraries (since `{catboost}` is more reliably installed in Python) while maintaining the same functionality: training a model on the Titanic dataset, evaluating accuracy, generating a confusion matrix, calculating AUC, and plotting an ROC curve. I’ll use Python’s `{pandas}` (similar to `{dplyr}`/`{plyr}`), `{scikit-learn}` for data splitting and evaluation, and `catboost` in Python for modeling, as it aligns with your goal of using CatBoost.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary and Conclusion\n",
        "\n",
        "CatBoost combines gradient boosting with innovative techniques like ordered target encoding and ordered boosting to handle categorical features and reduce overfitting. The equations above capture the core mathematical steps, from initializing the model to building trees and making predictions. For practical implementation, you can use the CatBoost library, which automates these steps while allowing customization of parameters like learning rate ($\\eta$), tree depth, and number of iterations ($M$). This tutorial provides a comprehensive overview of CatBoost's mathematical foundations and practical implementation in R, showcasing its effectiveness for tasks involving categorical data."
      ],
      "metadata": {
        "id": "X447y2U6DPWM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACNQyq1seLc7"
      },
      "source": [
        "\n",
        "## References\n",
        "\n",
        "1.  **CatBoost Documentation**\n",
        "\n",
        "    -   **Link**: [catboost.ai](https://catboost.ai)\\\n",
        "    -   **Description**: Official guide for CatBoost, covering its algorithm, R/Python usage, and tutorials for datasets like Titanic.\n",
        "\n",
        "2.  **Prokhorenkova et al. (2018). CatBoost: Unbiased Boosting with Categorical Features**\n",
        "\n",
        "    -   **Link**: [arXiv:1706.09516](https://arxiv.org/abs/1706.09516)\\\n",
        "    -   **Description**: Research paper introducing CatBoost’s ordered boosting and categorical feature handling.\n",
        "\n",
        "3.  **Kaggle Titanic Dataset**\n",
        "\n",
        "    -   **Link**: [kaggle.com/c/titanic](https://www.kaggle.com/c/titanic)\\\n",
        "    -   **Description**: Popular dataset with CatBoost tutorials for predicting the \"Survived\" variable.\n",
        "\n",
        "4.  [Catboost R Tutorial](https://github.com/catboost/catboost/blob/master/catboost/tutorials/r_tutorial.ipynb)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOO5QGdycFEQJ1PhdhffYl1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}