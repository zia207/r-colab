{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-00-tree-based-models-decision-tree-introduction-r.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH9ES7RYWUdv"
      },
      "source": [
        "![alt text](http://drive.google.com/uc?export=view&id=1bLQ3nhDbZrCCqy_WCxxckOne2lgVvn3l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrQshXqjWS7x"
      },
      "source": [
        "# 1. Decision Trees\n",
        "\n",
        "Decision trees are a type of supervised learning algorithm used for both classification and regression tasks. They work by splitting the data into subsets based on the value of input features, creating a tree-like structure of decisions. The goal is to create a model that predicts the target variable by learning simple decision rules inferred from the data features. Decision trees are intuitive and easy to interpret, making them a popular choice for many machine learning tasks. By understanding the mathematical foundation of decision trees, including splitting criteria like Gini impurity, entropy, information gain, and mean squared error, we can build more effective and accurate models.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHx3FkVeWnxQ"
      },
      "source": [
        "## Overview of Decision Trees\n",
        "\n",
        "A **decision tree** is a supervised machine learning model used for both **classification** and **regression** tasks. It represents decisions and their possible consequences, including chance event outcomes, costs, and utility, in a tree-like structure. The model recursively splits the input space into regions based on feature values and makes a decision based on the majority class (classification) or average value (regression) in that region. Decision trees are popular due to their interpretability, ease of use, and ability to handle both numerical and categorical data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TZggNGCrofU"
      },
      "source": [
        "### Structure of a Decision Tree\n",
        "\n",
        "-   `Root Node`: The topmost node, representing the entire dataset.\n",
        "-   `Internal Nodes`: Represent decision points based on a feature and a threshold (e.g., `Petal.Length <= 2.5`).\n",
        "-   `Branches`: Represent the outcome of a decision, leading to child nodes.\n",
        "-   `Leaf Nodes`: Terminal nodes that provide the final prediction (e.g., a class label or a numerical value).\n",
        "-   `Splitting Criterion`: A measure to decide how to split a node (e.g., Gini impurity, variance reduction).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYoXFAYgrqMM"
      },
      "source": [
        "### How Decision Trees Work\n",
        "\n",
        "1.  `Splitting`: Select a feature and threshold to split the data into two or more subsets, optimizing a criterion (e.g., reducing impurity).\n",
        "2.  `Recursion`: Repeat splitting for each subset until a stopping condition is met (e.g., maximum depth, minimum node size, or pure nodes).\n",
        "3.  `Prediction`: For a new observation, traverse the tree from the root to a leaf, following the decision rules, and return the leaf’s prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yij_QOcsSFd"
      },
      "source": [
        "## Types of Decision Tree Models\n",
        "\n",
        "Decision tree models vary based on their splitting criteria, handling of data, and pruning strategies. Below are the main types of decision tree models, including their descriptions, mathematical foundations, and common implementations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMTkykugsZXk"
      },
      "source": [
        "### CART (Classification and Regression Trees)\n",
        "\n",
        "The most widely used decision tree algorithm, developed by Breiman et al. (1984). It supports both classification and regression and uses impurity-based splitting criteria.\n",
        "\n",
        "-   `Splitting Criteria`:\n",
        "\n",
        "  -   **Classification**: Gini impurity, $G = 1 - \\sum_{k=1}^K p_k^2$, where $p_k$ is the proportion of class $k$. Alternative: Entropy, $H = -\\sum_{k=1}^K p_k \\log(p_k)$.\n",
        "\n",
        "  -   **Regression**: Variance reduction, $\\text{Var} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y})^2$, or Mean Squared Error (MSE).\n",
        "\n",
        "-   `Pruning`: Uses cost-complexity pruning, balancing tree size and error: $R_\\alpha(T) = R(T) + \\alpha \\cdot |T|$, where $R(T)$ is the error, $|T|$ is the number of nodes, and $\\alpha$ is the complexity parameter (`cp`).\n",
        "\n",
        "-   `Characteristics`:\n",
        "\n",
        "  -   Binary splits (two child nodes per split).\n",
        "\n",
        "  -   Prone to bias toward features with many levels.\n",
        "\n",
        "  -   Simple and fast but may overfit without pruning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALQW4gRwskbF"
      },
      "source": [
        "### Conditional Inference Trees (CITs)\n",
        "\n",
        "Developed by Hothorn et al. (2006), CITs use a statistical framework to reduce bias in variable selection and overfitting.\n",
        "\n",
        "-   `Splitting Criteria`: - Based on statistical hypothesis tests (e.g., permutation tests) to assess the association between a predictor $X_j$ and the response $Y$.\n",
        "\n",
        "   -   **Classification**: Chi-squared or permutation tests for categorical responses.\n",
        "\n",
        "   -   **Regression**: F-tests or correlation-based tests for continuous responses.\n",
        "\n",
        "-   `Test statistic` $T_j = \\sum_{i=1}^n g(X_{ij}) h(Y_i)$ with p-values adjusted (e.g., Bonferroni) to select the predictor with the smallest p-value.\n",
        "\n",
        "-   `Pruning`: Implicitly controlled by a significance threshold (`mincriterion`, typically 0.95, corresponding to $\\alpha = 0.05$.\n",
        "\n",
        "-   `Characteristics`:\n",
        "\n",
        "   -   Unbiased variable selection, avoiding preference for features with many split points.\n",
        "\n",
        "    -   Statistically rigorous, reducing overfitting.\n",
        "\n",
        "    -   Binary splits, similar to CART."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcQ6e5NwsmIR"
      },
      "source": [
        "### C4.5\n",
        "\n",
        "C4.5 is a supervised learning algorithm that builds a decision tree by recursively splitting the input space into regions based on feature values and assigning a class label to each region.\n",
        "\n",
        "-   `Splitting Criterion`:\n",
        "\n",
        "  -   **Gain Ratio**: Normalizes information gain by the intrinsic information of the split, $\\text{Gain Ratio} = \\frac{\\text{Gain}}{-\\sum_{i} \\frac{n_i}{n} \\log(\\frac{n_i}{n})}$.\n",
        "\n",
        "-   `Pruning`: Uses error-based pruning, removing branches that do not improve accuracy on a validation set.\n",
        "\n",
        "-   `Characteristics`:\n",
        "\n",
        "  -   Supports continuous features with binary splits (e.g., `X <= t`).\n",
        "\n",
        "   -   Multi-way splits for categorical features.\n",
        "\n",
        "   -   More robust than ID3 but less common than CART or CITs in modern implementations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeF9ShfAsqcQ"
      },
      "source": [
        "### Model-based Recursive Partitioning (MOB)\n",
        "\n",
        "MOB combines decision trees with parametric models (e.g., linear regression, logistic regression) fitted in terminal nodes. It’s designed to capture heterogeneous relationships.\n",
        "\n",
        "-   `Splitting Criterion`: - Statistical tests (e.g., score tests) to check parameter stability across splits.\n",
        "\n",
        "-   `Test statistic` based on the score function $T_j = \\sum \\psi_i(Z_{ij})$, where $\\psi_i$ is the score of the parametric model.\n",
        "\n",
        "-   `Characteristics`: - Fits a full parametric model (e.g., ( Y = \\beta\\_0 + \\beta\\_1 X_1 )) in each leaf. - Unbiased splitting, similar to CITs. - Ideal for data with varying predictor effects across subgroups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1rZrirXst9q"
      },
      "source": [
        "### Comparison of Decision Tree Models\n",
        "\n",
        "| Model | Splitting Criterion | Task Support | Pruning | Bias Handling | Implementation |\n",
        "|------------|------------|------------|------------|------------|------------|\n",
        "| **CART** | Gini, Entropy (classification); Variance (regression) | Classification, Regression | Cost-complexity pruning | Biased toward many splits | `rpart`, `sklearn` |\n",
        "| **CIT** | Statistical tests (chi-squared, F-test) | Classification, Regression | Statistical stopping | Unbiased | `partykit::ctree` |\n",
        "| **MOB** | Parameter stability tests | Classification, Regression | Statistical stopping | Unbiased | `partykit::mob` |\n",
        "| **C4.5** | Gain Ratio | Classification | Error-based pruning | Less biased | `RWeka::J48` |\n",
        "| **C5.0** | Gain Ratio | Classification | Error-based pruning | Less biased | `C50:C5.0` |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6kg0xFGRyj-"
      },
      "source": [
        "### Key Differences Between Classification and Regression\n",
        "\n",
        "| **Aspect** | **Classification** | **Regression** |\n",
        "|-------------------|---------------------------|--------------------------|\n",
        "| **Output** | Discrete class labels | Continuous numerical values |\n",
        "| **Splitting Criterion** | Gini, Entropy, Misclassification Error | Variance Reduction, MSE |\n",
        "| **Leaf Node Output** | Majority class | Mean (or median) of target values |\n",
        "| **Objective** | Maximize class purity | Minimize variance of target values |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V6D09SpR2--"
      },
      "source": [
        "## Summary and Conclusion\n",
        "\n",
        "Decision trees are powerful and interpretable models for classification and regression tasks. They can be implemented using various algorithms, each with its strengths and weaknesses. The choice of algorithm depends on the specific problem, data characteristics, and desired interpretability. Understanding the differences between these models helps in selecting the most appropriate one for a given task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZtIoviVSGOk"
      },
      "source": [
        "## Recommended Resources and Further Reading\n",
        "\n",
        "Below are recommended resources and further reading materials for learning about decision trees, a fundamental machine learning technique. These include books, academic papers, online courses, and tutorials, with links where available:\n",
        "\n",
        "\n",
        "### Books\n",
        "\n",
        "1. **\"Introduction to Statistical Learning\" by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani**\n",
        "   - **Description**: Provides a clear introduction to decision trees, including their construction and pruning, with practical examples.\n",
        "   - **Link**: [Springer](https://link.springer.com/book/10.1007/978-1-4614-7138-7) (free PDF available at [ISL Book](http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf)).\n",
        "   - **Relevance**: Chapter 8 covers decision trees and their role in ensemble methods.\n",
        "\n",
        "2. **\"The Elements of Statistical Learning\" by Trevor Hastie, Robert Tibshirani, and Jerome Friedman**\n",
        "   - **Description**: Offers an advanced treatment of decision trees, including theoretical aspects and extensions like ensemble methods.\n",
        "   - **Link**: [Springer](https://link.springer.com/book/10.1007/978-0-387-84858-7) (free PDF available at [ESL Book](https://hastie.su.domains/ElemStatLearn/)).\n",
        "   - **Relevance**: Chapter 9 delves into decision trees and their applications.\n",
        "\n",
        "### Academic Papers\n",
        "\n",
        "3. **\"Classification and Regression Trees (CART)\" by Leo Breiman et al. (1984)**\n",
        "   - **Description**: The seminal paper introducing the CART algorithm, a cornerstone of decision tree methodology.\n",
        "   - **Link**: [Taylor & Francis](https://www.tandfonline.com/doi/abs/10.1080/01621459.1984.10478273) (requires subscription or institutional access).\n",
        "   - **Relevance**: Provides the original framework for building and interpreting decision trees.\n",
        "\n",
        "4. **\"Induction of Decision Trees\" by J. Ross Quinlan (1986)**\n",
        "   - **Description**: Introduces the ID3 algorithm, an early decision tree algorithm, and its evolution into C4.5.\n",
        "   - **Link**: [Springer](https://link.springer.com/chapter/10.1007/3-540-17249-8_5) (requires subscription or institutional access).\n",
        "   - **Relevance**: Foundational work on decision tree induction.\n",
        "\n",
        "### Online Courses and Tutorials\n",
        "\n",
        "5. **\"Machine Learning by Andrew Ng\" (Coursera)**\n",
        "   - **Description**: Covers decision trees as part of supervised learning, with practical insights.\n",
        "   - **Link**: [Coursera](https://www.coursera.org/learn/machine-learning) (free to audit, subscription for certificate).\n",
        "   - **Relevance**: Week 6 includes decision trees and their implementation.\n",
        "\n",
        "6. **\"Practical Machine Learning with Python\" (edX)**\n",
        "   - **Description**: A hands-on course focusing on decision trees and their use in classification and regression tasks.\n",
        "   - **Link**: [edX](https://www.edx.org/course/practical-machine-learning-with-python) (free to audit, certificate with fee).\n",
        "   - **Relevance**: Includes practical examples of decision tree modeling.\n",
        "\n",
        "### Websites and Documentation\n",
        "\n",
        "7. **Scikit-Learn Documentation**\n",
        "   - **Description**: Provides detailed guides and examples for implementing decision trees using `DecisionTreeClassifier` and `DecisionTreeRegressor` in Python.\n",
        "   - **Link**: [Scikit-Learn](https://scikit-learn.org/stable/modules/tree.html) (free).\n",
        "   - **Relevance**: Offers theory, code examples, and parameter tuning.\n",
        "\n",
        "8. **RDocumentation (rpart Package)**\n",
        "   - **Description**: Documentation for R’s `rpart` package, which implements recursive partitioning for decision trees.\n",
        "   - **Link**: [RDocumentation](https://www.rdocumentation.org/packages/rpart/versions/4.1-15) (free).\n",
        "   - **Relevance**: Practical resource for building decision trees in R.\n",
        "\n",
        "### Additional Resources\n",
        "\n",
        "9. **\"A Simple Guide to Decision Trees\" (Towards Data Science)**\n",
        "   - **Description**: A blog post explaining decision trees with intuitive examples and visualizations.\n",
        "   - **Link**: [Towards Data Science](https://towardsdatascience.com/a-simple-guide-to-decision-trees-1f9f6e6c3d7f) (free).\n",
        "   - **Relevance**: Beginner-friendly overview of decision tree mechanics.\n",
        "\n",
        "10. **YouTube: \"Decision Trees in Machine Learning\" by StatQuest with Josh Starmer**\n",
        "    - **Description**: A video tutorial explaining how decision trees work with clear animations.\n",
        "    - **Link**: [YouTube](https://www.youtube.com/watch?v=7VeUPuFGJHk) (free).\n",
        "    - **Relevance**: Visual and intuitive introduction to decision trees.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Toble of Contents\n",
        "\n",
        "\n",
        "This section of tutorial is divided into several parts, each focusing on a specific type of decision tree model. The models covered include:\n",
        "\n",
        "1.1 [CART (Classification and Regression Trees)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-01-tree-based-models-decision-tree-cart-r.ipynb)\n",
        "\n",
        "1.2 [Conditional Inference Trees (CITs)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-02-tree-based-models-decision-tree-cit-r.ipynb)\n",
        "\n",
        "1.3 [Model-based Recursive Partitioning (MOB)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-03-tree-based-models-decision-tree-mob-r.ipynb)\n",
        "\n",
        "1.4 [C4.5 Model](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-04-tree-based-models-decision-tree-c45-r.ipynb)\n",
        "\n",
        "1.5 [C5.0 Model](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-05-tree-based-models-decision-tree-c50-r.ipynb)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMOVyguLX7DcTec4Jai+kFN",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
