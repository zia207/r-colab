{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-00-tree-based-models-bagging-introduction-r.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH9ES7RYWUdv"
      },
      "source": [
        "![alt text](http://drive.google.com/uc?export=view&id=1bLQ3nhDbZrCCqy_WCxxckOne2lgVvn3l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrQshXqjWS7x"
      },
      "source": [
        "# 2. Bagging or Bootstrap Aggregating\n",
        "Bagging, or Bootstrap Aggregating, is a powerful technique that can significantly improve the performance of machine learning models, especially when dealing with complex datasets. By leveraging the strengths of multiple models, bagging can reduce overfitting and increase the robustness of predictions. In this section will discuss the concept of bagging, its advantages, and how it works in practice. We will also explore its application different popular machine learning algorithms that utilizes bagging to enhance predictive performance.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHx3FkVeWnxQ"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Bagging is an ensemble learning technique that improves the stability and accuracy of machine learning algorithms. It is particularly effective for high-variance models like decision trees. The main idea behind bagging is to create multiple versions of a predictor and combine them to produce a single, more accurate prediction. Bagging works by training multiple models on different subsets of the training data, which are created by sampling with replacement (bootstrapping). Each model is trained independently, and their predictions are aggregated to produce a final output. This process helps to reduce overfitting and increase the robustness of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYaVCS4QZn3Y"
      },
      "source": [
        "### How Bagging Works\n",
        "\n",
        "Bagging works by following these steps:\n",
        "\n",
        "1.  `Bootstrap Sampling`: Generate multiple subsets of the training data by randomly sampling with replacement (each subset may contain duplicates and miss some original data points).\n",
        "\n",
        "2.  `Model Training:` Train a separate decision tree on each bootstrap sample. Each tree is built independently, typically without pruning, to capture diverse patterns.\n",
        "\n",
        "3.  `Aggregation`: Combine predictions from all trees:\n",
        "\n",
        "-   `Classification`: Majority voting across trees.\n",
        "-   `Regression`: Average predictions across trees.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i8b2h1zaMDu"
      },
      "source": [
        "Here is a flowchart illustrating the bagging process:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cfgtu_h1Zrg-"
      },
      "source": [
        "\n",
        "![alt text](http://drive.google.com/uc?export=view&id=13etqO1L0KZ_vMjtBH23ERSPov5AtW7qA)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6kg0xFGRyj-"
      },
      "source": [
        "Bagging is particularly effective for high-variance, unstable models like decision trees, as it mitigates their tendency to overfit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdBJ79TzaUq0"
      },
      "source": [
        "### Differences Between Bagged Trees, Random Forests, and Other Variants\n",
        "\n",
        "Below is a comparison of bagged trees and related ensemble methods, focusing on their key differences:\n",
        "\n",
        "| **Method**                     | **Description**                                                                 | **Key Features**                                                                 | **Use Case**                                                                 |\n",
        "|--------------------------------|--------------------------------------------------------------------------------|----------------------------------------------------------------------------------|------------------------------------------------------------------------------|\n",
        "| `Bagged Trees`*              | Standard bagging applied to decision trees. Each tree is trained on a bootstrap sample of the data. | - Uses all features for splits.<br>- Predictions are averaged or voted.<br>- Reduces variance. | General classification/regression tasks where variance reduction is needed. |\n",
        "| `Random Forests`            | Extends bagging by adding randomness in feature selection at each split.       | - Randomly selects a subset of features (mtry) for each split.<br>- Further reduces correlation between trees.<br>- More robust than bagged trees. | Classification/regression with improved robustness and feature importance.   |\n",
        "| `Quantile Regression Forests`| A variant of random forests for estimating conditional quantiles (e.g., median, 90th percentile). | - Stores all observations in leaf nodes (not just averages).<br>- Estimates quantiles of the target distribution.<br>- Useful for heteroscedastic data. | Predicting conditional quantiles, uncertainty estimation, risk analysis.     |\n",
        "| `Survival Forests`          | Random forests adapted for survival analysis (time-to-event data).             | - Handles censored data.<br>- Predicts survival probabilities or hazard functions.<br>- Uses specialized splitting criteria (e.g., log-rank test). | Survival analysis, e.g., medical research for time-to-event prediction.      |\n",
        "| `Extremely Randomized Forests (ExtraTrees)` | Introduces more randomness by selecting random split points (not optimal).     | - Randomizes both feature selection and split thresholds.<br>- Faster training due to less computation.<br>- May reduce overfitting in some cases. | Classification/regression where speed and robustness are priorities.          |\n",
        "| `Generalized Random Forest (GRF)` | A framework for customizing random forests for various tasks (e.g., causal inference, quantile regression). | - Flexible splitting rules tailored to specific objectives (e.g., treatment effects).<br>- Supports heterogeneous effect estimation.<br>- More theoretical grounding. | Causal inference, treatment effect estimation, customized prediction tasks.  |\n",
        "| `Distributed Random Forest (DRF)` | Random forests optimized for distributed computing environments.               | - Parallelizes tree construction across multiple nodes.<br>- Handles large-scale datasets.<br>- Often implemented in frameworks like H2O or Spark. | Big data applications, scalable machine learning on clusters.                |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-D7kSOdaZPo"
      },
      "source": [
        "### Key Differences Summarized\n",
        "\n",
        "- `Randomness`:\n",
        "  - Bagged trees use all features for splits.\n",
        "  - Random forests add feature subsampling.\n",
        "  - ExtraTrees add random split thresholds.\n",
        "  - GRF allows custom splitting rules.\n",
        "  \n",
        "- `Output Type`:\n",
        "  - Bagged trees and random forests predict means or classes.\n",
        "  - Quantile regression forests predict quantiles.\n",
        "  - Survival forests predict survival curves or hazard functions.\n",
        "  - GRF supports diverse outputs (e.g., treatment effects).\n",
        "  \n",
        "- `Scalability`:\n",
        "  - DRF is designed for distributed systems, unlike others which are typically single-machine.\n",
        "  \n",
        "- `Application`:\n",
        "  - Survival forests are specialized for time-to-event data.\n",
        "  - GRF is suited for causal inference.\n",
        "  - Others are general-purpose for classification/regression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v46Pny9IaaPp"
      },
      "source": [
        "### Advantages\n",
        "\n",
        "-   Reduces overfitting by averaging out noise across multiple trees.\n",
        "-   Handles high-dimensional data well (especially Random Forest and Extra Trees).\n",
        "-   Provides feature importance scores (e.g., in Random Forest).\n",
        "-   Parallelizable, as trees are trained independently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcYVM2Euafp5"
      },
      "source": [
        "### Limitations\n",
        "\n",
        "-   Less interpretable than a single decision tree.\n",
        "-   Computationally expensive for large datasets or many trees.\n",
        "-   May not perform as well as boosting methods (e.g., Gradient Boosting, XGBoost) for certain tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Topics Covered\n",
        "\n",
        "This section of tutorial will cover the following topics:\n",
        "\n",
        "2.1 [Bagged Trees](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-01-tree-based-models-bagging-bagged-trees-r.ipynb)\n",
        "    \n",
        "2.2 [Random Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-02-tree-based-models-bagging-randomforest-r.ipynb)\n",
        "    \n",
        "2.3 [Conditional Random Forests (cforest)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-03-tree-based-models-bagging-cforest-r.ipynb)\n",
        "    \n",
        "2.4 [Extremely Randomized Trees (Extra Trees)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-04-tree-based-models-bagging-extremely-randomized-trees-r.ipynb)\n",
        "    \n",
        "2.5 [Quantile Regression Forest (QRF)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-05-tree-based-models-bagging-quantile-regression-forest-r.ipynb)\n",
        "    \n",
        "2.6 [Random Forests Quantile Classifier](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-06-tree-based-models-bagging-quantile-classifier-forest-r.ipynb)\n",
        "    \n",
        "2.7 [Random Survival Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-07-tree-based-models-bagging-random-survival-forest-r.ipynb)\n",
        "    \n",
        "2.8 [Generalized Random Forests (GRF)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-00-tree-based-models-bagging-grf-introduction-r.ipynb)\n",
        "\n",
        "    \n",
        "2.8.1 [Survial Forests (SF)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-01-tree-based-models-bagging-grf-survival-forest-r.ipynb)\n",
        "      \n",
        "\n",
        "2.8.2 [Causal Forests (CF)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-02-tree-based-models-bagging-grf-causal-forest-r.ipynb)\n",
        "      \n",
        "\n",
        "2.8.3 [Causal Survival Forests (CSF)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-03-tree-based-models-bagging-grf-causal-survival-forest-r.ipynb)\n",
        "      \n",
        "\n",
        "2.8.4 [Multi-arm/multi-outcome Causal Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-04-tree-based-models-bagging-grf-arm-causal-forest-r.ipynb)\n",
        "      \n",
        "\n",
        "2.8.5 [Instrumental Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-05-tree-based-models-bagging-grf-instrumental-forest-r.ipynb)\n",
        "      \n",
        "\n",
        "2.8.6 [Linear Model Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-06-tree-based-models-bagging-grf-linear-model-forest-r.ipynb)\n",
        "   \n",
        "\n",
        "2.8.7 [Probability Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-07-tree-based-models-bagging-grf-probability-forest-r.ipynb)\n",
        "      \n",
        "\n",
        "2.8.8 [Regression Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-08-tree-based-models-bagging-grf-regression-forest-r.ipynb)\n",
        "      \n",
        "\n",
        "2.8.9 [Multi-task Regression Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-09-tree-based-models-bagging-grf-multitask-regression-forest-r.ipynb)\n",
        "\n",
        "     \n",
        "2.8.10 [Local Linear Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-10-tree-based-models-bagging-grf-local-linear-forest-r.ipynb)\n",
        "     \n",
        "\n",
        "2.8.11 [Boosted Regression Forest03-01](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-11-tree-based-models-bagging-grf-boosted-regression-forest-r.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V6D09SpR2--"
      },
      "source": [
        "## Summary and Conclusion\n",
        "\n",
        "Bagging is a powerful ensemble learning technique that enhances the performance of tree-based models by reducing overfitting and increasing robustness. By leveraging multiple decision trees trained on different subsets of data, bagging can significantly improve predictive accuracy. Random Forest and Extra Trees are popular implementations of bagging that introduce additional randomness to further enhance model performance. Understanding the principles and applications of bagging is essential for building effective machine learning models in various domains.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZtIoviVSGOk"
      },
      "source": [
        "## Refereences\n",
        "\n",
        "1.  Breiman, L. (1996). \"Bagging Predictors.\" Machine Learning, 24(2), 123-140.\n",
        "\n",
        "2.  Breiman, L. (1998). \"Arcing Classifiers.\" The Annals of Statistics, 26(3), 801-849.\n",
        "\n",
        "3.  Hastie, T., Tibshirani, R., & Friedman, J. (2009). \"The Elements of Statistical Learning: Data Mining, Inference, and Prediction\" (2nd Edition). Springer.\n",
        "\n",
        "4.  Kuhn, M., & Johnson, K. (2013). \"Applied Predictive Modeling.\" Springer.\n",
        "\n",
        "5.  James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). \"An Introduction to Statistical Learning: With Applications in R\" (1st Edition). Springer."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPMsECGcd1ZYZGEQHJlqADq",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
