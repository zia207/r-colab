{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-00-tree-based-models-bagging-introduction-r.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH9ES7RYWUdv"
      },
      "source": [
        "![alt text](http://drive.google.com/uc?export=view&id=1bLQ3nhDbZrCCqy_WCxxckOne2lgVvn3l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrQshXqjWS7x"
      },
      "source": [
        "# 2. Bagging or Bootstrap Aggregating\n",
        "Bagging, or Bootstrap Aggregating, is a powerful technique that can significantly improve the performance of machine learning models, especially when dealing with complex datasets. By leveraging the strengths of multiple models, bagging can reduce overfitting and increase the robustness of predictions. In this section will discuss the concept of bagging, its advantages, and how it works in practice. We will also explore its application different popular machine learning algorithms that utilizes bagging to enhance predictive performance.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHx3FkVeWnxQ"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Bagging is an ensemble learning technique that improves the stability and accuracy of machine learning algorithms. It is particularly effective for high-variance models like decision trees. The main idea behind bagging is to create multiple versions of a predictor and combine them to produce a single, more accurate prediction. Bagging works by training multiple models on different subsets of the training data, which are created by sampling with replacement (bootstrapping). Each model is trained independently, and their predictions are aggregated to produce a final output. This process helps to reduce overfitting and increase the robustness of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYaVCS4QZn3Y"
      },
      "source": [
        "### How Bagging Works\n",
        "\n",
        "Bagging works by following these steps:\n",
        "\n",
        "1.  `Bootstrap Sampling`: Generate multiple subsets of the training data by randomly sampling with replacement (each subset may contain duplicates and miss some original data points).\n",
        "\n",
        "2.  `Model Training:` Train a separate decision tree on each bootstrap sample. Each tree is built independently, typically without pruning, to capture diverse patterns.\n",
        "\n",
        "3.  `Aggregation`: Combine predictions from all trees:\n",
        "\n",
        "-   `Classification`: Majority voting across trees.\n",
        "-   `Regression`: Average predictions across trees.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i8b2h1zaMDu"
      },
      "source": [
        "Here is a flowchart illustrating the bagging process:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cfgtu_h1Zrg-"
      },
      "source": [
        "\n",
        "![alt text](http://drive.google.com/uc?export=view&id=13etqO1L0KZ_vMjtBH23ERSPov5AtW7qA)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6kg0xFGRyj-"
      },
      "source": [
        "Bagging is particularly effective for high-variance, unstable models like decision trees, as it mitigates their tendency to overfit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdBJ79TzaUq0"
      },
      "source": [
        "### Differences Between Bagged Trees, Random Forests, and Other Variants\n",
        "\n",
        "Below is a comparison of bagged trees and related ensemble methods, focusing on their key differences:\n",
        "\n",
        "| **Method**                     | **Description**                                                                 | **Key Features**                                                                 | **Use Case**                                                                 |\n",
        "|--------------------------------|--------------------------------------------------------------------------------|----------------------------------------------------------------------------------|------------------------------------------------------------------------------|\n",
        "| `Bagged Trees`*              | Standard bagging applied to decision trees. Each tree is trained on a bootstrap sample of the data. | - Uses all features for splits.<br>- Predictions are averaged or voted.<br>- Reduces variance. | General classification/regression tasks where variance reduction is needed. |\n",
        "| `Random Forests`            | Extends bagging by adding randomness in feature selection at each split.       | - Randomly selects a subset of features (mtry) for each split.<br>- Further reduces correlation between trees.<br>- More robust than bagged trees. | Classification/regression with improved robustness and feature importance.   |\n",
        "| `Quantile Regression Forests`| A variant of random forests for estimating conditional quantiles (e.g., median, 90th percentile). | - Stores all observations in leaf nodes (not just averages).<br>- Estimates quantiles of the target distribution.<br>- Useful for heteroscedastic data. | Predicting conditional quantiles, uncertainty estimation, risk analysis.     |\n",
        "| `Survival Forests`          | Random forests adapted for survival analysis (time-to-event data).             | - Handles censored data.<br>- Predicts survival probabilities or hazard functions.<br>- Uses specialized splitting criteria (e.g., log-rank test). | Survival analysis, e.g., medical research for time-to-event prediction.      |\n",
        "| `Extremely Randomized Forests (ExtraTrees)` | Introduces more randomness by selecting random split points (not optimal).     | - Randomizes both feature selection and split thresholds.<br>- Faster training due to less computation.<br>- May reduce overfitting in some cases. | Classification/regression where speed and robustness are priorities.          |\n",
        "| `Generalized Random Forest (GRF)` | A framework for customizing random forests for various tasks (e.g., causal inference, quantile regression). | - Flexible splitting rules tailored to specific objectives (e.g., treatment effects).<br>- Supports heterogeneous effect estimation.<br>- More theoretical grounding. | Causal inference, treatment effect estimation, customized prediction tasks.  |\n",
        "| `Distributed Random Forest (DRF)` | Random forests optimized for distributed computing environments.               | - Parallelizes tree construction across multiple nodes.<br>- Handles large-scale datasets.<br>- Often implemented in frameworks like H2O or Spark. | Big data applications, scalable machine learning on clusters.                |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-D7kSOdaZPo"
      },
      "source": [
        "### Key Differences Summarized\n",
        "\n",
        "- `Randomness`:\n",
        "  - Bagged trees use all features for splits.\n",
        "  - Random forests add feature subsampling.\n",
        "  - ExtraTrees add random split thresholds.\n",
        "  - GRF allows custom splitting rules.\n",
        "  \n",
        "- `Output Type`:\n",
        "  - Bagged trees and random forests predict means or classes.\n",
        "  - Quantile regression forests predict quantiles.\n",
        "  - Survival forests predict survival curves or hazard functions.\n",
        "  - GRF supports diverse outputs (e.g., treatment effects).\n",
        "  \n",
        "- `Scalability`:\n",
        "  - DRF is designed for distributed systems, unlike others which are typically single-machine.\n",
        "  \n",
        "- `Application`:\n",
        "  - Survival forests are specialized for time-to-event data.\n",
        "  - GRF is suited for causal inference.\n",
        "  - Others are general-purpose for classification/regression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v46Pny9IaaPp"
      },
      "source": [
        "### Advantages\n",
        "\n",
        "-   Reduces overfitting by averaging out noise across multiple trees.\n",
        "-   Handles high-dimensional data well (especially Random Forest and Extra Trees).\n",
        "-   Provides feature importance scores (e.g., in Random Forest).\n",
        "-   Parallelizable, as trees are trained independently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcYVM2Euafp5"
      },
      "source": [
        "### Limitations\n",
        "\n",
        "-   Less interpretable than a single decision tree.\n",
        "-   Computationally expensive for large datasets or many trees.\n",
        "-   May not perform as well as boosting methods (e.g., Gradient Boosting, XGBoost) for certain tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V6D09SpR2--"
      },
      "source": [
        "## Summary and Conclusion\n",
        "\n",
        "Bagging is a powerful ensemble learning technique that enhances the performance of tree-based models by reducing overfitting and increasing robustness. By leveraging multiple decision trees trained on different subsets of data, bagging can significantly improve predictive accuracy. Random Forest and Extra Trees are popular implementations of bagging that introduce additional randomness to further enhance model performance. Understanding the principles and applications of bagging is essential for building effective machine learning models in various domains.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recommended Resources and Further Reading\n",
        "\n",
        "Here are some recommended resources and further reading materials for learning about bagging (Bootstrap Aggregating), a key ensemble technique in machine learning. These include books, academic papers, online courses, and tutorials, with links where available:\n",
        "\n",
        "### Books\n",
        "\n",
        "1. **\"Introduction to Statistical Learning\" by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani**\n",
        "   - **Description**: Covers bagging as part of ensemble methods, with practical examples using random forests.\n",
        "   - **Link**: [Springer](https://link.springer.com/book/10.1007/978-1-4614-7138-7) (free PDF available at [ISL Book](http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf)).\n",
        "   - **Relevance**: Chapter 8 discusses bagging and its application in random forests.\n",
        "\n",
        "2. **\"The Elements of Statistical Learning\" by Trevor Hastie, Robert Tibshirani, and Jerome Friedman**\n",
        "   - **Description**: Provides an in-depth exploration of bagging, including its theoretical underpinnings and extensions.\n",
        "   - **Link**: [Springer](https://link.springer.com/book/10.1007/978-0-387-84858-7) (free PDF available at [ESL Book](https://hastie.su.domains/ElemStatLearn/)).\n",
        "   - **Relevance**: Chapter 10 details bagging and its relationship to other ensemble methods.\n",
        "\n",
        "### Academic Papers\n",
        "\n",
        "3. **\"Bagging Predictors\" by Leo Breiman (1996)**\n",
        "   - **Description**: The original paper introducing the bagging technique, explaining how bootstrap aggregation reduces variance.\n",
        "   - **Link**: [Springer](https://link.springer.com/article/10.1007/BF00058655) (free access with registration).\n",
        "   - **Relevance**: Foundational work that defines bagging and its impact on machine learning.\n",
        "\n",
        "4. **\"Random Forests\" by Leo Breiman (2001)**\n",
        "   - **Description**: Builds on bagging by introducing random feature selection, a key enhancement in random forests.\n",
        "   - **Link**: [Springer](https://link.springer.com/article/10.1023/A:1010933404324) (free access with registration).\n",
        "   - **Relevance**: Extends bagging concepts to create a robust ensemble method.\n",
        "\n",
        "### Online Courses and Tutorials\n",
        "5. **\"Machine Learning by Andrew Ng\" (Coursera)**\n",
        "   - **Description**: Introduces ensemble methods, including bagging, as part of supervised learning techniques.\n",
        "   - **Link**: [Coursera](https://www.coursera.org/learn/machine-learning) (free to audit, subscription for certificate).\n",
        "   - **Relevance**: Week 6 covers ensemble methods, including bagging.\n",
        "\n",
        "6. **\"Ensemble Methods in Machine Learning\" (DataCamp)**\n",
        "   - **Description**: A practical course focusing on bagging, boosting, and other ensemble techniques with hands-on exercises.\n",
        "   - **Link**: [DataCamp](https://www.datacamp.com/courses/ensemble-methods-in-python) (subscription required).\n",
        "   - **Relevance**: Includes detailed sections on bagging and its implementation.\n",
        "\n",
        "### Websites and Documentation\n",
        "7. **Scikit-Learn Documentation**\n",
        "   - **Description**: Offers guides and examples for implementing bagging using the `BaggingClassifier` and `BaggingRegressor` in Python.\n",
        "   - **Link**: [Scikit-Learn](https://scikit-learn.org/stable/modules/ensemble.html#bagging) (free).\n",
        "   - **Relevance**: Provides practical code examples and parameter explanations.\n",
        "\n",
        "8. **RDocumentation (randomForest Package)**\n",
        "   - **Description**: Documentation for R’s `randomForest` package, which uses bagging as a core component.\n",
        "   - **Link**: [RDocumentation](https://www.rdocumentation.org/packages/randomForest/versions/4.7-1.1) (free).\n",
        "   - **Relevance**: Practical resource for applying bagging in R.\n",
        "\n",
        "### Additional Resources\n",
        "\n",
        "9. **\"Bagging and Random Forests: A Simple Explanation\" (Towards Data Science)**\n",
        "   - **Description**: A blog post breaking down bagging and its evolution into random forests with intuitive examples.\n",
        "   - **Link**: [Towards Data Science](https://towardsdatascience.com/bagging-and-random-forests-a-simple-explanation-3b6b3ed4e003) (free).\n",
        "   - **Relevance**: Beginner-friendly overview with visualizations.\n",
        "\n",
        "10. **YouTube: \"Bagging in Machine Learning\" by StatQuest with Josh Starmer**\n",
        "    - **Description**: A video tutorial explaining bagging with clear animations and examples.\n",
        "    - **Link**: [YouTube](https://www.youtube.com/watch?v=D_2LkhMJcfY) (free).\n",
        "    - **Relevance**: Visual and intuitive introduction to bagging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "\n",
        "This section of tutorial will cover the following topics:\n",
        "\n",
        "2.1 [Bagged Trees](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-01-tree-based-models-bagging-bagged-trees-r.ipynb)\n",
        "    \n",
        "2.2 [Random Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-02-tree-based-models-bagging-randomforest-r.ipynb)\n",
        "    \n",
        "2.3 [Conditional Random Forests (cforest)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-03-tree-based-models-bagging-cforest-r.ipynb)\n",
        "    \n",
        "2.4 [Extremely Randomized Trees (Extra Trees)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-04-tree-based-models-bagging-extremely-randomized-trees-r.ipynb)\n",
        "    \n",
        "2.5 [Quantile Regression Forest (QRF)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-05-tree-based-models-bagging-quantile-regression-forest-r.ipynb)\n",
        "    \n",
        "2.6 [Random Forests Quantile Classifier](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-06-tree-based-models-bagging-quantile-classifier-forest-r.ipynb)\n",
        "    \n",
        "2.7 [Random Survival Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-07-tree-based-models-bagging-random-survival-forest-r.ipynb)\n",
        "    \n",
        "2.8 [Generalized Random Forests (GRF)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-00-tree-based-models-bagging-grf-introduction-r.ipynb)\n",
        "\n",
        "    \n",
        "2.8.1 [Survial Forests (SF)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-01-tree-based-models-bagging-grf-survival-forest-r.ipynb)\n",
        "      \n",
        "\n",
        "2.8.2 [Causal Forests (CF)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-02-tree-based-models-bagging-grf-causal-forest-r.ipynb)\n",
        "      \n",
        "\n",
        "2.8.3 [Causal Survival Forests (CSF)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-03-tree-based-models-bagging-grf-causal-survival-forest-r.ipynb)\n",
        "      \n",
        "\n",
        "2.8.4 [Multi-arm/multi-outcome Causal Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-04-tree-based-models-bagging-grf-arm-causal-forest-r.ipynb)\n",
        "      \n",
        "\n",
        "2.8.5 [Instrumental Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-05-tree-based-models-bagging-grf-instrumental-forest-r.ipynb)\n",
        "      \n",
        "\n",
        "2.8.6 [Linear Model Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-06-tree-based-models-bagging-grf-linear-model-forest-r.ipynb)\n",
        "   \n",
        "\n",
        "2.8.7 [Probability Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-07-tree-based-models-bagging-grf-probability-forest-r.ipynb)\n",
        "      \n",
        "\n",
        "2.8.8 [Regression Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-08-tree-based-models-bagging-grf-regression-forest-r.ipynb)\n",
        "      \n",
        "\n",
        "2.8.9 [Multi-task Regression Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-09-tree-based-models-bagging-grf-multitask-regression-forest-r.ipynb)\n",
        "\n",
        "     \n",
        "2.8.10 [Local Linear Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-10-tree-based-models-bagging-grf-local-linear-forest-r.ipynb)\n",
        "     \n",
        "\n",
        "2.8.11 [Boosted Regression Forest03-01](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-11-tree-based-models-bagging-grf-boosted-regression-forest-r.ipynb)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPMsECGcd1ZYZGEQHJlqADq",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
