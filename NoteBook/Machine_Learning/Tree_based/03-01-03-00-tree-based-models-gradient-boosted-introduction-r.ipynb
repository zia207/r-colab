{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-00-tree-based-models-gradient-boosted-introduction-r.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH9ES7RYWUdv"
      },
      "source": [
        "![alt text](http://drive.google.com/uc?export=view&id=1bLQ3nhDbZrCCqy_WCxxckOne2lgVvn3l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrQshXqjWS7x"
      },
      "source": [
        "# 3. Gradient Boosted Trees (GBT)\n",
        "\n",
        "GradGradient Boosted Trees (GBT) are a powerful ensemble learning method that builds decision trees sequentially to minimize a loss function using gradient descent. Known for high predictive accuracy, especially with structured data, GBT is widely used in various applications. This section summarizes how GBT works and highlights different models, including XGBoost, LightGBM, and CatBoost, each with unique strengths for specific scenarios.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3CdFuZZGhxQ"
      },
      "source": [
        "This section of the tutorial is divided into several parts, each focusing on a specific type of gradient boosted tree model. The models covered include:\n",
        "\n",
        "3.1 [Gradient Boosting Machine (GBM)](0https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-01-tree-based-models-gradient-boosted-gbm-r.ipynb)\n",
        "\n",
        "3.2 [Light Gradient Boosting Machine (lightGBM)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-02-tree-based-models-gradient-boosted-lightgbm-r.ipynb)\n",
        "\n",
        "3.3 [Extreme Gradient Boosting (XGboost)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-03-tree-based-models-gradient-boosted-xboost-r.ipynb)\n",
        "\n",
        "3.4 [Categorical Boosting (CatBoost)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-04-tree-based-models-gradient-boosted-catboost-r.ipynb)\n",
        "\n",
        "3.5 [Adaptive Boosting (AdaBoost)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-05-tree-based-models-gradient-boosted-adaboost-r.ipynb)\n",
        "\n",
        "3.6 [Gradient Boosted Survival Model](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-06-tree-based-models-gradient-boosted-survival-model-r.ipynb)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHx3FkVeWnxQ"
      },
      "source": [
        "## Overview\n",
        "\n",
        "**Gradient Boosted Trees (GBT)** is a powerful machine learning technique used for both regression and classification tasks. It is an ensemble method that builds multiple decision trees sequentially, where each tree corrects the errors of the previous ones by minimizing a loss function using gradient descent. GBT is widely used due to its high accuracy and ability to model complex, non-linear relationships in data. Popular implementations include **XGBoost**, **LightGBM**, and **CatBoost**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBkrRo0PG0Om"
      },
      "source": [
        "### Key Components\n",
        "\n",
        "-   `Loss Function`: Guides the optimization (e.g., mean squared error for regression, log-loss for classification, or custom losses).\n",
        "-   `Learning Rate*` Controls the contribution of each tree, balancing speed and accuracy.\n",
        "-   `Regularization`: Techniques like tree depth limits, L1/L2 penalties, or shrinkage prevent overfitting.\n",
        "-   `Gradient Descent`: Updates are made in the direction of the negative gradient of the loss function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYaVCS4QZn3Y"
      },
      "source": [
        "### How Gradient Boosted Trees (GBT) Work\n",
        "\n",
        "GBT builds an ensemble of decision trees in a **sequential** manner, unlike bagging methods (e.g., Random Forest), which build trees independently. The core idea is to iteratively improve predictions by fitting new trees to the residuals (errors) of the previous model, guided by gradient descent to minimize a specified loss function.\n",
        "\n",
        "1.  Initialization\n",
        "\n",
        "-   Start with an initial prediction, typically a constant value (e.g., the mean of the target variable for regression or log-odds for classification).\n",
        "\n",
        "2.  Compute Residuals\n",
        "\n",
        "-   Calculate the residuals (errors) between the current predictions and the actual target values based on a loss function (e.g., mean squared error for regression, log-loss for classification).\n",
        "\n",
        "3.  Fit a Decision Tree\n",
        "\n",
        "-   Train a new decision tree to predict the residuals. The tree is typically shallow (e.g., limited depth) to prevent overfitting.\n",
        "\n",
        "4.  Update Predictions\n",
        "\n",
        "-   Add the new tree’s predictions to the existing model, scaled by a **learning rate** (a small value, e.g., 0.1, to control the contribution of each tree and ensure stability).\n",
        "\n",
        "5.  Iterate\n",
        "\n",
        "-   Repeat steps 2–4 for a specified number of trees (iterations) or until the loss converges.\n",
        "\n",
        "6.  Final Prediction\n",
        "\n",
        "-   Combine the predictions from all trees (weighted by the learning rate) to produce the final output:\n",
        "    -   For `regression`: Sum the initial prediction and all tree outputs.\n",
        "    -   For `classification`: Combine tree outputs (e.g., log-odds) and apply a transformation (e.g., sigmoid for binary classification)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i8b2h1zaMDu"
      },
      "source": [
        "Below is a visual representation of the GBT process:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cfgtu_h1Zrg-"
      },
      "source": [
        "![alt text](http://drive.google.com/uc?export=view&id=1mX4VA1y7gj4dTO8FWcIY3QZGq08brJ_T)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpY-RTtYJYCh"
      },
      "source": [
        "### Advantages\n",
        "\n",
        "-   High predictive accuracy, often outperforming other algorithms.\n",
        "-   Captures complex, non-linear relationships and feature interactions.\n",
        "-   Flexible with various loss functions and regularization techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOH74tQeJauP"
      },
      "source": [
        "### Disadvantages\n",
        "\n",
        "-   Computationally intensive and sensitive to hyperparameter tuning.\n",
        "-   Less interpretable than single decision trees.\n",
        "-   Can overfit if not properly regularized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v46Pny9IaaPp"
      },
      "source": [
        "### Advantages\n",
        "\n",
        "-   Reduces overfitting by averaging out noise across multiple trees.\n",
        "-   Handles high-dimensional data well (especially Random Forest and Extra Trees).\n",
        "-   Provides feature importance scores (e.g., in Random Forest).\n",
        "-   Parallelizable, as trees are trained independently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcYVM2Euafp5"
      },
      "source": [
        "### Limitations\n",
        "\n",
        "-   Less interpretable than a single decision tree.\n",
        "-   Computationally expensive for large datasets or many trees.\n",
        "-   May not perform as well as boosting methods (e.g., Gradient Boosting, XGBoost) for certain tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyvQBNgjLCiq"
      },
      "source": [
        "## Different Types of GBT Models\n",
        "\n",
        "Here's a brief overview of XGBoost, LightGBM, CatBoost, AdaBoost, and GBM models, focusing on their key characteristics and use cases:\n",
        "\n",
        "1.  **GBM (Gradient Boosting Machine)**:\n",
        "\n",
        "   -   `Overview`: A general gradient boosting framework that builds trees sequentially, minimizing a loss function (e.g., mean squared error) via gradient descent.\n",
        "   -   `Key Features`: Flexible loss functions, supports regression and classification, and focuses on reducing residuals in each step.\n",
        "   -   `Use Cases`: Applied in predictive modeling tasks like financial forecasting, medical diagnosis, or customer churn prediction.\n",
        "   -   `Strengths`: Strong theoretical foundation, customizable, and effective for small-to-medium datasets.\n",
        "   -   `Weaknesses`: Slower than optimized frameworks like XGBoost or LightGBM; sensitive to hyperparameter settings.\n",
        "    \n",
        "2.  **LightGBM (Light Gradient Boosting Machine)**:\n",
        "\n",
        "   -   `Overview`: A gradient boosting framework by Microsoft, optimized for efficiency and large datasets. It uses histogram-based learning and leaf-wise tree growth.\n",
        "   -   `Key Features`: Faster training than XGBoost, lower memory usage, and support for categorical features without one-hot encoding.\n",
        "   -   `Use Cases`: Suitable for large-scale datasets in tasks like fraud detection, recommendation systems, and time-series forecasting.\n",
        "   -   `Strengths`: High speed, memory efficiency, and good performance on imbalanced data.\n",
        "   -   `Weaknesses`: May overfit on small datasets; less interpretable due to leaf-wise splitting.\n",
        "   \n",
        "3.  **XGBoost (Extreme Gradient Boosting)**:\n",
        "\n",
        "   -   `Overview`: An optimized gradient boosting framework designed for speed and performance. It uses a regularized objective function to reduce overfitting and supports parallel tree building.\n",
        "   -   `Key Features`: Handles missing values, supports custom loss functions, and includes L1/L2 regularization. It uses a second-order approximation for optimization.\n",
        "   -   `Use Cases`: Widely used in structured/tabular data tasks like classification, regression, and ranking problems (e.g., Kaggle competitions).\n",
        "   -   `Strengths`: High accuracy, scalability, and robust handling of diverse datasets.\n",
        "   -   `Weaknesses`: Can be computationally intensive and requires careful hyperparameter tuning.\n",
        "   \n",
        "4.  **CatBoost (Categorical Boosting)**:\n",
        "\n",
        "   -   `Overview`: A gradient boosting library by Yandex, designed to handle categorical features natively. It uses ordered boosting to reduce overfitting.\n",
        "   -   `Key Features`: Automatic categorical feature encoding, robust to noisy data, and symmetric tree structures for faster predictions.\n",
        "   -   `Use Cases`: Ideal for datasets with many categorical variables, such as customer segmentation or risk modeling.\n",
        "   -   `Strengths`: Reduces preprocessing needs, strong out-of-box performance, and good handling of categorical data.\n",
        "   -   `Weaknesses`: Slower training compared to LightGBM; less flexible for custom loss functions.\n",
        "   \n",
        "5.  **AdaBoost (Adaptive Boosting)**:\n",
        "\n",
        "  -   Overview: A boosting algorithm that combines weak learners (e.g., shallow decision trees) by focusing on misclassified samples in each iteration.\n",
        "  -   Key Features: Assigns weights to samples, increasing weights for misclassified ones, and combines predictions via weighted voting or averaging.\n",
        "  -   Use Cases: Used in simpler classification tasks or when interpretability is needed, like face detection or text classification.\n",
        "  -   Strengths: Simple, less prone to overfitting than some other boosting methods, and works well with weak models.\n",
        "  -   Weaknesses: Sensitive to noisy data and outliers; less competitive compared to modern gradient boosting methods.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJeWlbNGLG7y"
      },
      "source": [
        "Below is a table summarizing the key differences between **GBM**, **lightGNM**, **XGBoost**, **CatBoost**, and **AdaBoost**, based on algorithmic approach, categorical feature handling, speed/scalability, overfitting/robustness, tree structure, ease of use, and use case suitability.\n",
        "\n",
        "| **Aspect** | **XGBoost** | **LightGBM** | **CatBoost** | **AdaBoost** | **GBM** |\n",
        "|------------|------------|------------|------------|------------|------------|\n",
        "| **Algorithmic Approach** | Gradient boosting with second-order approximation, L1/L2 regularization | Histogram-based gradient boosting, leaf-wise tree growth | Ordered boosting, symmetric trees | Weight-based boosting, combines weak learners via voting | Standard gradient boosting, minimizes loss via gradient descent |\n",
        "| **Categorical Features** | Requires one-hot encoding or preprocessing | Native support via feature value splitting | Native, automatic encoding with target statistics | Requires preprocessing (e.g., one-hot encoding) | Requires preprocessing (e.g., one-hot encoding) |\n",
        "| **Speed & Scalability** | Fast, parallelized, but memory-intensive | Fastest, low memory via histogram binning, great for large datasets | Slower training, fast predictions, good for medium datasets | Fast for small datasets, scales poorly | Slower, less optimized for large datasets |\n",
        "| **Overfitting & Robustness** | Regularization reduces overfitting, sensitive to noise without tuning | Leaf-wise growth risks overfitting on small data, needs parameter control | Ordered boosting reduces overfitting, robust to noisy/categorical data | Less overfitting, highly sensitive to outliers/noise | Prone to overfitting without tuning, no built-in regularization |\n",
        "| **Tree Structure** | Level-wise growth, balanced trees | Leaf-wise growth, deeper asymmetric trees | Symmetric trees for stability and speed | Shallow trees (e.g., stumps) as weak learners | Level-wise growth, balanced trees |\n",
        "| **Ease of Use & Tuning** | Needs careful tuning (learning rate, max depth) | Less tuning than XGBoost, but leaf-wise needs attention | Minimal tuning, strong out-of-box performance | Simple, few parameters, less competitive | Extensive tuning needed, less user-friendly |\n",
        "| **Use Case Suitability** | Structured data, Kaggle, ranking tasks | Large-scale, high-dimensional (e.g., fraud detection, recommendations) | Categorical-heavy data (e.g., customer segmentation, risk modeling) | Simple tasks, interpretability (e.g., text classification, face detection) | Custom loss functions, small/medium predictive tasks (e.g., forecasting) |\n",
        "\n",
        "This table highlights the trade-offs, with **LightGBM** excelling in speed, **CatBoost** in categorical feature handling, **XGBoost** in flexibility, **AdaBoost** in simplicity, and **GBM** in customization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgCK8oBxLKlW"
      },
      "source": [
        "## R-Packages for GBT Models\n",
        "\n",
        "Below is a table listing R packages for **GBM**, **lightGNM**, **XGBoost**, **CatBoost**, and **AdaBoost**, models, along with their key details. These packages enable the implementation of these boosting algorithms in R for machine learning tasks.\n",
        "\n",
        "| **Model** | **R Package** | **Description** | **Installation** | **Key Features** |\n",
        "|---------------|---------------|---------------|---------------|---------------|\n",
        "| **XGBoost** | `xgboost` | A scalable and efficient implementation of extreme gradient boosting. | `install.packages(\"xgboost\")` | Parallel tree boosting, supports custom loss functions, handles missing values, L1/L2 regularization. |\n",
        "| **LightGBM** | `lightgbm` | A high-performance gradient boosting framework optimized for speed and memory. | `install.packages(\"lightGBM\")` | Histogram-based learning, leaf-wise tree growth, native categorical feature support, GPU support. |\n",
        "| **CatBoost** | `catboost` | A gradient boosting library designed for categorical feature handling. | for linux: `devtools::install_url('https://github.com/catboost/catboost/releases/ download/v1.2.2/catboost-R-Linux-1.2.2.tgz`) | Automatic categorical feature encoding, symmetric trees, ordered boosting, GPU support. |\n",
        "| **AdaBoost** | `adabag` | Implements adaptive boosting for classification tasks. | `install.packages(\"adabag\")` | Focuses on reweighting misclassified samples, supports decision stumps, simple to use. |\n",
        "| **GBM** | `gbm` | General gradient boosting machine for regression and classification. | `install.packages(\"gbm\")` | Flexible loss functions, supports regression/classification, but slower and less optimized. |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Topics Covered\n",
        "\n",
        "This section of the tutorial is divided into several parts, each focusing on a specific type of gradient boosted tree model. The models covered include:\n",
        "\n",
        "3.1 [Gradient Boosting Machine (GBM)](0https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-01-tree-based-models-gradient-boosted-gbm-r.ipynb)\n",
        "\n",
        "3.2 [Light Gradient Boosting Machine (lightGBM)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-02-tree-based-models-gradient-boosted-lightgbm-r.ipynb)\n",
        "\n",
        "3.3 [Extreme Gradient Boosting (XGboost)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-03-tree-based-models-gradient-boosted-xboost-r.ipynb)\n",
        "\n",
        "3.4 [Categorical Boosting (CatBoost)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-04-tree-based-models-gradient-boosted-catboost-r.ipynb)\n",
        "\n",
        "3.5 [Adaptive Boosting (AdaBoost)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-05-tree-based-models-gradient-boosted-adaboost-r.ipynb)\n",
        "\n",
        "3.6 [Gradient Boosted Survival Model](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-06-tree-based-models-gradient-boosted-survival-model-r.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V6D09SpR2--"
      },
      "source": [
        "## Summary and Conclusion\n",
        "\n",
        "Gradient Boosted Trees (GBT) are a powerful ensemble learning technique that builds decision trees sequentially to minimize a loss function using gradient descent. They excel in predictive accuracy, especially for structured data, and are widely used in various applications. This next section provides an overview of GBT, how it works, and the different types of GBT models available. Each model has unique features and strengths, making them suitable for different scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZtIoviVSGOk"
      },
      "source": [
        "## Refereences\n",
        "\n",
        "1.  *The Elements of Statistical Learning* by Hastie et al. (2009). Covers boosting theory. Springer, ISBN: 978-0-387-84857-0. Free PDF: https://hastie.su.domains/ElemStatLearn/\n",
        "2.  *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow* by Géron (2022). Practical GBT examples. O’Reilly, ISBN: 978-1-098-12597-4.\n",
        "3.  *Greedy Function Approximation: A Gradient Boosting Machine* by Friedman (2001). The Annals of Statistics, DOI: 10.1214/aos/1013203451.\n",
        "4.  *Gradient Boosting in R* by DataCamp. Hands-on XGBoost/LightGBM tutorial. https://www.datacamp.com/community/tutorials/xgboost-in-r\n",
        "5.  *Applied Predictive Modeling* by Kuhn & Johnson (2013). GBT in R with `caret`. Springer, ISBN: 978-1-4614-6848-6."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM9xCPZJR/1p7XWMr3NGIDm",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
