{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKSlVB4TFDBdcgcHoa6mLM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-06-00-tree-based-models-gradient-boosted-survival-model-introduction-r.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![alt text](http://drive.google.com/uc?export=view&id=1bLQ3nhDbZrCCqy_WCxxckOne2lgVvn3l)"
      ],
      "metadata": {
        "id": "IqHg6L5grcTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.6 Gradient Boosted Survival Model\n",
        "\n",
        "Gradient Boosted Survival Models are a powerful class of machine learning models that extend traditional survival analysis techniques by incorporating gradient boosting methods. These models are particularly useful for predicting time-to-event outcomes while handling censored data effectively. This notebook provides an introduction to diffrent types of  Gradient Boosted Survival Models, focusing on their implementation in R."
      ],
      "metadata": {
        "id": "ailTVUhkrfrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Overview\n",
        "\n",
        "A **Gradient Boosted Survival Model** is a machine learning approach that combines gradient boosting with survival analysis to predict the time until an event of interest occurs, such as failure, death, or churn. Survival models deal with time-to-event data, which is often censored (i.e., the event hasn’t occurred for some subjects during the observation period). Gradient boosting enhances these models by iteratively building an ensemble of weak learners (typically decision trees) to minimize a loss function tailored to survival data."
      ],
      "metadata": {
        "id": "naRrVgLurjNi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How It Works\n",
        "\n",
        "Gradient boosting iteratively adds decision trees, each correcting the errors of the previous ones, by optimizing a loss function (e.g., negative log-likelihood or a survival-specific loss). In survival analysis, the model accounts for:\n",
        "\n",
        "- `Censored data`: Observations where the event hasn’t occurred by the end of the study.\n",
        "- `Time-to-event`: Predicting not just if an event occurs but when.\n",
        "- `Risk or hazard`: Estimating the probability of the event at different time points.\n",
        "\n",
        "The model outputs predictions like survival probabilities, hazard functions, or cumulative risks over time, depending on the specific implementation."
      ],
      "metadata": {
        "id": "C4dnQDfQrkDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Components\n",
        "\n",
        "1. `Base Learners`: Typically decision trees, which are combined to form the final model.\n",
        "2. `Loss Function`: A survival-specific loss, such as partial likelihood (Cox model-inspired) or a pseudo-residual-based loss.\n",
        "3. `Gradient Descent`: Used to minimize the loss by adjusting the model in the direction of steepest descent.\n",
        "4. `Regularization`: Techniques like shrinkage (learning rate) or tree depth constraints prevent overfitting."
      ],
      "metadata": {
        "id": "L3s9jQU_rqm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Types of Gradient Boosted Survival Models\n",
        "\n",
        "There are several types of gradient boosted survival models, each tailored to specific survival analysis frameworks or assumptions. Below, I’ll provide a brief description of **Cox Proportional Hazards-Based Gradient Boosting**,  **Gradient Boosting Survival Tree (GBST)**, **Accelerated Failure Time (AFT) Gradient Boosting**, and **Survival Gradient Boosting with Custom Loss Functions**, focusing on their methodologies, assumptions, and applications. I’ll then compare them to highlight their differences and similarities, ensuring a clear understanding of how they fit within the broader framework of gradient boosted survival models. The explanations will be concise yet comprehensive, building on the context of survival analysis and gradient boosting."
      ],
      "metadata": {
        "id": "SHyrkkLCrvll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cox Proportional Hazards-Based Gradient Boosting\n",
        "\n",
        "Cox Proportional Hazards-Based Gradient Boosting is a machine learning approach that extends the Cox proportional hazards model by using gradient boosting to model the log-hazard function as a sum of decision trees. It assumes that the hazard ratios are constant over time (proportional hazards) and optimizes the negative log-partial likelihood to predict risk scores for survival data, accounting for censoring. This method excels in handling complex, non-linear relationships between covariates and survival outcomes, making it suitable for clinical applications like predicting patient survival in medical studies."
      ],
      "metadata": {
        "id": "WdWXD2sxrwqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Gradient Boosting Survival Tree (GBST)\n",
        "\n",
        "Gradient Boosting Survival Tree (GBST) is a specialized gradient boosting approach for survival analysis that uses survival trees as base learners, designed to handle time-to-event data with censoring. These trees split nodes based on survival-specific criteria, such as log-rank tests, to maximize differences in survival outcomes. GBST iteratively combines these trees to optimize a survival-related loss function, producing survival probabilities or hazard functions. Its strength lies in its interpretability and ability to model complex survival patterns without strict assumptions like proportional hazards, making it ideal for medical research applications like predicting patient survival times."
      ],
      "metadata": {
        "id": "jyVlxLaIry2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Accelerated Failure Time (AFT) Gradient Boosting\n",
        "\n",
        "Accelerated Failure Time (AFT) Gradient Boosting is a machine learning method for survival analysis that models the log-survival time as a function of covariates, assuming that predictors accelerate or decelerate the time to an event. It uses gradient boosting to combine decision trees, optimizing a likelihood-based loss function (e.g., Weibull or log-normal) that accounts for censored data. Unlike hazard-based models, AFT directly predicts survival times, making it intuitive for applications like predicting time to machine failure or patient recovery."
      ],
      "metadata": {
        "id": "6H-7QfYGr2s0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Survival Gradient Boosting with Custom Loss Functions\n",
        "\n",
        "\n",
        "Survival Gradient Boosting with Custom Loss Functions is a flexible machine learning approach for survival analysis that uses gradient boosting to optimize user-defined loss functions, such as the smoothed concordance index or integrated Brier score. Unlike standard survival models, it allows tailored loss functions to capture complex survival patterns without strict assumptions like proportional hazards. By combining decision trees iteratively, it predicts outcomes like survival probabilities or risk rankings, making it ideal for research settings with non-standard problems, such as high-dimensional genomic data."
      ],
      "metadata": {
        "id": "8OsNN7NOr5F4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Differences\n",
        "\n",
        "Below is a detailed comparison of **GBST**, **Cox-Based Gradient Boosting**, **AFT Gradient Boosting**, and **Custom Loss Gradient Boosting**:\n",
        "\n",
        "| **Aspect**                     | **GBST**                                  | **Cox-Based**                           | **AFT-Based**                           | **Custom Loss**                         |\n",
        "|--------------------------------|-------------------------------------------|-----------------------------------------|-----------------------------------------|-----------------------------------------|\n",
        "| `Base Learners`              | Survival trees with survival-specific splits (e.g., log-rank test). | Standard decision trees.                | Standard decision trees.                | Standard decision trees or custom.      |\n",
        "| `Splitting Criteria`         | Log-rank test, Kaplan-Meier differences, or hazard-based metrics. | Optimized for Cox partial likelihood.   | Optimized for log-survival time differences. | Depends on loss (e.g., concordance index). |\n",
        "| `Loss Function`              | Survival-specific (e.g., Cox likelihood, log-rank-based). | Negative log-partial likelihood (Cox). | AFT likelihood (e.g., Weibull, log-normal). | Custom (e.g., smoothed concordance index, Brier score). |\n",
        "| `Assumptions`                | Non-parametric or semi-parametric, no strict PH assumption. | Proportional hazards (PH).              | Log-linear effect on survival time, possible parametric distribution. | Minimal, depends on loss function.      |\n",
        "| `Output`                     | Survival probabilities, hazard functions, or risk scores. | Risk scores (log-hazard ratios).        | Predicted survival times.               | Varies (risk scores, probabilities, rankings). |\n",
        "| `Flexibility`                | Moderate (tied to survival trees).        | Moderate (PH assumption limits flexibility). | Moderate (distribution assumption).     | High (customizable loss functions).     |\n",
        "| `Interpretability`           | High for tree splits (survival-specific). | Moderate (risk scores less intuitive).  | High (direct time predictions).         | Varies (depends on loss complexity).    |\n",
        "| `Use Case`                   | Complex survival patterns, interpretable splits (e.g., medical research). | PH-assumed survival tasks (e.g., clinical trials). | Time prediction tasks (e.g., industrial failure). | Non-standard survival problems (e.g., genomics). |\n",
        "\n"
      ],
      "metadata": {
        "id": "OG8a4Xabr7b2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary and Conclusion\n",
        "\n",
        "Gradient Boosted Survival Models, including Cox Proportional Hazards-Based Gradient Boosting, Gradient Boosting Survival Tree (GBST), Accelerated Failure Time (AFT) Gradient Boosting, and Survival Gradient Boosting with Custom Loss Functions, offer powerful tools for survival analysis. Each model has unique strengths and assumptions, making them suitable for different types of survival data and research questions. GBST excels in interpretability and flexibility, while Cox-based models are robust for proportional hazards scenarios. AFT models provide intuitive time predictions, and custom loss functions allow tailored approaches to complex survival problems. Understanding these models enables researchers to effectively analyze time-to-event data in various fields, particularly in medical research."
      ],
      "metadata": {
        "id": "_DPaQ-UKr9zy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## References\n",
        "\n",
        "1. Chen, T., & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. *KDD '16*, 785–794.  \n",
        "\n",
        "\n",
        "2. Hothorn, T., et al. (2006). Survival Ensembles. *Biostatistics, 7*(3), 355–373.  \n",
        "\n",
        "3. Li, K., et al. (2022). Efficient Gradient Boosting for Prognostic Biomarker Discovery. *Bioinformatics, 38*(6), 1631–1638.  \n",
        "\n",
        "4. Wang, Z., & Wang, C. Y. (2018). Gradient Boosting for Concordance Index. *Comput Math Methods Med, 2018*, 8734680.  \n",
        "  \n",
        "\n",
        "5. Zhang, H., et al. (2019). Gradient Boosting Survival Tree. *arXiv:1908.03385*.  \n"
      ],
      "metadata": {
        "id": "FmEOIzP2r_wO"
      }
    }
  ]
}