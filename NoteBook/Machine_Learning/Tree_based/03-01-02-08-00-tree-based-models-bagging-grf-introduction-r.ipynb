{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNU+Rzrd8BFKpJI12Jk0tWw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-00-tree-based-models-bagging-grf-introduction-r.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![alt text](http://drive.google.com/uc?export=view&id=1bLQ3nhDbZrCCqy_WCxxckOne2lgVvn3l)"
      ],
      "metadata": {
        "id": "pt4LOwtPnjto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.8 Generalized Random Forests (GRF)\n",
        "\n",
        "Generalized Random Forests (GRF) is a powerful extension of the traditional random forest algorithm, designed to handle a wide range of statistical tasks, particularly in causal inference and heterogeneous treatment effect estimation. This notebook provides an overview of GRF, its key concepts, and practical implementations using the `{grf}` package in R.\n"
      ],
      "metadata": {
        "id": "K7I-lUYgnvzz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section is part of the larger series on tree-based models and bagging techniques, focusing on advanced applications of random forests in statistical modeling and machine learning.\n",
        "\n",
        "\n",
        "2.8.1 [Survial Forests (SF)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-01-tree-based-models-bagging-grf-survival-forest-r.ipynb)\n",
        "\n",
        "      \n",
        "2.8.2 [Causal Forests (CF)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-02-tree-based-models-bagging-grf-causal-forest-r.ipynb)\n",
        "\n",
        "      \n",
        "2.8.3 [Causal Survival Forests (CSF)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-03-tree-based-models-bagging-grf-causal-survival-forest-r.ipynb)\n",
        "\n",
        "      \n",
        "2.8.4 [Multi-arm/multi-outcome Causal Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-04-tree-based-models-bagging-grf-arm-causal-forest-r.ipynb)\n",
        "\n",
        "      \n",
        "2.8.5 [Instrumental Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-05-tree-based-models-bagging-grf-instrumental-forest-r.ipynb)\n",
        "\n",
        "      \n",
        "2.8.6 [Linear Model Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-06-tree-based-models-bagging-grf-linear-model-forest-r.ipynb)\n",
        "\n",
        "   \n",
        "2.8.7 [Probability Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-07-tree-based-models-bagging-grf-probability-forest-r.ipynb)\n",
        "      \n",
        "2.8.8 [Regression Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-08-tree-based-models-bagging-grf-regression-forest-r.ipynb)\n",
        "\n",
        "      \n",
        "2.8.9 [Multi-task Regression Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-09-tree-based-models-bagging-grf-multitask-regression-forest-r.ipynb)\n",
        "     \n",
        "2.8.10 [Local Linear Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-10-tree-based-models-bagging-grf-local-linear-forest-r.ipynb)\n",
        "\n",
        "     \n",
        "2.8.11 [Boosted Regression Forest03-01](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-11-tree-based-models-bagging-grf-boosted-regression-forest-r.ipynb)"
      ],
      "metadata": {
        "id": "JTio3WTUcC90"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "Generalized Random Forests (GRF) is a machine learning framework that extends the classical random forest algorithm to handle a broader range of statistical tasks beyond simple regression and classification. It’s designed to estimate heterogeneous treatment effects, causal effects, and other complex relationships in data, particularly in observational studies or experiments. GRF is rooted in the idea of using decision trees to partition data but adapts the splitting criteria to focus on specific statistical goals, such as estimating causal effects or other user-defined parameters.\n",
        "\n",
        "Random forests are ensembles of decision trees where each tree is trained on a random subset of data and features. Predictions are aggregated (e.g., via averaging for regression or majority voting for classification). GRF builds on this by modifying how trees are constructed to target specific estimation problems. Unlike standard random forests, which optimize for predicting outcomes (e.g., minimizing mean squared error), GRF optimizes for estimating parameters like treatment effects or conditional means. It uses a flexible framework to estimate *heterogeneous effects* — how the effect of a treatment or intervention varies across different subgroups or conditions."
      ],
      "metadata": {
        "id": "fo_y05eMoJsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Features\n",
        "\n",
        "1. **Flexible Estimation**: Targets heterogeneous treatment effects, quantiles, or local parameters.\n",
        "2. **Local Splitting**: Splits trees to maximize heterogeneity in the target parameter.\n",
        "3. **Honest Splitting**: Separates data for splitting and estimation to reduce overfitting.\n",
        "4. **Asymptotic Guarantees**: Provides valid statistical inference (e.g., confidence intervals).\n",
        "5. **Handles Confounding**: Robust for observational data studies.\n",
        "6. **Implementation**: Available in R’s `grf` package for causal, quantile, and regression forests."
      ],
      "metadata": {
        "id": "XQC7YmaYoPON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How GRF Works\n",
        "\n",
        "GRF is rooted in the idea of using decision trees to partition data but adapts the splitting criteria to focus on specific statistical goals, such as estimating causal effects or other user-defined parameters.\n",
        "\n",
        "-   `Tree Splitting`: In GRF, trees are grown by splitting data to maximize heterogeneity in the target parameter (e.g., treatment effect) rather than just prediction accuracy. For example, in causal forests, splits are chosen to maximize differences in treatment effects across subgroups.\n",
        "\n",
        "-   `Honest Splitting`: GRF often uses “honest” estimation, where one subset of data is used to build the tree structure (splitting) and another to estimate effects within each leaf. This reduces overfitting and improves statistical validity.\n",
        "\n",
        "-   `Ensemble Estimation`: Like random forests, GRF aggregates estimates across many trees to produce robust, stable results.\n"
      ],
      "metadata": {
        "id": "xPoNP5YdosyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applications\n",
        "\n",
        "-   `Causal Inference`: Estimating treatment effects in randomized experiments or observational data studies (e.g., how a drug affects different patients).\n",
        "\n",
        "-   `Conditional Average Treatment Effect (CATE)`: GRF is particularly useful for estimating how treatment effects vary across covariates (e.g., age, income, or other features).\n",
        "\n",
        "-   `Instrumental Variables`: GRF can handle settings where treatment assignment is not fully randomized, using instrumental variables to estimate causal effects.\n",
        "\n",
        "-   `Quantile Regression`: Estimating conditional quantiles (e.g., median or other percentiles) instead of just means.\n",
        "\n",
        "-   `Policy Evaluation`: Identifying subgroups that benefit most from a policy or intervention."
      ],
      "metadata": {
        "id": "qS7xnKYgovaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advantages of GRF\n",
        "\n",
        "-   Handles high-dimensional data and complex interactions between variables.\n",
        "\n",
        "-   Provides robust estimates of heterogeneous effects without requiring strong parametric assumptions.\n",
        "\n",
        "-   Works well in both experimental and observational data settings.\n",
        "\n",
        "-   Open-source implementations (e.g., the `grf` package in R) make it accessible.\n"
      ],
      "metadata": {
        "id": "mbCPm99Gox3O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Limitations\n",
        "\n",
        "-   Computationally intensive, especially for large datasets or complex splitting criteria.\n",
        "\n",
        "-   Requires careful tuning of parameters (e.g., number of trees, minimum node size).\n",
        "\n",
        "-   In observational studies, results depend on assumptions like unconfoundedness (no unmeasured confounders).\n",
        "\n",
        "-   Interpretability of results can be challenging in very high-dimensional settings."
      ],
      "metadata": {
        "id": "i6n8NzM-oyfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generalized Random Forests (GRF) with R\n",
        "\n",
        "The [`{grf}`](https://grf-labs.github.io/grf/index.html) package in R can implement various Generalized Random Forest (GRF) models, including **Causal Forest**, **Causal Survival Forest**, **Multi-Arm/Multi-Outcome Causal Forest**, **Instrumental Forest**, **LM Forest**, **Probability Forest**, **Quantile Forest**, **Regression Forest**, **Multi-Task Regression Forest**, **Local Linear Forest**, **Boosted Regression Forest**, and **Survival Forest**.\n",
        "\n",
        "Here’s a brief description of each forest-based method, focusing on their core purpose and application in causal inference, machine learning, or statistical modeling:\n",
        "\n",
        "1.  **Causal Forest**: An extension of random forests for estimating heterogeneous treatment effects in causal inference. It uses decision trees to partition data and estimate treatment effects at different covariate levels, useful for identifying how treatment impacts vary across subgroups.\n",
        "\n",
        "2.  **Causal Survival Forest**: A variant of causal forests designed for survival data. It estimates treatment effects on time-to-event outcomes, accounting for censoring, and is used in settings like medical trials to assess treatment impacts on survival times.\n",
        "\n",
        "3.  **Multi-Arm/Multi-Outcome Causal Forest**: An extension of causal forests for multiple treatment arms or multiple outcome variables. It estimates treatment effects across different interventions or outcomes simultaneously, ideal for complex experiments with several treatments or endpoints.\n",
        "\n",
        "4.  **Instrumental Forest**: A method combining random forests with instrumental variable (IV) techniques. It estimates causal effects in the presence of unmeasured confounding by leveraging an instrumental variable that affects the treatment but not the outcome directly.\n",
        "\n",
        "5.  **LM Forest (Local Multi-Task Forest)**: Combines local linear regression with random forests to estimate treatment effects or predictions. It uses tree-based splits to define local regions and applies linear models within them, balancing flexibility and interpretability.\n",
        "\n",
        "6.  **Probability Forest**: A random forest variant for estimating conditional probabilities, such as class probabilities in classification tasks. It predicts the probability of an outcome given covariates, useful in applications requiring probabilistic outputs.\n",
        "\n",
        "7.  **Quantile Forest**: An adaptation of random forests to estimate conditional quantiles of the outcome distribution (e.g., median, 90th percentile). It’s used for understanding the distribution of outcomes, especially in regression tasks with heterogeneous effects.\n",
        "\n",
        "8.  **Regression Forest**: A standard random forest for regression tasks, predicting continuous outcomes by averaging predictions from multiple decision trees. It’s robust to non-linear relationships and widely used for general predictive modeling.\n",
        "\n",
        "9.  **Multi-Task Regression Forest**: An extension of regression forests that predicts multiple related continuous outcomes simultaneously. It leverages shared information across tasks to improve prediction accuracy, useful in multi-output regression problems.\n",
        "\n",
        "10. **Local Linear Forest**: Combines random forests with local linear regression to estimate smooth, non-linear relationships. It uses tree-based splits to define local regions and fits linear models within them, improving prediction accuracy for complex data.\n",
        "\n",
        "11. **Boosted Regression Forest**: Combines boosting with random forests to improve regression performance. It iteratively builds trees, focusing on correcting errors from previous trees, enhancing predictive power for complex datasets.\n",
        "\n",
        "12. **Survival Forest**: A random forest-based method for survival analysis, predicting time-to-event outcomes while handling censoring. It estimates survival probabilities or hazard functions, commonly used in medical and reliability studies.\n",
        "\n",
        "Each method leverages the flexibility of tree-based models while tailoring to specific tasks like causal inference, survival analysis, or multi-task learning. For deeper details or implementation, tools like R’s `grf` package or Python’s `econml` library are often used."
      ],
      "metadata": {
        "id": "xdsQYC1Jo4lh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Differences Among Generalized Forest-Based Methods\n",
        "\n",
        "The forest-based methods are built on the random forest framework but differ in their objectives, data types, and approaches to modeling. Below, the summary of the main differences among **Causal Forest**, **Causal Survival Forest**, **Multi-Arm/Multi-Outcome Causal Forest**, **Instrumental Forest**, **LM Forest**, **Probability Forest**, **Quantile Forest**, **Regression Forest**, **Multi-Task Regression Forest**, **Local Linear Forest**, **Boosted Regression Forest**, and **Survival Forest**, focusing on their purpose, outcome type, key methodology, and use case.\n",
        "\n",
        "For clarity, group similar methods and highlight distinctions concisely.\n",
        "\n",
        "1. Core Objective\n",
        "\n",
        "-   `Causal Inference Focus`:\n",
        "\n",
        "    -   **Causal Forest**: Estimates heterogeneous treatment effects (how treatment impact varies across covariates) for continuous or binary outcomes. Used in randomized or observational studies to understand who benefits most from a treatment.\n",
        "    -   **Causal Survival Forest**: Focuses on treatment effects for time-to-event (survival) outcomes, accounting for censoring. Tailored for survival data in causal inference, e.g., medical trials.\n",
        "    -   **Multi-Arm/Multi-Outcome Causal Forest**: Extends Causal Forest to handle multiple treatments (arms) or multiple outcomes simultaneously, estimating treatment effects across diverse scenarios.\n",
        "    -   **Instrumental Forest**: Estimates causal effects in the presence of unmeasured confounding using an instrumental variable (IV), which affects treatment but not the outcome directly.\n",
        "\n",
        "-   `Prediction Focus`:\n",
        "\n",
        "-   **Regression Forest**: Predicts continuous outcomes using standard random forest regression. General-purpose for non-linear regression tasks.\n",
        "\n",
        "-   **Multi-Task Regression Forest**: Predicts multiple continuous outcomes simultaneously, leveraging shared information across related tasks for improved acuracy.\n",
        "\n",
        "-   **Probability Forest**: Estimates conditional probabilities for categorical outcomes, used in classification tasks requiring probabilistic outputs.\n",
        "\n",
        "-   **Quantile Forest**: Estimates conditional quantiles (e.g., median, 90th percentile) of the outcome distribution, capturing heterogeneity in outcome distributions.\n",
        "\n",
        "-   **Survival Forest**: Predicts survival probabilities or hazard functions for time-to-event data, handling censoring without a causal inference focus.\n",
        "\n",
        "-   **Hybrid/Enhanced Prediction**:\n",
        "\n",
        "-   **LM Forest**: Combines random forests with local linear regression to estimate treatment effects or predictions, balancing tree-based flexibility with linear smoothing.\n",
        "\n",
        "-   **Local Linear Forest**: Similar to LM Forest but emphasizes smooth, non-linear predictions by fitting local linear models within tree-defined regions.\n",
        "\n",
        "-   **Boosted Regression Forest**: Enhances regression forests with boosting, iteratively correcting errors to improve prediction accuracy for complex continuous outcomes.\n",
        "\n",
        "2. Outcome Type\n",
        "\n",
        "-   `Continuous Outcomes`: Regression Forest, Multi-Task Regression Forest, Local Linear Forest, Boosted Regression Forest, and LM Forest (for continuous predictions or treatment effects).\n",
        "-   `Categorical/Probabilistic Outcomes`: Probability Forest (focuses on class probabilities).\n",
        "-   `Quantile Outcomes`: Quantile Forest (estimates specific quantiles of continuous outcomes).\n",
        "-   `Time-to-Event (Survival) Outcomes`: Survival Forest, Causal Survival Forest (with Causal Survival Forest focusing on treatment effects).\n",
        "-   `Flexible/Multiple Outcomes`: Causal Forest (continuous or binary), Multi-Arm/Multi-Outcome Causal Forest (multiple outcome types), Instrumental Forest (typically continuous or binary).\n",
        "\n",
        "3. Key Methodological Differences\n",
        "\n",
        "-   `Causal vs. Predictive`:\n",
        "    -   Causal methods (Causal Forest, Causal Survival Forest, Multi-Arm/Multi-Outcome Causal Forest, Instrumental Forest) focus on estimating causal effects, often requiring assumptions like unconfoundedness or valid instruments (for Instrumental Forest).\n",
        "    -   Predictive methods (Regression Forest, Multi-Task Regression Forest, Probability Forest, Quantile Forest, Survival Forest) focus on accurate prediction without causal assumptions.\n",
        "    -   Hybrid methods (LM Forest, Local Linear Forest) blend causal estimation or prediction with local linear adjustments for smoother results.\n",
        "-   `Handling Confounding`:\n",
        "    -   Instrumental Forest uniquely addresses unmeasured confounding using an IV, unlike other causal forests that assume no unmeasured confounders.\n",
        "    -   Causal Forest and its variants rely on observed covariates to control for confounding.\n",
        "-   `Model Structure`:\n",
        "    -   Standard tree-based: Regression Forest, Probability Forest, Quantile Forest, Survival Forest, Causal Forest, Causal Survival Forest, Multi-Arm/Multi-Outcome Causal Forest, Instrumental Forest.\n",
        "    -   Boosting-enhanced: Boosted Regression Forest uses iterative error correction, unlike standard random forests.\n",
        "    -   Linear-enhanced: LM Forest and Local Linear Forest incorporate local linear regression within tree splits for smoother estimates.\n",
        "-   `Multi-Task Capability`:\n",
        "    -   Multi-Task Regression Forest and Multi-Arm/Multi-Outcome Causal Forest handle multiple outcomes or treatments, leveraging shared information across tasks.\n",
        "    -   Others focus on single outcomes or treatments.\n",
        "\n",
        "4. Use Case Differences\n",
        "\n",
        "-   `Causal Inference`:\n",
        "\n",
        "-   **Causal Forest**: Personalized medicine, policy evaluation (e.g., estimating treatment effects of a drug or intervention across subgroups).\n",
        "\n",
        "-   **Causal Survival Forest**: Medical trials with survival outcomes (e.g., effect of a drug on patient survival time).\n",
        "\n",
        "-   **Multi-Arm/Multi-Outcome Causal Forest**: Complex experiments, e.g., comparing multiple drugs or evaluating multiple health outcomes.\n",
        "\n",
        "-   **Instrumental Forest**: Observational studies with confounding, e.g., evaluating policy impacts using an IV like random encouragement.\n",
        "\n",
        "-   `Prediction`:\n",
        "\n",
        "-   **Regression Forest**: General regression tasks, e.g., predicting house prices or sales.\n",
        "\n",
        "-   **Multi-Task Regression Forest**: Multi-output prediction, e.g., forecasting multiple economic indicators.\n",
        "\n",
        "-   **Probability Forest**: Classification with probability outputs, e.g., predicting customer churn likelihood.\n",
        "\n",
        "-   **Quantile Forest**: Risk analysis, e.g., estimating extreme values like 95th percentile losses in finance.\n",
        "\n",
        "-   **Survival Forest**: Survival prediction, e.g., estimating patient survival probabilities in medical studies.\n",
        "\n",
        "-   `Enhanced Prediction`:\n",
        "\n",
        "-   **LM Forest/Local Linear Forest**: Scenarios requiring smooth predictions or causal estimates, e.g., economic forecasting or treatment effect estimation with interpretable local trends.\n",
        "\n",
        "-   **Boosted Regression Forest**: High-accuracy regression for complex datasets, e.g., predicting energy consumption.\n",
        "\n",
        "5. Data Requirements\n",
        "\n",
        "-   `Causal Methods`: Require treatment assignment data (and an IV for Instrumental Forest). Causal Survival Forest needs survival data (time-to-event, censoring indicators).\n",
        "-   `Survival Forest`: Needs survival data but no treatment information.\n",
        "-   `Others`: Standard covariates and outcomes (continuous for Regression/Quantile Forests, categorical for Probability Forest, multiple outcomes for Multi-Task Regression Forest)."
      ],
      "metadata": {
        "id": "HlwZF3NJpR-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary Table of Key Differences\n",
        "\n",
        "| Method | Objective | Outcome Type | Key Feature | Use Case |\n",
        "|---------------|---------------|---------------|---------------|---------------|\n",
        "| Causal Forest | Causal effect estimation | Continuous/Binary | Heterogeneous treatment effects | Policy evaluation, personalized medicine |\n",
        "| Causal Survival Forest | Causal effect on survival | Time-to-event | Treatment effects with censoring | Medical trials |\n",
        "| Multi-Arm/Multi-Outcome Causal Forest | Multi-treatment/outcome causal effects | Flexible | Multiple arms/outcomes | Complex experiments |\n",
        "| Instrumental Forest | Causal effect with confounding | Continuous/Binary | Uses instrumental variable | Observational studies |\n",
        "| LM Forest | Prediction/Causal estimation | Continuous | Local linear regression in trees | Smooth causal/predictive modeling |\n",
        "| Probability Forest | Probability estimation | Categorical | Conditional class probabilities | Classification tasks |\n",
        "| Quantile Forest | Quantile estimation | Continuous | Conditional quantiles | Risk analysis, distribution modeling |\n",
        "| Regression Forest | Prediction | Continuous | Standard random forest regression | General regression tasks |\n",
        "| Multi-Task Regression Forest | Multi-output prediction | Continuous (multiple) | Shared learning across outputs | Multi-output forecasting |\n",
        "| Local Linear Forest | Smooth prediction | Continuous | Local linear smoothing | Smooth non-linear regression |\n",
        "| Boosted Regression Forest | Enhanced prediction | Continuous | Boosting with random forests | High-accuracy regression |\n",
        "| Survival Forest | Survival prediction | Time-to-event | Survival probabilities, censoring | Medical/reliability analysis |\n"
      ],
      "metadata": {
        "id": "H-cJfJEwpW9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary and Conclusion\n",
        "\n",
        "Generalized Random Forests (GRF) is a versatile framework that extends traditional random forests to address complex statistical tasks, particularly in causal inference and heterogeneous treatment effect estimation. By adapting tree-based methods to focus on specific estimation goals, GRF provides robust, flexible tools for analyzing diverse data types and relationships. Following the principles outlined in this notebook, users can effectively implement GRF models using the `{grf}` package in R, enabling advanced statistical analysis and machine learning applications."
      ],
      "metadata": {
        "id": "tRWmOMDDpZnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "1.  Athey, Susan, Julie Tibshirani, and Stefan Wager. \"Generalized Random Forests\". Annals of Statistics, 47(2), 2019.\n",
        "\n",
        "2.  Wager, Stefan, and Susan Athey. \"Estimation and Inference of Heterogeneous Treatment Effects using Random Forests\". Journal of the American Statistical Association, 113(523), 2018.\n",
        "\n",
        "3.  Cui, Y., Kosorok, M. R., Sverdrup, E., Wager, S., & Zhu, R. (2023). Estimating Heterogeneous Treatment Effects with Right-Censored Data via Causal Survival Forests. *Journal of the Royal Statistical Society: Series B*, 85(2).[ ](https://grf-labs.github.io/grf/reference/causal_survival_forest.html)\n",
        "\n",
        "4.  Sverdrup, E., & Wager, S. (2024). Treatment Heterogeneity with Right-Censored Outcomes Using grf. *ASA Lifetime Data Science Newsletter*.[ ](https://search.r-project.org/CRAN/refmans/grf/html/causal_survival_forest.html)\n",
        "\n",
        "5.  [generalized random forests](https://grf-labs.github.io/grf/)\n",
        "\n",
        "6.  [grf Causal Survival Forest documentation](https://grf-labs.github.io/grf/articles/causal_survival_forest.html)\n",
        "\n",
        "7.  [Original Causal Survival Forests paper](https://arxiv.org/abs/2006.09639)\n",
        "\n",
        "8.  [Random Survival Forests](https://cran.r-project.org/web/packages/randomForestSRC/vignettes/randomForestSRC-Survival.pdf)"
      ],
      "metadata": {
        "id": "c9tSHMqCpcRN"
      }
    }
  ]
}