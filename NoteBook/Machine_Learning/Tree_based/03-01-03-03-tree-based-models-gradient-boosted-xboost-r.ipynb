{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN8IPeL2vRgBxo/us5TQTxM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-03-tree-based-models-gradient-boosted-xboost-r.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![alt text](http://drive.google.com/uc?export=view&id=1bLQ3nhDbZrCCqy_WCxxckOne2lgVvn3l)"
      ],
      "metadata": {
        "id": "4qrItz_mJNWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.3 Extreme Gradient Boosting (XGBoost)\n",
        "\n",
        "XGBoost (Extreme Gradient Boosting) is a scalable and efficient machine learning algorithm for supervised learning tasks, particularly effective for regression, classification, and ranking problems. It is an optimized implementation of gradient boosting, a method that builds an ensemble of weak prediction models (typically decision trees) in a sequential manner to produce a strong predictive model. XGBoost is widely used due to its high performance, flexibility, and ability to handle large datasets with missing values, regularization, and parallel processing. This notebook will provide an overview of XGBoost, explain how it works, and demonstrate how to implement it in R for classification and regression tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "Dzp9ZseROTcY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "XGBoost constructs a model by combining multiple decision trees, where each tree corrects the errors of the previous ones. It uses gradient boosting, which minimizes a loss function by iteratively adding trees, with each tree trained to predict the residuals (errors) of the prior ensemble. XGBoost enhances this process with additional features like regularization, second-order gradient optimization, and efficient handling of sparse data.\n"
      ],
      "metadata": {
        "id": "mLkAu88QToVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Features of XGBoost\n",
        "\n",
        "- **Gradient Boosting Framework**: Combines weak learners (typically decision trees) into a strong predictive model.\n",
        "- **Optimization**: Uses advanced techniques like second-order gradient optimization (Newton boosting) for faster convergence.\n",
        "- **Regularization**: Includes L1 (Lasso) and L2 (Ridge) regularization to prevent overfitting.\n",
        "- **Parallelization**: Supports parallel and distributed computing for faster training.\n",
        "- **Handling Missing Data**: Automatically handles missing values by learning the best imputation strategy.\n",
        "- **Feature Importance**: Provides insights into feature importance for model interpretability.\n",
        "- **Flexibility**: Supports custom loss functions and evaluation metrics."
      ],
      "metadata": {
        "id": "WL2L7JRduO3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How XGBoost Works\n",
        "\n",
        "XGBoost is based on the gradient boosting framework, which iteratively combines weak learners (typically decision trees) to form a strong predictive model. It optimizes a loss function while adding regularization to prevent overfitting. Below is a detailed explanation of how XGBoost works, including the key equations.\n",
        "\n",
        "1.  Objective Function\n",
        "\n",
        "The `objective function` is a mathematical expression that a model aims to minimize during training. It quantifies the model’s performance by combining a loss function, which measures prediction error, with a regularization term, which controls model complexity to prevent overfitting. In XGBoost, the objective function guides the construction of decision trees in the gradient boosting process.\n",
        "\n",
        "The core of XGBoost is to minimize an objective function that combines a loss function and a regularization term. For a dataset with $n$samples and $m$ features, the objective function at iteration $t$ is:\n",
        "\n",
        "$$ \\text{Obj}^{(t)} = \\sum_{i=1}^n l(y_i, \\hat{y}_i^{(t)}) + \\sum_{k=1}^t \\Omega(f_k) $$\n",
        "\n",
        "-   $l(y_i, \\hat{y}_i^{(t)})$: Loss function measuring the difference between the true label $y_i$ and the predicted value $\\hat{y}_i^{(t)}$.\n",
        "\n",
        "Common loss functions include:\n",
        "\n",
        "-   Mean Squared Error (MSE) for regression:\n",
        "\n",
        "$$ $l(y_i, \\hat{y}_i) = (y_i - \\hat{y}_i)^2 $$\n",
        "\n",
        "-   Log-loss for binary classification:\n",
        "\n",
        "$$ l(y_i, \\hat{y}_i) = -[y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)] $$\n",
        "\n",
        "-   $\\Omega(f_k)$: Regularization term for the $k$-th tree to penalize model complexity and prevent overfitting:\n",
        "\n",
        "$$ \\Omega(f_k) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^T w_j^2 $$\n",
        "\n",
        "-   $T$: Number of leaves in the tree.\n",
        "\n",
        "-   $w_j$: Weight (score) of the $j$-th leaf.\n",
        "\n",
        "-   $\\gamma$: Penalty for the number of leaves (controls tree size).\n",
        "\n",
        "-   $\\lambda$: L2 regularization parameter on leaf weights.\n",
        "\n",
        "2.  Additive Training\n",
        "\n",
        "XGBoost builds the model additively. At iteration $t$, the prediction for sample $i$ is:\n",
        "\n",
        "$$ \\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + \\eta f_t(x_i) $$\n",
        "\n",
        "-   $f_t(x_i)$: Output of the $t$-th tree for input $x_i$.\n",
        "\n",
        "-   $\\eta$: Learning rate (shrinkage factor) to scale the contribution of each tree.\n",
        "\n",
        "The goal is to find the tree $f_t$ that minimizes the objective function.\n",
        "\n",
        "3.  Second-Order Approximation\n",
        "\n",
        "To optimize the objective function, XGBoost uses a second-order Taylor approximation of the loss function. For a given sample ( i ), the loss function is approximated as:\n",
        "\n",
        "$$ l(y_i, \\hat{y}_i^{(t)}) \\approx l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \\frac{1}{2} h_i f_t(x_i)^2 $$\n",
        "\n",
        "-   $g_i = \\partial_{\\hat{y}_i^{(t-1)}} l(y_i, \\hat{y}_i^{(t-1)})$: First-order gradient (gradient of the loss w.r.t. the prediction).\n",
        "\n",
        "-   $h_i = \\partial_{\\hat{y}_i^{(t-1)}}^2 l(y_i, \\hat{y}_i^{(t-1)})$: Second-order gradient (Hessian of the loss).\n",
        "\n",
        "The objective function becomes:\n",
        "\n",
        "$$ \\text{Obj}^{(t)} \\approx \\sum_{i=1}^n \\left[ g_i f_t(x_i) + \\frac{1}{2} h_i f_t(x_i)^2 \\right] + \\Omega(f_t) $$\n",
        "\n",
        "4.  Tree Structure Optimization\n",
        "\n",
        "Each tree partitions the data into leaves, and each leaf has a weight $w_j$. For a given tree structure, the optimal leaf weight for leaf $j$ is:\n",
        "\n",
        "$$ w_j^* = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda} $$\n",
        "\n",
        "-   $I_j$: Set of samples in leaf $j$.\n",
        "\n",
        "-   The corresponding objective value for the leaf is:\n",
        "\n",
        "$$ \\text{Obj}_j = -\\frac{1}{2} \\frac{(\\sum_{i \\in I_j} g_i)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma $$\n",
        "\n",
        "The algorithm greedily searches for the best split by maximizing the gain:\n",
        "\n",
        "$$ \\text{Gain} = \\frac{1}{2} \\left[ \\frac{(\\sum_{i \\in I_L} g_i)^2}{\\sum_{i \\in I_L} h_i + \\lambda} + \\frac{(\\sum_{i \\in I_R} g_i)^2}{\\sum_{i \\in I_R} h_i + \\lambda} - \\frac{(\\sum_{i \\in I} g_i)^2}{\\sum_{i \\in I} h_i + \\lambda} \\right] - \\gamma $$\n",
        "\n",
        "-   $I_L$, $I_R$: Left and right child nodes after a split.\n",
        "\n",
        "-   $I$: Parent node before the split."
      ],
      "metadata": {
        "id": "ENBS5aQpTVar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here below is a flowchart illustrating the XGBoost workflow:\n",
        "\n",
        "\n",
        "![alt text](http://drive.google.com/uc?export=view&id=1Dc3bkYebbiQG5D-q-LOG0BbDeATZliXn\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uWCTcHkzvHE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advantages of XGBoost\n",
        "\n",
        "1. `High Performance`: Consistently outperforms other algorithms in machine learning competitions (e.g., Kaggle) due to its accuracy and efficiency.\n",
        "2. `Scalability`: Handles large datasets efficiently through parallel processing and optimized memory usage.\n",
        "3. `Robustness to Overfitting`: Built-in regularization (L1 and L2) and tree pruning reduce overfitting compared to traditional gradient boosting.\n",
        "4. `Handles Diverse Data`: Works well with structured/tabular data, missing values, and categorical features (with preprocessing).\n",
        "5. `Flexibility`: Supports a wide range of tasks (classification, regression, ranking) and custom optimization objectives.\n",
        "6. `Feature Importance`: Provides clear insights into which features drive predictions, aiding interpretability.\n",
        "7. `Community and Ecosystem`: Widely used, with strong community support, extensive documentation, and integration with tools like scikit-learn and Spark."
      ],
      "metadata": {
        "id": "NuhrbXQxGIIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Limitations of XGBoost\n",
        "\n",
        "1. `Complexity`: Requires careful hyperparameter tuning (e.g., learning rate, max depth, number of trees) to achieve optimal performance, which can be time-consuming.\n",
        "2. `Computational Cost`: Despite optimizations, training can be resource-intensive for very large datasets or deep trees compared to simpler models like linear regression.\n",
        "3. `Not Ideal for Unstructured Data`: Less effective for unstructured data (e.g., images, text) compared to deep learning models like neural networks.\n",
        "4. `Interpretability`: While feature importance is provided, the ensemble nature of many trees can make it harder to interpret compared to single decision trees or linear models.\n",
        "5. `Sensitivity to Noisy Data`: Can overfit on noisy datasets if not properly tuned or regularized.\n",
        "6. `Cold Start Problem`: Struggles with new or unseen data patterns unless retrained, unlike online learning algorithms.\n",
        "7. `Over-reliance on Tuning`: Performance heavily depends on hyperparameter optimization, which requires expertise or extensive grid search.\n"
      ],
      "metadata": {
        "id": "70UMhkxtGPsU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Tunable Hyperparameters in XGBoost\n",
        "\n",
        "1. General Parameters\n",
        "\n",
        "- `booste`: Type of learner (`gbtree`, `gblinear`, `dart`). Default: `gbtree`.\n",
        "\n",
        "2. Booster Parameters\n",
        "\n",
        "- `eta (learning_rate)`: Step size shrinkage (0.01–0.3). Default: 0.3.\n",
        "- `max_depth`: Max tree depth (3–10). Default: 6.\n",
        "- `min_child_weight`: Min sum of instance weights in a child (1–10). Default: 1.\n",
        "- `subsample`: Fraction of data sampled per tree (0.5–1). Default: 1.\n",
        "- `colsample_bytree`: Fraction of features per tree (0.5–1). Default: 1.\n",
        "- `colsample_bylevel`: Fraction of features per level (0.5–1). Default: 1.\n",
        "- `colsample_bynode`: Fraction of features per node (0.5–1). Default: 1.\n",
        "- `gamma*` Min loss reduction for a split (0–5). Default: 0.\n",
        "- `lambda (reg_lambda)`: L2 regularization on weights (0–10). Default: 1.\n",
        "- `alpha (reg_alpha)`: L1 regularization on weights (0–1). Default: 0.\n",
        "\n",
        "3. Training Parameters\n",
        "\n",
        "- `n_estimators (num_boost_round)`: Number of trees (100–1000). Default: 100.\n",
        "- `early_stopping_rounds`: Stops if no improvement (10–50). Default: None.\n",
        "\n",
        "4. Task-Specific Parameters\n",
        "\n",
        "- `objective`: Loss function (e.g., `reg:squarederror`, `binary:logistic`). Depends on task.\n",
        "- *`eval_metric`: Evaluation metric (e.g., `rmse`, `logloss`, `auc`). Matches objective.\n",
        "\n",
        "5. Tuning Tips\n",
        "\n",
        "- Start with defaults, adjust `eta`, `max_depth`, `subsample`, `colsample_bytree` first.\n",
        "- Use lower `eta` with more `n_estimators` for robustness.\n",
        "- Increase `gamma`, `lambda`, `alpha` to reduce overfitting.\n",
        "- Apply early stopping with a validation set.\n",
        "- Use grid search, random search, or Bayesian optimization (e.g., Optuna)."
      ],
      "metadata": {
        "id": "EEG0JrW9GQRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Differences Between XGBoost, GBM, LightGBM, and Random Forest\n",
        "\n",
        "XGBoost, GBM (Gradient Boosting Machine), LightGBM, and Random Forest are all tree-based machine learning algorithms, but they differ in their approach, optimization techniques, and performance characteristics. Below is a detailed comparison of these algorithms, followed by a table summarizing their differences.\n",
        "\n",
        "Here’s a table summarizing the key differences across **GBM**, **XGBoost**, **LightGBM**, and **Random Forest**:\n",
        "\n",
        "| **Aspect**                  | **GBM**                                                                 | **XGBoost**                                                             | **LightGBM**                                                            | **Random Forest**                                                       |\n",
        "|-----------------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------|\n",
        "| `Algorithm Type`          | Gradient boosting (sequential tree building)                            | Optimized gradient boosting                                             | Gradient boosting with histogram-based optimization                     | Bagging-based ensemble of independent trees                            |\n",
        "| `Tree Construction`       | Sequential, additive (each tree corrects previous errors)               | Sequential, additive with regularization                                | Sequential, leaf-wise growth (splits best leaf)                        | Parallel, independent trees with bagging                               |\n",
        "| `Optimization`            | Gradient descent (first-order)                                          | Second-order gradient (Newton boosting) for faster convergence          | Gradient-based one-side sampling (GOSS) and histogram binning           | No gradient optimization; uses random feature subsets and bagging       |\n",
        "| `Speed`                   | Slower due to lack of parallelization and optimizations                 | Faster than GBM due to parallelization and caching                      | Faster than XGBoost for large datasets due to histogram-based approach  | Fast due to parallel tree construction, but no sequential optimization  |\n",
        "| `Memory Efficiency`       | Moderate to high memory usage                                           | Improved memory efficiency over GBM                                     | Highly memory-efficient due to histogram binning                        | Moderate to high, depending on number of trees                         |\n",
        "| `Regularization`          | Limited or none (depends on implementation)                            | L1 (Lasso) and L2 (Ridge) regularization to prevent overfitting         | L1 and L2 regularization, similar to XGBoost                            | No explicit regularization; relies on randomness and averaging          |\n",
        "| `Handling Missing Values` | Not natively supported (requires preprocessing)                        | Natively handles missing values by learning optimal split directions    | Natively handles missing values                                         | Not natively supported (requires imputation)                           |\n",
        "| `Scalability`             | Poor for large datasets due to sequential processing                    | Good scalability with parallel and distributed computing                | Excellent for large datasets due to histogram-based approach            | Good scalability due to parallel tree building                         |\n",
        "| `Categorical Features`    | Requires preprocessing (e.g., one-hot encoding)                         | Requires preprocessing, but handles sparse data well                    | Native support for categorical features (no encoding needed)            | Requires preprocessing (e.g., one-hot encoding)                         |\n",
        "| `Feature Importance`      | Supported, but less refined                                            | Detailed feature importance (gain, cover, frequency)                    | Detailed feature importance, similar to XGBoost                         | Feature importance based on Gini impurity or permutation importance     |\n",
        "| `Overfitting Control`     | Prone to overfitting without careful tuning                             | Strong control via regularization and early stopping                    | Strong control via regularization and efficient leaf-wise growth        | Less prone to overfitting due to bagging and random feature selection   |\n",
        "| `Interpretability`        | Moderate (complex due to sequential trees)                              | Moderate, but feature importance aids interpretability                  | Moderate, similar to XGBoost                                            | Relatively interpretable due to independent trees                      |\n",
        "|`Use Case Suitability`    | General-purpose, smaller datasets                                       | High-performance tasks (e.g., Kaggle competitions, tabular data)        | Large-scale datasets, high-dimensional data                            | General-purpose, robust for noisy data, tabular data                   |\n",
        "| `Implementation*`        | Available in scikit-learn, R, etc.                                      | Dedicated library (`xgboost`), integrates with scikit-learn, Spark      | Dedicated library (`lightgbm`), integrates with scikit-learn, Spark     | Available in scikit-learn, R, and other libraries                      |\n",
        "| `GPU Support`            | Limited or none (depends on implementation)                            | Supported for faster training                                           | Strong GPU support for large-scale training                            | Limited or no GPU support in most implementations                      |\n",
        "| `Hyperparameter Tuning`   | Moderate tuning complexity                                             | Extensive tuning (learning rate, max depth, etc.)                      | Moderate tuning, but fewer parameters than XGBoost                      | Simpler tuning (e.g., number of trees, max features)                   |\n",
        "| `Limitations`             | Slow, lacks advanced optimizations, no native missing value handling    | Requires tuning, less effective for unstructured data                   | May overfit with leaf-wise growth if not tuned properly                | Less accurate than boosting for complex patterns, no sequential learning|\n",
        "\n"
      ],
      "metadata": {
        "id": "a6b4S4mWSAyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Distinctions\n",
        "\n",
        "1. Boosting vs. Bagging\n",
        "\n",
        "   - GBM, XGBoost, and LightGBM use **boosting**, where trees are built sequentially to correct errors, making them more powerful for capturing complex patterns but prone to overfitting if not tuned.\n",
        "   - Random Forest uses **bagging**, building independent trees in parallel, which is faster and more robust to noise but may not capture intricate patterns as effectively.\n",
        "\n",
        "2. Optimization and Speed\n",
        "\n",
        "   - GBM is the slowest due to its basic implementation and lack of parallelization.\n",
        "   - XGBoost improves on GBM with parallel processing and second-order optimization.\n",
        "   - LightGBM is optimized for speed and memory efficiency, especially for large datasets, using histogram-based techniques and leaf-wise growth.\n",
        "   - Random Forest is fast due to parallel tree construction but lacks the iterative optimization of boosting methods.\n",
        "\n",
        "3. Scalability and Data Handling\n",
        "\n",
        "   - LightGBM excels with large, high-dimensional datasets due to its histogram-based approach and native categorical feature support.\n",
        "   - XGBoost is scalable but slightly less efficient than LightGBM for very large datasets.\n",
        "   - GBM struggles with large datasets due to its sequential nature.\n",
        "   - Random Forest scales well but requires preprocessing for missing values and categorical features.\n",
        "\n",
        "4. Overfitting and Robustness\n",
        "\n",
        "   - XGBoost and LightGBM include regularization to control overfitting, but their sequential nature makes them sensitive to noise if not tuned.\n",
        "   - Random Forest is inherently robust to overfitting due to bagging and randomness but may underperform on complex tasks.\n",
        "   - GBM is prone to overfitting without careful tuning or regularization.\n",
        "\n",
        "5. Use Cases\n",
        "\n",
        "   - `GBM`: Suitable for smaller datasets or when simplicity is preferred, but less common in modern applications.\n",
        "   - `XGBoost`: Widely used in competitions and tabular data tasks requiring high accuracy (e.g., finance, healthcare).\n",
        "   - `LightGBM`: Preferred for large-scale datasets, such as in big data environments or real-time applications.\n",
        "   - `Random Forest`: Ideal for quick prototyping, noisy data, or when interpretability and robustness are prioritized.\n"
      ],
      "metadata": {
        "id": "LKRKhCnskRNu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup R in Python Runtype"
      ],
      "metadata": {
        "id": "CyOope_pwUVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install {rpy2}\n",
        "\n",
        "{rpy2} is a Python package that provides an interface to the R programming language, allowing Python users to run R code, call R functions, and manipulate R objects directly from Python. It enables seamless integration between Python and R, leveraging R's statistical and graphical capabilities while using Python's flexibility. The package supports passing data between the two languages and is widely used for statistical analysis, data visualization, and machine learning tasks that benefit from R's specialized libraries."
      ],
      "metadata": {
        "id": "SDp3ULld8Gb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall rpy2 -y\n",
        "!pip install rpy2==3.5.1\n",
        "%load_ext rpy2.ipython"
      ],
      "metadata": {
        "id": "CiM6y-Mw8AJp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90147ae4-9b65-43aa-9337-0e9078bded4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: rpy2 3.5.17\n",
            "Uninstalling rpy2-3.5.17:\n",
            "  Successfully uninstalled rpy2-3.5.17\n",
            "Collecting rpy2==3.5.1\n",
            "  Downloading rpy2-3.5.1.tar.gz (201 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.7/201.7 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from rpy2==3.5.1) (1.17.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from rpy2==3.5.1) (3.1.6)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from rpy2==3.5.1) (2025.2)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.11/dist-packages (from rpy2==3.5.1) (5.3.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.10.0->rpy2==3.5.1) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->rpy2==3.5.1) (3.0.2)\n",
            "Building wheels for collected packages: rpy2\n",
            "  Building wheel for rpy2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rpy2: filename=rpy2-3.5.1-cp311-cp311-linux_x86_64.whl size=314976 sha256=2e13201d406d9d6191f3b10476c47d4a7cdb0413940e5e772f2bc7552774a6a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/e9/55/d1/47be85a5f3f1e1f4d1e91cb5e3a4dcb40dd72147f184c5a5ef\n",
            "Successfully built rpy2\n",
            "Installing collected packages: rpy2\n",
            "Successfully installed rpy2-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Google Drive"
      ],
      "metadata": {
        "id": "O1zeuaCowiBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J-4ie4bwiJ1",
        "outputId": "7fbcb3de-1494-450b-9e3b-865a156479a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Extreme Gradient Boosting from Scratch\n",
        "\n",
        "Fitting an `XGBoost` model manually without using R packages like {xgboost} is a complex task because XGBoost relies on optimized implementations for gradient boosting, tree construction, and parallel processing. However, it’s possible to implement a simplified version of the XGBoost algorithm from scratch in R by following its core principles: gradient boosting with decision trees, second-order gradient optimization, and regularization. This requires manually coding the tree construction, gradient and Hessian calculations, and the objective function optimization, which is computationally intensive and less efficient than the optimized {xgboost} package. Below, I’ll explain how to implement a basic version of XGBoost for regression, focusing on the mathematical framework and providing R code without external packages. The explanation will include equations and a step-by-step process to make it clear."
      ],
      "metadata": {
        "id": "ztl1QBDb4MYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Generation\n",
        "\n",
        "- Generates synthetic data: 100 samples with 2 features (X).\n",
        "- Regression target: `y_reg = 3*X1 + 2*X2 + noise`.\n",
        "- Classification target: `y_class = 1 if X1 + X2 > 1, else 0`.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lP2lCzFHBUip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Data Generation\n",
        "set.seed(123)\n",
        "n <- 100\n",
        "X <- matrix(runif(n * 2), ncol = 2)  # 2 features\n",
        "y_reg <- 3 * X[, 1] + 2 * X[, 2] + rnorm(n, 0, 0.1)  # Regression target\n",
        "y_class <- as.integer(X[, 1] + X[, 2] > 1)  # Classification target (0 or 1)"
      ],
      "metadata": {
        "id": "aTjVIV5ZBW96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization and Parameters\n",
        "\n",
        "- Sets hyperparameters: `n_trees=10`, `eta=0.1`, `max_depth=3`, `min_child_weight=1`, `lambda=1`, `gamma=0.`\n",
        "- Initializes predictions to 0 (regression) or 0.5 (classification).\n"
      ],
      "metadata": {
        "id": "fwIhiSZNCO9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Initialization and Parameters\n",
        "n_trees <- 10\n",
        "eta <- 0.1\n",
        "max_depth <- 3\n",
        "min_child_weight <- 1\n",
        "lambda <- 1\n",
        "gamma <- 0\n",
        "\n",
        "# Loss functions\n",
        "mse_loss <- function(y, pred) (y - pred)^2\n",
        "logloss <- function(y, pred) {\n",
        "  pred <- pmin(pmax(pred, 1e-15), 1 - 1e-15)  # Avoid log(0)\n",
        "  -y * log(pred) - (1 - y) * log(1 - pred)\n",
        "}"
      ],
      "metadata": {
        "id": "vYPiX59GCiTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient and Hessian Calculation\n",
        "\n",
        "To compute the `gradients` and `Hessian`s for regression and classification tasks, we define a function that calculates these values based on the predictions and true labels. The gradients represent the first derivative of the loss function, while the Hessians represent the second derivative."
      ],
      "metadata": {
        "id": "8aFvX9PX6t7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Gradient and Hessian Calculation\n",
        "compute_grad_hess <- function(y, pred, task = \"regression\") {\n",
        "  if (task == \"regression\") {\n",
        "    list(grad = 2 * (pred - y), hess = rep(2, length(y)))\n",
        "  } else {\n",
        "    pred <- pmin(pmax(pred, 1e-15), 1 - 1e-15)  # Stabilize for classification\n",
        "    list(grad = (pred - y) / (pred * (1 - pred)), hess = 1 / (pred * (1 - pred)))\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "id": "foG5zY1Kcbe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding the Best Split\n",
        "\n",
        "To find the best split for a given feature, we iterate through unique values of that feature and calculate the gain from splitting the data at each value. The gain is computed using the gradients and Hessians for the left and right splits, following the XGBoost formula. Applies `min_child_weight`, `lambda`, and `gamma` to control splits.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VPJv1wf3xLmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Finding the Best Split\n",
        "find_best_split <- function(X, grad, hess, feature_idx, min_child_weight, lambda, gamma) {\n",
        "  n <- nrow(X)\n",
        "  values <- sort(unique(X[, feature_idx]))\n",
        "  best_gain <- -Inf\n",
        "  best_split <- NULL\n",
        "  total_G <- sum(grad)\n",
        "  total_H <- sum(hess)\n",
        "\n",
        "  for (val in values[-length(values)]) {\n",
        "    left_idx <- which(X[, feature_idx] <= val)\n",
        "    right_idx <- which(X[, feature_idx] > val)\n",
        "\n",
        "    if (length(left_idx) == 0 || length(right_idx) == 0) next\n",
        "\n",
        "    G_left <- sum(grad[left_idx])\n",
        "    H_left <- sum(hess[left_idx])\n",
        "    G_right <- total_G - G_left\n",
        "    H_right <- total_H - H_left\n",
        "\n",
        "    if (H_left < min_child_weight || H_right < min_child_weight) next\n",
        "\n",
        "    gain <- 0.5 * ((G_left^2 / (H_left + lambda)) + (G_right^2 / (H_right + lambda)) -\n",
        "                   (total_G^2 / (total_H + lambda))) - gamma\n",
        "\n",
        "    if (gain > best_gain) {\n",
        "      best_gain <- gain\n",
        "      best_split <- list(feature = feature_idx, value = val, gain = gain)\n",
        "    }\n",
        "  }\n",
        "  best_split\n",
        "}"
      ],
      "metadata": {
        "id": "aSuBBDLKDHML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the Decision Tree\n",
        "\n",
        "To build the decision tree, we recursively split the data based on the best splits found in the previous step. The tree construction continues until reaching the maximum depth or when no further splits improve the gain. Each leaf node is assigned a weight based on the gradients and Hessians of the samples in that leaf. Computes optimal leaf weights: `-sum(grad)/(sum(hess) + lambda)`."
      ],
      "metadata": {
        "id": "tzQKJ-urbCtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Building the Decision Tree\n",
        "build_tree <- function(X, grad, hess, depth, max_depth, min_child_weight, lambda, gamma) {\n",
        "  if (depth >= max_depth || nrow(X) < 2) {\n",
        "    w <- -sum(grad) / (sum(hess) + lambda)\n",
        "    return(list(leaf = TRUE, weight = w))\n",
        "  }\n",
        "\n",
        "  best_split <- NULL\n",
        "  best_gain <- -Inf\n",
        "  for (feature_idx in 1:ncol(X)) {\n",
        "    split <- find_best_split(X, grad, hess, feature_idx, min_child_weight, lambda, gamma)\n",
        "    if (!is.null(split) && split$gain > best_gain) {\n",
        "      best_gain <- split$gain\n",
        "      best_split <- split\n",
        "    }\n",
        "  }\n",
        "\n",
        "  if (is.null(best_split) || best_gain <= 0) {\n",
        "    w <- -sum(grad) / (sum(hess) + lambda)\n",
        "    return(list(leaf = TRUE, weight = w))\n",
        "  }\n",
        "\n",
        "  left_idx <- which(X[, best_split$feature] <= best_split$value)\n",
        "  right_idx <- which(X[, best_split$feature] > best_split$value)\n",
        "\n",
        "  left_subtree <- build_tree(X[left_idx, , drop = FALSE], grad[left_idx], hess[left_idx],\n",
        "                            depth + 1, max_depth, min_child_weight, lambda, gamma)\n",
        "  right_subtree <- build_tree(X[right_idx, , drop = FALSE], grad[right_idx], hess[right_idx],\n",
        "                             depth + 1, max_depth, min_child_weight, lambda, gamma)\n",
        "\n",
        "  list(leaf = FALSE, feature = best_split$feature, value = best_split$value,\n",
        "       left = left_subtree, right = right_subtree)\n",
        "}"
      ],
      "metadata": {
        "id": "Z8mxvvBkbGBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicting with the Tree\n",
        "\n",
        "To make predictions with the decision tree, we traverse the tree based on the feature values of the input data. If we reach a leaf node, we return the weight assigned to that leaf. This function recursively checks the feature value against the split value to navigate through the tree. Applies sigmoid transformation for classification predictions."
      ],
      "metadata": {
        "id": "OFCR0x3ZbLAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Predicting with the Tree\n",
        "predict_tree <- function(tree, x) {\n",
        "  if (tree$leaf) return(tree$weight)\n",
        "  if (x[tree$feature] <= tree$value) {\n",
        "    predict_tree(tree$left, x)\n",
        "  } else {\n",
        "    predict_tree(tree$right, x)\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "id": "5DqGjw3RbLyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Boosting Loop\n",
        "\n",
        "The main boosting loop iterates through the number of trees to be built. In each iteration, it computes the gradients and Hessians for the current predictions, builds a decision tree based on these values, and updates the predictions by adding the tree's output scaled by the learning rate (`eta`). The predictions are updated for both regression (direct addition) and classification (applying a sigmoid function)."
      ],
      "metadata": {
        "id": "okobJFvfbTzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Main Boosting Loop\n",
        "xgboost_train <- function(X, y, n_trees, eta, max_depth, min_child_weight, lambda, gamma, task = \"regression\") {\n",
        "  n <- nrow(X)\n",
        "  pred <- rep(ifelse(task == \"regression\", 0, 0.5), n)\n",
        "  trees <- list()\n",
        "\n",
        "  for (t in 1:n_trees) {\n",
        "    gh <- compute_grad_hess(y, pred, task)\n",
        "    tree <- build_tree(X, gh$grad, gh$hess, 0, max_depth, min_child_weight, lambda, gamma)\n",
        "    trees[[t]] <- tree\n",
        "\n",
        "    for (i in 1:n) {\n",
        "      pred[i] <- pred[i] + eta * predict_tree(tree, X[i, ])\n",
        "      if (task == \"classification\") pred[i] <- 1 / (1 + exp(-pred[i]))  # Sigmoid\n",
        "    }\n",
        "  }\n",
        "\n",
        "  list(trees = trees, pred = pred, task = task)\n",
        "}"
      ],
      "metadata": {
        "id": "eClvfwtHbUoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Prediction and Evaluation\n",
        "\n",
        "- Predicts using the ensemble of trees.\n",
        "- Evaluates regression with `MSE` and classification with `log-loss` and `accuracy`.\n"
      ],
      "metadata": {
        "id": "xJhzNiflbZ7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Final Prediction and Evaluation\n",
        "predict_xgboost <- function(model, X) {\n",
        "  n <- nrow(X)\n",
        "  pred <- rep(ifelse(model$task == \"regression\", 0, 0.5), n)\n",
        "\n",
        "  for (tree in model$trees) {\n",
        "    for (i in 1:n) {\n",
        "      pred[i] <- pred[i] + eta * predict_tree(tree, X[i, ])\n",
        "      if (model$task == \"classification\") pred[i] <- 1 / (1 + exp(-pred[i]))\n",
        "    }\n",
        "  }\n",
        "  pred\n",
        "}\n",
        "\n",
        "# Evaluation Metrics\n",
        "mse <- function(y_true, y_pred) mean((y_true - y_pred)^2)\n",
        "logloss_eval <- function(y_true, y_pred) {\n",
        "  y_pred <- pmin(pmax(y_pred, 1e-15), 1 - 1e-15)\n",
        "  mean(-y_true * log(y_pred) - (1 - y_true) * log(1 - y_pred))\n",
        "}\n",
        "accuracy <- function(y_true, y_pred) mean(y_true == (y_pred > 0.5))"
      ],
      "metadata": {
        "id": "UPgnmNq7bgpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Evaluate Regression Model\n"
      ],
      "metadata": {
        "id": "zd78Wrt6bk6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Train and evaluate regression model\n",
        "model_reg <- xgboost_train(X, y_reg, n_trees, eta, max_depth, min_child_weight, lambda, gamma, task = \"regression\")\n",
        "pred_reg <- predict_xgboost(model_reg, X)\n",
        "mse_reg <- mse(y_reg, pred_reg)\n",
        "cat(\"Regression MSE:\", mse_reg, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MCBol9qbloE",
        "outputId": "729a1148-a1c7-4ae3-ec29-4e4d3bdc48a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regression MSE: 1.003863 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Evaluate Classification Model"
      ],
      "metadata": {
        "id": "CtuWG-OJbukf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Train and evaluate classification model\n",
        "model_class <- xgboost_train(X, y_class, n_trees, eta, max_depth, min_child_weight, lambda, gamma, task = \"classification\")\n",
        "pred_class <- predict_xgboost(model_class, X)\n",
        "logloss_class <- logloss_eval(y_class, pred_class)\n",
        "acc_class <- accuracy(y_class, pred_class)\n",
        "cat(\"Classification Log-loss:\", logloss_class, \"\\n\")\n",
        "cat(\"Classification Accuracy:\", acc_class, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0r8G-cJbux5",
        "outputId": "6bcd1a05-06a5-46c5-aee0-34e28a4466a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Log-loss: 0.7281187 \n",
            "Classification Accuracy: 0.48 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extreme Gradient Boosting with R\n",
        "\n",
        "The `{xgboost}` package in R is an optimized implementation of the XGBoost (eXtreme Gradient Boosting) algorithm, a powerful and efficient machine learning method for supervised learning tasks like regression, classification, and ranking. It’s widely used for its high performance, scalability, and ability to handle complex datasets, making it a go-to tool for predicting outcomes. Below is a concise overview of the `{xgboost}` package, covering its purpose, key features, functionality, and how it can be applied to classification and regression problems.\n",
        "\n",
        "Key Features of `{xgboost}`:\n",
        "\n",
        "1.  `Gradient Boosting`:\n",
        "    -   Builds sequential decision trees, each correcting errors of the previous ones, using gradient descent to minimize a loss function.\n",
        "    -   Supports second-order gradient optimization (Hessian) for faster convergence.\n",
        "2.  `Objective Functions`:\n",
        "    -   Supports various tasks:\n",
        "        -   Regression: `reg:squarederror` (MSE), `reg:linear` (deprecated).\n",
        "        -   Classification: `binary:logistic`, `multi:softmax`, `multi:softprob`.\n",
        "        -   Ranking: `rank:pairwise`.\n",
        "    -   Custom objectives can be defined.\n",
        "3.  `Regularization`:\n",
        "    -   L1 (`alpha`) and L2 (`lambda`) penalties on leaf weights.\n",
        "    -   Minimum child weight and maximum depth to control tree complexity.\n",
        "    -   Gamma penalty for adding leaves.\n",
        "4.  `Performance Optimization`:\n",
        "    -   Parallel tree construction using multiple CPU cores.\n",
        "    -   Approximate split finding for large datasets.\n",
        "    -   Handles missing values automatically.\n",
        "5.  `Flexibility`:\n",
        "    -   Supports sparse data, categorical variables (via encoding), and custom evaluation metrics.\n",
        "    -   Cross-validation and early stopping for optimal iterations.\n",
        "\n",
        "Key Functions in `{xgboost}` are:\n",
        "\n",
        "-   `xgb.DMatrix()`: Creates a data matrix optimized for XGBoost, handling numeric and categorical data efficiently.\n",
        "-   `xgb.train()`: Trains the model with customizable parameters (e.g., `max.depth`, `eta`, `nrounds`).\n",
        "-   `xgb.cv()`: Performs k-fold cross-validation to tune hyperparameters.\n",
        "-   `xgb.importance()`: Computes feature importance based on gain, frequency, or cover.\n",
        "-   `predict()`: Generates predictions for new data.\n",
        "-   `xgb.save/load{}`: Saves and loads models for reuse.\n"
      ],
      "metadata": {
        "id": "n5Z1SpIUbgLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check amd Install Required R Packages\n",
        "\n",
        "Following R packages are required to run this notebook. If any of these packages are not installed, you can install them using the code below:"
      ],
      "metadata": {
        "id": "yXu-XY0mw1A8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "packages <- c('tidyverse',\n",
        "              'plyr',\n",
        "              'xgboost',\n",
        "              'Metrics',\n",
        "              'fastDummies',\n",
        "              'ggpmisc'\n",
        "\n",
        "         )"
      ],
      "metadata": {
        "id": "TeYB57l0wz5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Missing Packages"
      ],
      "metadata": {
        "id": "q7NH-51RYeIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Install missing packages\n",
        "new.packages <- packages[!(packages %in% installed.packages(lib='drive/My Drive/R/')[,\"Package\"])]\n",
        "if(length(new.packages)) install.packages(new.packages, lib='drive/My Drive/R/')"
      ],
      "metadata": {
        "id": "a0aTMYTHAraZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verify Installation"
      ],
      "metadata": {
        "id": "Ah43eXWcYfpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# set library path\n",
        ".libPaths('drive/My Drive/R')\n",
        "# Verify installation\n",
        "cat(\"Installed packages:\\n\")\n",
        "print(sapply(packages, requireNamespace, quietly = TRUE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9hpg7qnArfh",
        "outputId": "9cd856c3-9e7d-4e49-b68e-de988ef0af75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installed packages:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: Registered S3 methods overwritten by 'ggpp':\n",
            "  method                  from   \n",
            "  heightDetails.titleGrob ggplot2\n",
            "  widthDetails.titleGrob  ggplot2\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tidyverse        plyr     xgboost     Metrics fastDummies     ggpmisc \n",
            "       TRUE        TRUE        TRUE        TRUE        TRUE        TRUE \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load R Packages"
      ],
      "metadata": {
        "id": "MV7R29xfyWQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# set library path\n",
        ".libPaths('drive/My Drive/R')\n",
        "# Load packages with suppressed messages\n",
        "invisible(lapply(packages, function(pkg) {\n",
        "  suppressPackageStartupMessages(library(pkg, character.only = TRUE))\n",
        "}))"
      ],
      "metadata": {
        "id": "232jNAHBykUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Loaded Packages"
      ],
      "metadata": {
        "id": "ULxgK067YntF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Check loaded packages\n",
        "cat(\"Successfully loaded packages:\\n\")\n",
        "print(search()[grepl(\"package:\", search())])# Check loaded packageswer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcAOZzmSYk4n",
        "outputId": "bc6a5fbb-d429-42f7-8902-621e61b1d0e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded packages:\n",
            " [1] \"package:ggpmisc\"     \"package:ggpp\"        \"package:fastDummies\"\n",
            " [4] \"package:Metrics\"     \"package:xgboost\"     \"package:plyr\"       \n",
            " [7] \"package:lubridate\"   \"package:forcats\"     \"package:stringr\"    \n",
            "[10] \"package:dplyr\"       \"package:purrr\"       \"package:readr\"      \n",
            "[13] \"package:tidyr\"       \"package:tibble\"      \"package:ggplot2\"    \n",
            "[16] \"package:tidyverse\"   \"package:tools\"       \"package:stats\"      \n",
            "[19] \"package:graphics\"    \"package:grDevices\"   \"package:utils\"      \n",
            "[22] \"package:datasets\"    \"package:methods\"     \"package:base\"       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regression with XGBoost\n",
        "\n",
        "This section of tutorial will focus on using the XGBoost package to perform regression analysis. We will load the dataset, preprocess it, and then fit an XGBoost model for regression. The dataset will be split into training and testing sets, and we will evaluate the model's performance using RMSE (Root Mean Squared Error)."
      ],
      "metadata": {
        "id": "QabVFLr5jgrD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data and Data Preparation\n",
        "\n"
      ],
      "metadata": {
        "id": "rSJqB05rO5ES"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFpt_H01OPsa"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "# Load Boston Housing dataset\n",
        "data(\"Boston\", package = \"MASS\")\n",
        "\n",
        "# Create a data frame with selected variables\n",
        "df <- Boston %>%\n",
        "  dplyr::select(medv, crim, zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black, lstat)\n",
        "\n",
        "# Convert to matrix (assuming 'me' was a typo; no mean calculation needed here)\n",
        "m <- as.matrix(df)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "set.seed(123)  # For reproducibility\n",
        "indices <- sample(1:nrow(df), size = 0.75 * nrow(df))\n",
        "train <- m[indices, ]\n",
        "test <- m[-indices, ]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load the train and test data into the xgboost dataset object\n",
        "\n",
        "The train and test datasets are converted into `xgb.DMatrix` objects, which are optimized data structures for XGBoost. This step is crucial for efficient training and prediction."
      ],
      "metadata": {
        "id": "UPiv_OTsmKB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Define predictor and response variables in training set\n",
        "train_x <- data.matrix(train[, -1])  # All columns except medv\n",
        "train_y <- train[, 1]               # medv column\n",
        "\n",
        "# Define predictor and response variables in testing set\n",
        "test_x <- data.matrix(test[, -1])\n",
        "test_y <- test[, 1]\n",
        "\n",
        "# Define final training and testing sets\n",
        "dtrain <- xgb.DMatrix(data = train_x, label = train_y)\n",
        "dtest <- xgb.DMatrix(data = test_x, label = test_y)"
      ],
      "metadata": {
        "id": "nmHkqU4kmNWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit Initial Model with Fixed Parameters\n",
        "\n",
        "Next, we'll fit the XGBoost model by using the `xgb.train()` function, which displays the training and testing RMSE (root mean squared error) for each round of boosting. We use a few tunning prameters with default values."
      ],
      "metadata": {
        "id": "k6EFndFdP5qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "#  Parameters for XGBoost model\n",
        "params <- list(\n",
        "  objective = \"reg:squarederror\",\n",
        "  eval_metric = \"rmse\",\n",
        "  max_depth = 6,\n",
        "  eta = 0.3,\n",
        "  gamma = 0,\n",
        "  colsample_bytree = 0.8,\n",
        "  min_child_weight = 1,\n",
        "  subsample = 0.8\n",
        ")\n",
        "\n",
        "#fit XGBoost model and display training and testing data at each round\n",
        "initial_model <- xgb.train(\n",
        "  params = params,\n",
        "  data = dtrain,\n",
        "  nrounds = 100,\n",
        "  watchlist = list(train = dtrain, test = dtest),\n",
        "  early_stopping_rounds = 10,\n",
        "  verbose = 0\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "p7fPSLSxsZie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "summary(initial_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyjnaLHUP946",
        "outputId": "e3f33286-26b1-47db-8f18-7313074e222e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                Length Class              Mode       \n",
            "handle              1  xgb.Booster.handle externalptr\n",
            "raw             43986  -none-             raw        \n",
            "best_iteration      1  -none-             numeric    \n",
            "best_ntreelimit     1  -none-             numeric    \n",
            "best_score          1  -none-             numeric    \n",
            "best_msg            1  -none-             character  \n",
            "niter               1  -none-             numeric    \n",
            "evaluation_log      3  data.table         list       \n",
            "call                7  -none-             call       \n",
            "params              9  -none-             list       \n",
            "callbacks           2  -none-             list       \n",
            "feature_names      19  -none-             character  \n",
            "nfeatures           1  -none-             numeric    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predict and Evaluate Initial Model"
      ],
      "metadata": {
        "id": "ioqlVpARP_pN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Prediction and Evaluation\n",
        "yhat_train_initial <- predict(initial_model, train_x)  # Use train_x (columns 2-14)\n",
        "yhat_test_initial <- predict(initial_model, test_x)    # Use test_x (columns 2-14)\n",
        "\n",
        "# Define RMSE function\n",
        "RMSE <- function(actual, predicted) {\n",
        "  sqrt(mean((actual - predicted)^2))\n",
        "}\n",
        "\n",
        "rmse_train_initial <- RMSE(train_y, yhat_train_initial)\n",
        "rmse_test_initial <- RMSE(test_y, yhat_test_initial)\n",
        "cat(\"Initial Model Training RMSE:\", rmse_train_initial, \"\\n\")\n",
        "cat(\"Initial Model Testing RMSE:\", rmse_test_initial, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwTfSah-QYwz",
        "outputId": "b210fae0-2db1-45da-82aa-813fdd419aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Model Training RMSE: 0.6862006 \n",
            "Initial Model Testing RMSE: 3.863504 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyperparameter Tuning for the Best Parameters\n",
        "\n",
        "To optimize the XGBoost model, we can perform cross-validation to find the best hyperparameters. This involves testing different combinations of parameters like `max_depth`, `eta`, and `nrounds` to minimize the RMSE on the validation set.\n"
      ],
      "metadata": {
        "id": "GqOYNF6YnQMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Define Parameter Grid and Initialize Variables"
      ],
      "metadata": {
        "id": "PpIEnQNDoWLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Hyperparameter Tuning for the Best Parameters\n",
        "# Define Parameter Grid and Initialize Variables\n",
        "\n",
        "params <- list(\n",
        "  objective = \"reg:squarederror\",\n",
        "  eval_metric = \"rmse\",\n",
        "  max_depth = 6,\n",
        "  eta = 0.3,\n",
        "  gamma = 0,\n",
        "  colsample_bytree = 0.8,\n",
        "  min_child_weight = 1,\n",
        "  subsample = 0.8\n",
        ")\n",
        "\n",
        "# Cross-validation for hyperparameter tuning\n",
        "param_grid <- expand.grid(\n",
        "  max_depth = c(3, 6, 9),\n",
        "  eta = c(0.1, 0.3, 0.5),\n",
        "  min_child_weight = c(1, 3, 5)\n",
        ")\n",
        "\n",
        "best_params <- NULL\n",
        "best_error <- Inf\n",
        "cv_results <- data.frame()\n",
        "\n",
        "for (i in 1:nrow(param_grid)) {\n",
        "  params_cv <- list(\n",
        "    objective = \"reg:squarederror\",\n",
        "    eval_metric = \"rmse\",\n",
        "    max_depth = param_grid$max_depth[i],\n",
        "    eta = param_grid$eta[i],\n",
        "    gamma = 0,\n",
        "    colsample_bytree = 0.8,\n",
        "    min_child_weight = param_grid$min_child_weight[i],\n",
        "    subsample = 0.8\n",
        "  )\n",
        "\n",
        "  cv_model <- xgb.cv(\n",
        "    params = params_cv,\n",
        "    data = dtrain,\n",
        "    nrounds = 200,\n",
        "    nfold = 5,\n",
        "    early_stopping_rounds = 10,\n",
        "    verbose = 0\n",
        "  )\n",
        "\n",
        "  # Use test_rmse_mean for regression\n",
        "  min_error <- min(cv_model$evaluation_log$test_rmse_mean)\n",
        "  best_nrounds <- which.min(cv_model$evaluation_log$test_rmse_mean)\n",
        "\n",
        "  # Populate cv_results\n",
        "  cv_results <- rbind(cv_results, data.frame(\n",
        "    max_depth = param_grid$max_depth[i],\n",
        "    eta = param_grid$eta[i],\n",
        "    min_child_weight = param_grid$min_child_weight[i],\n",
        "    test_rmse = min_error,\n",
        "    nrounds = best_nrounds\n",
        "  ))\n",
        "\n",
        "  if (min_error < best_error) {\n",
        "    best_error <- min_error\n",
        "    best_params <- params_cv\n",
        "    best_params$nrounds <- best_nrounds\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "id": "RjnlDmSzoWzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### The best parameters"
      ],
      "metadata": {
        "id": "EuPB-9Tnot5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Output results\n",
        "cat(\"\\nBest Parameters:\\n\")\n",
        "print(best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYbWhgako6J4",
        "outputId": "eb8cbd34-41a0-4f30-d346-a2149e6d198e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best Parameters:\n",
            "$objective\n",
            "[1] \"reg:squarederror\"\n",
            "\n",
            "$eval_metric\n",
            "[1] \"rmse\"\n",
            "\n",
            "$max_depth\n",
            "[1] 3\n",
            "\n",
            "$eta\n",
            "[1] 0.3\n",
            "\n",
            "$gamma\n",
            "[1] 0\n",
            "\n",
            "$colsample_bytree\n",
            "[1] 0.8\n",
            "\n",
            "$min_child_weight\n",
            "[1] 1\n",
            "\n",
            "$subsample\n",
            "[1] 0.8\n",
            "\n",
            "$nrounds\n",
            "[1] 72\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Train and Validate Model with Best Parameters"
      ],
      "metadata": {
        "id": "qlo7UGqjpMJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Train final model with best parameters\n",
        "final_model <- xgb.train(\n",
        "  params = best_params,\n",
        "  data = dtrain,\n",
        "  nrounds = best_params$nrounds,\n",
        "  watchlist = list(train = dtrain, test = dtest),\n",
        "  verbose = 0\n",
        ")"
      ],
      "metadata": {
        "id": "MWa_RuR3pMhJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d472b93d-1fbf-481d-9526-e99682f1b53a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[02:01:14] WARNING: src/learner.cc:767: \n",
            "Parameters: { \"nrounds\" } are not used.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Predictions and Evaluation"
      ],
      "metadata": {
        "id": "Q99NFqvipTF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Predictions and Evaluation\n",
        "yhat_train_final <- predict(final_model, train_x)\n",
        "yhat_test_final <- predict(final_model, test_x)\n",
        "\n",
        "# Train and test RMSE\n",
        "rmse_train_final <- RMSE(train_y, yhat_train_final)\n",
        "rmse_test_final <- RMSE(test_y, yhat_test_final)\n",
        "cat(\"Final Model Training RMSE:\", rmse_train_final, \"\\n\")\n",
        "cat(\"Final Model Testing RMSE:\", rmse_test_final, \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDzGkXFPpaFR",
        "outputId": "90aaac77-c898-4141-d60b-68ac4e41639d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Model Training RMSE: 0.04151871 \n",
            "Final Model Testing RMSE: 3.436067 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "\n",
        "# 1:1 Plot of Predicted vs Observed values\n",
        "test_df <- data.frame(medv = test_y, medv_pred = yhat_test_final)\n",
        "\n",
        "formula <- y ~ x\n",
        "\n",
        "ggplot(test_df, aes(medv, medv_pred)) +\n",
        "  geom_point() +\n",
        "  geom_smooth(method = \"lm\") +\n",
        "  stat_poly_eq(use_label(c(\"eq\", \"adj.R2\")), formula = formula) +\n",
        "  ggtitle(\"XGBoost: Predicted vs Observed House Prices\") +\n",
        "  xlab(\"Observed medv ($1000s)\") + ylab(\"Predicted medv ($1000s)\") +\n",
        "  scale_x_continuous(limits = c(0, 60), breaks = seq(0, 60, 10)) +\n",
        "  scale_y_continuous(limits = c(0, 60), breaks = seq(0, 60, 10)) +\n",
        "  theme(\n",
        "    panel.background = element_rect(fill = \"grey95\", colour = \"gray75\", size = 0.5, linetype = \"solid\"),\n",
        "    axis.line = element_line(colour = \"grey\"),\n",
        "    plot.title = element_text(size = 14, hjust = 0.5),\n",
        "    axis.title.x = element_text(size = 14),\n",
        "    axis.title.y = element_text(size = 14),\n",
        "    axis.text.x = element_text(size = 13, colour = \"black\"),\n",
        "    axis.text.y = element_text(size = 13, angle = 90, vjust = 0.5, hjust = 0.5, colour = \"black\")\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "Ic0gVXQEzDOd",
        "outputId": "1d4390b0-161e-4399-a9c6-852aaad0fd94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`geom_smooth()` using formula = 'y ~ x'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Variable Importance\n",
        "\n",
        "Variable importance in XGBoost indicates how much each feature contributes to the model's predictions. It helps identify which predictors are most influential in determining the target variable. The importance can be measured using metrics like gain, cover, and frequency. Gain measures the contribution of a feature to the model's accuracy, cover indicates the relative quantity of observations concerned by a feature, and frequency counts how often a feature is used in trees."
      ],
      "metadata": {
        "id": "5AKztLwDzNj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R -w 700 -h 600 -u px\n",
        "# Calculate variable importance\n",
        "importance <- xgb.importance(model = final_model)\n",
        "xgb.plot.importance(importance, rel_to_first = TRUE, xlab = \"Relative importance\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "IOT-EVsOzOZP",
        "outputId": "8f63e2f6-280a-4ef6-f7a2-65ae16877f53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Early Stopping\n",
        "\n",
        "Early stopping in XGBoost is a technique used to prevent overfitting by monitoring the model's performance on a validation set during training. If the model's performance does not improve for a specified number of rounds (defined by `early_stopping_rounds`), training is halted. This helps to find the optimal number of boosting rounds without excessive computation and ensures that the model generalizes well to unseen data.\n",
        "\n",
        "In XGBoost, this is implemented by monitoring a performance metric (e.g., RMSE for regression) on a validation dataset during training. If the metric does not improve for a defined number of consecutive rounds (early_stopping_rounds), training stops, and the model returns the iteration with the best performance. This helps select an optimal number of boosting rounds (nrounds) without overfitting to the training data."
      ],
      "metadata": {
        "id": "USrLldhbrbGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Train final model with best parameters\n",
        "final_model_early <- xgb.train(\n",
        "  params = best_params,\n",
        "  data = dtrain,\n",
        "  nrounds = best_params$nrounds,\n",
        "  watchlist = list(train = dtrain, test = dtest),\n",
        "  early_stopping_rounds = 10,\n",
        "  verbose = 1  # Show training progress\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MP5768CXrmcS",
        "outputId": "8ff09e3b-8b84-4ec4-9991-448247689bc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[02:03:29] WARNING: src/learner.cc:767: \n",
            "Parameters: { \"nrounds\" } are not used.\n",
            "\n",
            "[1]\ttrain-rmse:21.638485\ttest-rmse:21.775883 \n",
            "Multiple eval metrics are present. Will use test_rmse for early stopping.\n",
            "Will train until test_rmse hasn't improved in 10 rounds.\n",
            "\n",
            "[2]\ttrain-rmse:19.595134\ttest-rmse:19.682217 \n",
            "[3]\ttrain-rmse:17.738096\ttest-rmse:17.877446 \n",
            "[4]\ttrain-rmse:16.116296\ttest-rmse:16.332263 \n",
            "[5]\ttrain-rmse:14.662949\ttest-rmse:14.852240 \n",
            "[6]\ttrain-rmse:13.368751\ttest-rmse:13.665619 \n",
            "[7]\ttrain-rmse:12.131636\ttest-rmse:12.454425 \n",
            "[8]\ttrain-rmse:11.025802\ttest-rmse:11.364955 \n",
            "[9]\ttrain-rmse:10.028260\ttest-rmse:10.400720 \n",
            "[10]\ttrain-rmse:9.162763\ttest-rmse:9.565926 \n",
            "[11]\ttrain-rmse:8.373756\ttest-rmse:8.835636 \n",
            "[12]\ttrain-rmse:7.655286\ttest-rmse:8.154447 \n",
            "[13]\ttrain-rmse:6.977153\ttest-rmse:7.562153 \n",
            "[14]\ttrain-rmse:6.390442\ttest-rmse:7.103871 \n",
            "[15]\ttrain-rmse:5.848049\ttest-rmse:6.618085 \n",
            "[16]\ttrain-rmse:5.363889\ttest-rmse:6.197285 \n",
            "[17]\ttrain-rmse:4.943103\ttest-rmse:5.873801 \n",
            "[18]\ttrain-rmse:4.546261\ttest-rmse:5.568162 \n",
            "[19]\ttrain-rmse:4.193099\ttest-rmse:5.314607 \n",
            "[20]\ttrain-rmse:3.859973\ttest-rmse:5.074572 \n",
            "[21]\ttrain-rmse:3.556746\ttest-rmse:4.850377 \n",
            "[22]\ttrain-rmse:3.293285\ttest-rmse:4.674483 \n",
            "[23]\ttrain-rmse:3.052906\ttest-rmse:4.519206 \n",
            "[24]\ttrain-rmse:2.815836\ttest-rmse:4.376563 \n",
            "[25]\ttrain-rmse:2.611141\ttest-rmse:4.263266 \n",
            "[26]\ttrain-rmse:2.420886\ttest-rmse:4.175961 \n",
            "[27]\ttrain-rmse:2.260560\ttest-rmse:4.080775 \n",
            "[28]\ttrain-rmse:2.117684\ttest-rmse:4.009830 \n",
            "[29]\ttrain-rmse:1.969709\ttest-rmse:3.934436 \n",
            "[30]\ttrain-rmse:1.839061\ttest-rmse:3.875676 \n",
            "[31]\ttrain-rmse:1.732419\ttest-rmse:3.834315 \n",
            "[32]\ttrain-rmse:1.614902\ttest-rmse:3.817084 \n",
            "[33]\ttrain-rmse:1.514337\ttest-rmse:3.785500 \n",
            "[34]\ttrain-rmse:1.420402\ttest-rmse:3.761572 \n",
            "[35]\ttrain-rmse:1.331589\ttest-rmse:3.738015 \n",
            "[36]\ttrain-rmse:1.260698\ttest-rmse:3.711442 \n",
            "[37]\ttrain-rmse:1.188413\ttest-rmse:3.700058 \n",
            "[38]\ttrain-rmse:1.124529\ttest-rmse:3.686268 \n",
            "[39]\ttrain-rmse:1.062923\ttest-rmse:3.679293 \n",
            "[40]\ttrain-rmse:1.004403\ttest-rmse:3.670549 \n",
            "[41]\ttrain-rmse:0.955388\ttest-rmse:3.653410 \n",
            "[42]\ttrain-rmse:0.908519\ttest-rmse:3.643698 \n",
            "[43]\ttrain-rmse:0.865946\ttest-rmse:3.632176 \n",
            "[44]\ttrain-rmse:0.821439\ttest-rmse:3.625679 \n",
            "[45]\ttrain-rmse:0.789145\ttest-rmse:3.622838 \n",
            "[46]\ttrain-rmse:0.753807\ttest-rmse:3.612988 \n",
            "[47]\ttrain-rmse:0.714555\ttest-rmse:3.604888 \n",
            "[48]\ttrain-rmse:0.680584\ttest-rmse:3.594142 \n",
            "[49]\ttrain-rmse:0.655525\ttest-rmse:3.585686 \n",
            "[50]\ttrain-rmse:0.629731\ttest-rmse:3.580703 \n",
            "[51]\ttrain-rmse:0.602934\ttest-rmse:3.581260 \n",
            "[52]\ttrain-rmse:0.579810\ttest-rmse:3.579878 \n",
            "[53]\ttrain-rmse:0.557645\ttest-rmse:3.578457 \n",
            "[54]\ttrain-rmse:0.535801\ttest-rmse:3.575061 \n",
            "[55]\ttrain-rmse:0.511888\ttest-rmse:3.569904 \n",
            "[56]\ttrain-rmse:0.491999\ttest-rmse:3.572140 \n",
            "[57]\ttrain-rmse:0.479181\ttest-rmse:3.570457 \n",
            "[58]\ttrain-rmse:0.465371\ttest-rmse:3.567567 \n",
            "[59]\ttrain-rmse:0.449432\ttest-rmse:3.560742 \n",
            "[60]\ttrain-rmse:0.435711\ttest-rmse:3.561111 \n",
            "[61]\ttrain-rmse:0.418371\ttest-rmse:3.553561 \n",
            "[62]\ttrain-rmse:0.405412\ttest-rmse:3.556420 \n",
            "[63]\ttrain-rmse:0.397263\ttest-rmse:3.555778 \n",
            "[64]\ttrain-rmse:0.379173\ttest-rmse:3.555009 \n",
            "[65]\ttrain-rmse:0.365966\ttest-rmse:3.555643 \n",
            "[66]\ttrain-rmse:0.359351\ttest-rmse:3.556891 \n",
            "[67]\ttrain-rmse:0.346995\ttest-rmse:3.559149 \n",
            "[68]\ttrain-rmse:0.340620\ttest-rmse:3.558827 \n",
            "[69]\ttrain-rmse:0.328334\ttest-rmse:3.556874 \n",
            "[70]\ttrain-rmse:0.316994\ttest-rmse:3.556469 \n",
            "[71]\ttrain-rmse:0.311002\ttest-rmse:3.556288 \n",
            "Stopping. Best iteration:\n",
            "[61]\ttrain-rmse:0.418371\ttest-rmse:3.553561\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Evaluate the model with early stopping\n",
        "yhat_train_early <- predict(final_model_early, train_x)\n",
        "yhat_test_early <- predict(final_model_early, test_x)\n",
        "\n",
        "# Train and test RMSE\n",
        "rmse_train_early <- RMSE(train_y, yhat_train_early)\n",
        "rmse_test_early <- RMSE(test_y, yhat_test_early)\n",
        "\n",
        "# Print results\n",
        "cat(\"Early Stopping Training RMSE:\", rmse_train_early, \"\\n\")\n",
        "cat(\"Early Stopping Testing RMSE:\", rmse_test_early, \"\\n\")\n",
        "cat(\"Best Iteration:\", final_model_early$best_iteration, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_mE8_Lsrjc8",
        "outputId": "4b3121c8-f4c7-47b6-d880-708a516a5eab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early Stopping Training RMSE: 0.4183715 \n",
            "Early Stopping Testing RMSE: 3.553561 \n",
            "Best Iteration: 61 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification with XGBoost\n",
        "\n",
        "In this section, we will use the XGBoost package to perform classification on a dataset. We will load the dataset, preprocess it, and then fit an XGBoost model for classification. The dataset will be split into training and testing sets, and we will evaluate the model's performance using accuracy and confusion matrix."
      ],
      "metadata": {
        "id": "Irqlj8sYYKas"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data and Data Preparation\n",
        "\n",
        "We will use`health insurance` dataset to predict the product type (A, B, or C) based on various features such as age, household size, position level, and absence records."
      ],
      "metadata": {
        "id": "NRpPJB6-YUUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "data <-readr::read_csv(\"https://github.com/zia207/r-colab/raw/main/Data/Machine_Learning/health_insurance.csv\")\n",
        "# Convert gender to numeric (Male: 0, Female: 1)\n",
        "data$gender <- as.numeric(factor(data$gender, levels = c(\"Male\", \"Female\"))) - 1\n",
        "\n",
        "# Convert product to numeric labels (A: 0, B: 1, C: 2)\n",
        "data$product <- as.numeric(factor(data$product, levels = c(\"A\", \"B\", \"C\"))) - 1\n",
        "\n",
        "# Select features and target\n",
        "features <- c(\"age\", \"household\", \"position_level\", \"absent\", \"gender\")\n",
        "X <- data[, features]\n",
        "y <- data$product\n",
        "# Split data into training (80%) and test (20%) sets using base R\n",
        "set.seed(123)\n",
        "n <- nrow(data)\n",
        "train_indices <- sample(1:n, size = 0.8 * n, replace = FALSE)\n",
        "X_train <- X[train_indices, ]\n",
        "y_train <- y[train_indices]\n",
        "X_test <- X[-train_indices, ]\n",
        "y_test <- y[-train_indices]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfumDV8yYaI2",
        "outputId": "da2cb713-d822-48eb-f26d-2c45776defe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows: 1448 Columns: 6\n",
            "── Column specification ────────────────────────────────────────────────────────\n",
            "Delimiter: \",\"\n",
            "chr (2): product, gender\n",
            "dbl (4): age, household, position_level, absent\n",
            "\n",
            "ℹ Use `spec()` to retrieve the full column specification for this data.\n",
            "ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convert data to DMatrix format for XGBoost"
      ],
      "metadata": {
        "id": "_rQ4MN1FkZ0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Convert data to DMatrix format for XGBoost\n",
        "dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)\n",
        "dtest <- xgb.DMatrix(data = as.matrix(X_test), label = y_test)"
      ],
      "metadata": {
        "id": "4Grpxt1KkagW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit lightGBM Classification Model"
      ],
      "metadata": {
        "id": "5yliW4QeZMpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Initial model training with default parameters\n",
        "params <- list(\n",
        "  objective = \"multi:softmax\",\n",
        "  num_class = 3,\n",
        "  eval_metric = \"merror\",\n",
        "  max_depth = 6,\n",
        "  eta = 0.3,\n",
        "  gamma = 0,\n",
        "  colsample_bytree = 0.8,\n",
        "  min_child_weight = 1,\n",
        "  subsample = 0.8\n",
        ")\n",
        "\n",
        "initial_model <- xgb.train(\n",
        "  params = params,\n",
        "  data = dtrain,\n",
        "  nrounds = 100,\n",
        "  watchlist = list(train = dtrain, test = dtest),\n",
        "  early_stopping_rounds = 10,\n",
        "  verbose = 0\n",
        ")"
      ],
      "metadata": {
        "id": "0H3OVcIrZNK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predictions and Evaluation\n",
        "\n"
      ],
      "metadata": {
        "id": "Gp7pWvYaZS7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Predict on test set\n",
        "y_pred_initial <- predict(initial_model, dtest)\n",
        "\n",
        "# Evaluate performance\n",
        "confusion_matrix_initial <- table(Predicted = y_pred_initial, Actual = y_test)\n",
        "accuracy_initial <- sum(diag(confusion_matrix_initial)) / sum(confusion_matrix_initial)\n",
        "\n",
        "# Print results\n",
        "cat(\"\\nTest Set Accuracy:\", round(accuracy_initial, 4), \"\\n\")\n",
        "cat(\"\\nConfusion Matrix:\\n\")\n",
        "print(confusion_matrix_initial)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3Kv94NoZTyA",
        "outputId": "c043e9c6-8b6d-4147-e9f3-bd212c4fe175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Set Accuracy: 0.8034 \n",
            "\n",
            "Confusion Matrix:\n",
            "         Actual\n",
            "Predicted  0  1  2\n",
            "        0 80  2 10\n",
            "        1 17 84 13\n",
            "        2  7  8 69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyperparameter Tuning for the Best Parameters"
      ],
      "metadata": {
        "id": "FIr4gd7Ikuv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Define Parameter Grid and Initialize Variables"
      ],
      "metadata": {
        "id": "Zi_NWubmkzUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Cross-validation for hyperparameter tuning\n",
        "param_grid <- expand.grid(\n",
        "  max_depth = c(3, 6, 9),\n",
        "  eta = c(0.1, 0.3, 0.5),\n",
        "  min_child_weight = c(1, 3, 5)\n",
        ")\n",
        "\n",
        "best_params <- NULL\n",
        "best_error <- Inf\n",
        "cv_results <- data.frame()\n",
        "\n",
        "for (i in 1:nrow(param_grid)) {\n",
        "  params_cv <- list(\n",
        "    objective = \"multi:softmax\",\n",
        "    num_class = 3,\n",
        "    eval_metric = \"merror\",\n",
        "    max_depth = param_grid$max_depth[i],\n",
        "    eta = param_grid$eta[i],\n",
        "    gamma = 0,\n",
        "    colsample_bytree = 0.8,\n",
        "    min_child_weight = param_grid$min_child_weight[i],\n",
        "    subsample = 0.8\n",
        "  )\n",
        "\n",
        "  cv_model <- xgb.cv(\n",
        "    params = params_cv,\n",
        "    data = dtrain,\n",
        "    nrounds = 200,\n",
        "    nfold = 5,\n",
        "    early_stopping_rounds = 10,\n",
        "    verbose = 0\n",
        "  )\n",
        "\n",
        "  min_error <- min(cv_model$evaluation_log$test_merror_mean)\n",
        "  best_nrounds <- which.min(cv_model$evaluation_log$test_merror_mean)\n",
        "\n",
        "  cv_results <- rbind(cv_results, data.frame(\n",
        "    max_depth = param_grid$max_depth[i],\n",
        "    eta = param_grid$eta[i],\n",
        "    min_child_weight = param_grid$min_child_weight[i],\n",
        "    test_merror = min_error,\n",
        "    nrounds = best_nrounds\n",
        "  ))\n",
        "\n",
        "  if (min_error < best_error) {\n",
        "    best_error <- min_error\n",
        "    best_params <- params_cv\n",
        "    best_params$nrounds <- best_nrounds\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "id": "UMzN860Yk0XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### The Best Parameters"
      ],
      "metadata": {
        "id": "JE0H5GJ5k5z5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Print results\n",
        "cat(\"Best Parameters:\\n\")\n",
        "print(best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ5V-jdqk_Ro",
        "outputId": "9b1df514-f5e7-4250-e1ac-b0226edf4eda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters:\n",
            "$objective\n",
            "[1] \"multi:softmax\"\n",
            "\n",
            "$num_class\n",
            "[1] 3\n",
            "\n",
            "$eval_metric\n",
            "[1] \"merror\"\n",
            "\n",
            "$max_depth\n",
            "[1] 3\n",
            "\n",
            "$eta\n",
            "[1] 0.3\n",
            "\n",
            "$gamma\n",
            "[1] 0\n",
            "\n",
            "$colsample_bytree\n",
            "[1] 0.8\n",
            "\n",
            "$min_child_weight\n",
            "[1] 3\n",
            "\n",
            "$subsample\n",
            "[1] 0.8\n",
            "\n",
            "$nrounds\n",
            "[1] 12\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Train Final Model with Best Parameters"
      ],
      "metadata": {
        "id": "zQ0l-7_HlDhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Train final model with best parameters\n",
        "final_model <- xgb.train(\n",
        "  params = best_params,\n",
        "  data = dtrain,\n",
        "  nrounds = best_params$nrounds,\n",
        "  watchlist = list(train = dtrain, test = dtest),\n",
        "  verbose = 0\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guT8QqwtlFvx",
        "outputId": "3e1f14e5-bb3b-4fdc-ecbc-e57aadccef85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[19:19:40] WARNING: src/learner.cc:767: \n",
            "Parameters: { \"nrounds\" } are not used.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Predictions and Evaluation"
      ],
      "metadata": {
        "id": "Tx6tVeXzlMNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Predict on test set\n",
        "y_pred <- predict(final_model, dtest)\n",
        "\n",
        "# Evaluate performance\n",
        "confusion_matrix <- table(Predicted = y_pred, Actual = y_test)\n",
        "accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)\n",
        "\n",
        "\n",
        "cat(\"\\nTest Set Accuracy:\", round(accuracy, 4), \"\\n\")\n",
        "cat(\"\\nConfusion Matrix:\\n\")\n",
        "print(confusion_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNc2qbtelNAu",
        "outputId": "cba156b5-5548-4ed1-ac0f-286a13f38e1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Set Accuracy: 0.8103 \n",
            "\n",
            "Confusion Matrix:\n",
            "         Actual\n",
            "Predicted  0  1  2\n",
            "        0 87  5 14\n",
            "        1 17 85 15\n",
            "        2  0  4 63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Feature Importance\n",
        "\n"
      ],
      "metadata": {
        "id": "a-JpJLkCZbAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Save feature importance plot\n",
        "importance <- xgb.importance(feature_names = colnames(X_train), model = final_model)\n",
        "xgb.plot.importance(importance, main = \"Feature Importance\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "lWflJ_FPZoso",
        "outputId": "724252d9-5267-4bd7-a173-7d2a92203136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary and Conclusion\n",
        "\n",
        "XGBoost is a powerful gradient boosting algorithm that iteratively builds decision trees to minimize a regularized objective function. It uses first- and second-order gradients to optimize tree construction, incorporates regularization to prevent overfitting, and leverages efficient algorithms for scalability. The key equations involve the objective function, gradient/Hessian calculations, and the gain metric for tree splitting, enabling robust and accurate predictions. This R-tutorials covered the implementation of XGBoost for both regression and classification tasks, demonstrating its flexibility and effectiveness in handling complex datasets. The manual implementation provided a deeper understanding of the underlying mechanics, while the use of the `{xgboost}` package showcased its practical application in real-world scenarios."
      ],
      "metadata": {
        "id": "Nb0kdVIuGlK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## References\n",
        "\n",
        "1.  Chen, T., & Guestrin, C. (2016). *XGBoost: A Scalable Tree Boosting System*. In *KDD '16* (pp. 785–794). arXiv:1603.02754.\n",
        "\n",
        "2.  Kuhn, M., & Johnson, K. (2019). *Applied Predictive Modeling*. Springer. ISBN: 978-1461468486.\n",
        "\n",
        "3.  Zhang, Y., & Haghani, A. (2015). *A gradient boosting method to improve travel time prediction*. *Transportation Research Part C*, 58, 308–324. DOI: 10.1016/j.trc.2015.02.019.\n",
        "\n",
        "4.  [XGBoost in R: A Step-by-Step Example DataCamp](https://www.datacamp.com/community/tutorials/xgboost-in-r)\n",
        "\n",
        "5.  [Machine Learning with XGBoost in R](Towards%20Data%20Science.%20https://towardsdatascience.com/machine-learning-with-xgboost-in-r)\n",
        "\n",
        "6.  [Gradient Boosting and Parameter Tuning in R](https://www.kaggle.com/code/camnugent/gradient-boosting-and-parameter-tuning-in-r/)\n",
        "\n",
        "7. [XGBoost Documentation](https://xgboost.readthedocs.io/en/stable/index.html)\n",
        "\n"
      ],
      "metadata": {
        "id": "ACNQyq1seLc7"
      }
    }
  ]
}