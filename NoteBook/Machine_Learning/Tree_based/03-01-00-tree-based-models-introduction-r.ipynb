{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-00-tree-based-models-introduction-r.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH9ES7RYWUdv"
      },
      "source": [
        "![alt text](http://drive.google.com/uc?export=view&id=1bLQ3nhDbZrCCqy_WCxxckOne2lgVvn3l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMDwUkD2NkMC"
      },
      "source": [
        "# Tree-based Machine Learning Models \n",
        "\n",
        "Tree-based models are a class of machine learning algorithms that use decision trees as building blocks to model relationships between features and target variables. These models partition the feature space into regions (via hierarchical splits) to make predictions. They are versatile, handling both classification and regression tasks, and are known for their interpretability (for single trees) and robustness (for ensembles).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrQshXqjWS7x"
      },
      "source": [
        "## Overview of Tree-based Algorithms\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Tree-based models are a class of machine learning algorithms that use decision trees as building blocks to model relationships between features and target variables. These models partition the feature space into regions (via hierarchical splits) to make predictions. They are versatile, handling both classification and regression tasks, and are known for their interpretability (for single trees) and robustness (for ensembles).\n",
        "\n",
        "These models split the data into subsets based on feature values, forming a hierarchical structure where:\n",
        "\n",
        "- `Nodes` represent decisions or conditions based on input features.\n",
        "- `Branches` represent the outcomes of those decisions.\n",
        "- `Leaves` represent the final predictions or outputs (e.g., a class label or a continuous value).\n",
        "\n",
        "Tree-based models are widely used for both **classification** (assigning categories to inputs) and **regression** (predicting continuous values).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhEJ07-7N28Y"
      },
      "source": [
        "## Advantages\n",
        "\n",
        "* Easy to interpret (especially single decision trees).\n",
        "* Handles both numerical and categorical data.\n",
        "* Robust to outliers and missing values.\n",
        "* Can model complex relationships without feature scaling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJiqtoVpN3oD"
      },
      "source": [
        "## Limitations\n",
        "\n",
        "* Single decision trees can overfit.\n",
        "* May require tuning to achieve optimal performance (e.g., tree depth, number of trees).\n",
        "* Can be computationally expensive for large datasets or deep trees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9su3bmygWk-u"
      },
      "source": [
        "\n",
        "## Types of Tree-Based Models\n",
        "\n",
        "Tree-based models can be broadly categorized into two main types: **single tree models** and **ensemble methods**. Single tree models consist of a single decision tree, while ensemble methods combine multiple trees to improve performance and robustness. The most common tree-based models include:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHx3FkVeWnxQ"
      },
      "source": [
        "### Decision Tree\n",
        "\n",
        "A **Decision Tree** is a predictive model that recursively splits the input data into regions based on feature values and makes a decision based on the majority class (for classification) or average value (for regression) in that region.\n",
        "\n",
        "- **How it works**:\n",
        "  - **Root Node**: Represents the entire dataset.\n",
        "  - **Splitting**: At each node, the algorithm selects a feature and a threshold that best splits the data into subsets, optimizing a criterion like Gini impurity (for classification) or mean squared error (for regression).\n",
        "  - **Branches**: Represent the decision rules (e.g., \"Age > 30\").\n",
        "  - **Leaf Nodes**: Terminal nodes that provide the final prediction.\n",
        "  - The tree is built by recursively splitting until a stopping condition is met (e.g., maximum depth, minimum samples per leaf).\n",
        "  \n",
        "- **Advantages**:\n",
        "  - Easy to interpret and visualize.\n",
        "  - Handles both categorical and numerical features.\n",
        "  - Non-linear relationships are captured naturally.\n",
        "\n",
        "- **Disadvantages**:\n",
        "  - Prone to **overfitting**, especially with deep trees, leading to poor generalization.\n",
        "  - Sensitive to small changes in data, which can result in entirely different trees.\n",
        "  - Biased toward features with more levels in categorical data.\n",
        "\n",
        "- **Example**:\n",
        "  To predict whether a person buys a product:\n",
        "  - Root: \"Is Income > $50K?\"\n",
        "    - Yes → Branch: \"Is Age > 25?\" → Leaf: \"Buy\"\n",
        "    - No → Branch: \"Is Student?\" → Leaf: \"Don’t Buy\"  \n",
        "- **Variants**: CART (Classification and Regression Trees), ID3, C4.5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZihdpLhOWoZe"
      },
      "source": [
        "### Bagging (Bootstrap Aggregating)\n",
        "\n",
        "**Bagging**, short for **Bootstrap Aggregating**, is an `ensemble` method that improves the stability and accuracy of decision trees by combining predictions from multiple trees trained on different subsets of the data.\n",
        "\n",
        "- **How it works**:\n",
        "  - **Bootstrap Sampling**: Randomly sample the dataset with replacement to create multiple subsets (bootstrap samples). Each subset is of the same size as the original dataset but may contain duplicates.\n",
        "  - **Train Independent Trees**: Build a decision tree for each bootstrap sample. Each tree is trained independently, without pruning, to capture diverse patterns.\n",
        "  - **Aggregation**:\n",
        "    - For **classification**: Combine predictions by majority voting across all trees.\n",
        "    - For **regression**: Average the predictions from all trees.\n",
        "  - The most well-known bagging algorithm is **Random Forest**, which adds randomness by selecting a random subset of features at each split.\n",
        "\n",
        "- **Advantages**:\n",
        "  - Reduces **variance** and overfitting by averaging predictions from diverse trees.\n",
        "  - Robust to noisy data and outliers.\n",
        "  - Random Forest improves on bagging by reducing correlation between trees through feature randomness.\n",
        "\n",
        "- **Disadvantages**:\n",
        "  - Less interpretable than a single decision tree.\n",
        "  - Computationally expensive due to training multiple trees.\n",
        "  - May not perform as well as boosting methods for complex datasets.\n",
        "\n",
        "- **Example**:\n",
        "  In a Random Forest for predicting house prices:\n",
        "  - Create 100 bootstrap samples from the dataset.\n",
        "  - Train 100 decision trees, each considering a random subset of features (e.g., square footage, location).\n",
        "  - Predict the price by averaging the predictions of all 100 trees.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sB4BIHXzWrFG"
      },
      "source": [
        "### Gradient Boosted Trees (GBT)\n",
        "\n",
        "**Gradient Boosted Trees (GBT)** is another `ensemble` method that builds decision trees sequentially, where each tree corrects the errors of the previous ones, optimizing a loss function using gradient descent.\n",
        "\n",
        "- **How it works**:\n",
        "  - **Initialization**: Start with an initial prediction (e.g., mean for regression or log-odds for classification).\n",
        "  - **Sequential Training**:\n",
        "    - Compute the **residuals** (errors) of the current model’s predictions.\n",
        "    - Fit a new decision tree to predict these residuals.\n",
        "    - Update the model by adding the new tree’s predictions, scaled by a **learning rate** (to control step size).\n",
        "  - **Gradient Descent**: The process minimizes a loss function (e.g., mean squared error for regression, log-loss for classification) by iteratively moving in the direction of the negative gradient.\n",
        "  - **Final Prediction**: Combine predictions from all trees, weighted by the learning rate.\n",
        "  - Popular implementations include **XGBoost**, **LightGBM**, and **CatBoost**.\n",
        "\n",
        "- **Advantages**:\n",
        "  - Highly accurate, often outperforming other algorithms in predictive tasks.\n",
        "  - Handles complex, non-linear relationships and feature interactions well.\n",
        "  - Flexible with customizable loss functions and regularization to prevent overfitting.\n",
        "\n",
        "- **Disadvantages**:\n",
        "  - Computationally intensive and slower to train than bagging.\n",
        "  - Sensitive to hyperparameter tuning (e.g., learning rate, tree depth).\n",
        "  - Less interpretable than single decision trees or bagging.\n",
        "\n",
        "- **Example**:\n",
        "  To predict customer churn:\n",
        "  - Start with an initial prediction (e.g., average churn rate).\n",
        "  - Compute residuals for mispredicted customers.\n",
        "  - Train a tree to predict these residuals.\n",
        "  - Update predictions and repeat for, say, 100 trees, combining them to get the final churn probability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGtCtD5EW0hz"
      },
      "source": [
        "### Comparison of Decision Trees, Bagging, and GBT\n",
        "\n",
        "| **Aspect**                | **Decision Tree**                     | **Bagging (e.g., Random Forest)**    | **Gradient Boosted Trees**          |\n",
        "|---------------------------|---------------------------------------|--------------------------------------|-------------------------------------|\n",
        "| `Structure`             | Single tree                          | Multiple independent trees          | Multiple sequential trees           |\n",
        "| `Training`              | Single pass                          | Parallel on bootstrap samples       | Sequential, correcting errors       |\n",
        "| `Goal`                  | Minimize impurity (e.g., Gini, MSE)  | Reduce variance via aggregation      | Minimize loss via gradient descent  |\n",
        "| `Strength`              | Simple, interpretable                | Robust, reduces overfitting         | High accuracy, handles complexity   |\n",
        "| `Weakness`              | Overfitting, unstable                | Less accurate than boosting         | Computationally intensive, complex  |\n",
        "| `Example Algorithm`     | CART                                 | Random Forest                       | XGBoost, LightGBM, CatBoost         |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl-amTyjW23x"
      },
      "source": [
        "\n",
        "## Applications  \n",
        "\n",
        "- **Decision Trees**: Simple rule-based systems (e.g., customer segmentation).  \n",
        "- **Random Forest/Gradient Boosting**: High-stakes tasks like fraud detection, ranking.  \n",
        "- **Extra Trees**: Large datasets requiring fast training.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Topics Covered\n",
        "\n",
        "\n",
        "This section of tutorial will cover the following topics:\n",
        "\n",
        "### 1.  [Decision Trees](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-00-tree-based-models-decision-tree-introduction-r.ipynb)\n",
        "\n",
        "1.1 [CART (Classification and Regression Trees)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-01-tree-based-models-decision-tree-cart-r.ipynb)\n",
        "\n",
        "1.2 [Conditional Inference Trees (CITs)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-02-tree-based-models-decision-tree-cit-r.ipynb)\n",
        "\n",
        "1.3 [Model-based Recursive Partitioning (MOB)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-03-tree-based-models-decision-tree-mob-r.ipynb)\n",
        "\n",
        "1.4 [C4.5 Model](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-04-tree-based-models-decision-tree-c45-r.ipynb)\n",
        "\n",
        "1.5 [C5.0 Model](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-05-tree-based-models-decision-tree-c50-r.ipynb)\n",
        "\n",
        "\n",
        "### 2.  [Bagging or Bootstrap Aggregating](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-00-tree-based-models-bagging-introduction-r.ipynb)\n",
        "\n",
        "2.1 [Bagged Trees](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-01-tree-based-models-bagging-bagged-trees-r.ipynb)\n",
        "    \n",
        "2.2 [Random Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-02-tree-based-models-bagging-randomforest-r.ipynb)\n",
        "    \n",
        "2.3 [Conditional Random Forests (cforest)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-03-tree-based-models-bagging-cforest-r.ipynb)\n",
        "    \n",
        "2.4 [Extremely Randomized Trees (Extra Trees)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-04-tree-based-models-bagging-extremely-randomized-trees-r.ipynb)\n",
        "    \n",
        "2.5 [Quantile Regression Forest (QRF)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-05-tree-based-models-bagging-quantile-regression-forest-r.ipynb)\n",
        "    \n",
        "2.6 [Random Forests Quantile Classifier](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-06-tree-based-models-bagging-quantile-classifier-forest-r.ipynb)\n",
        "    \n",
        "2.7 [Random Survival Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-07-tree-based-models-bagging-survival-forest-r.ipynb)\n",
        "    \n",
        "2.8 [Generalized Random Forests (GRF)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-00-tree-based-models-bagging-grf-introduction-r.ipynb)\n",
        "\n",
        "    \n",
        "2.8.1 [Survial Forests (SF)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-01-tree-based-models-bagging-grf-survival-forest-r.ipynb)\n",
        "\n",
        "      \n",
        "2.8.2 [Causal Forests (CF)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-02-tree-based-models-bagging-grf-causal-forest-r.ipynb)\n",
        "\n",
        "      \n",
        "2.8.3 [Causal Survival Forests (CSF)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-03-tree-based-models-bagging-grf-causal-survival-forest-r.ipynb)\n",
        "\n",
        "      \n",
        "2.8.4 [Multi-arm/multi-outcome Causal Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-04-tree-based-models-bagging-grf-arm-causal-forest-r.ipynb)\n",
        "\n",
        "      \n",
        "2.8.5 [Instrumental Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-05-tree-based-models-bagging-grf-instrumental-forest-r.ipynb)\n",
        "\n",
        "      \n",
        "2.8.6 [Linear Model Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-06-tree-based-models-bagging-grf-linear-model-forest-r.ipynb)\n",
        "\n",
        "   \n",
        "2.8.7 [Probability Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-07-tree-based-models-bagging-grf-probability-forest-r.ipynb)\n",
        "\n",
        "      \n",
        "2.8.8 [Regression Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-08-tree-based-models-bagging-grf-regression-forest-r.ipynb)\n",
        "\n",
        "      \n",
        "2.8.9 [Multi-task Regression Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-09-tree-based-models-bagging-grf-multitask-regression-forest-r.ipynb)\n",
        "     \n",
        "2.8.10 [Local Linear Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-10-tree-based-models-bagging-grf-local-linear-forest-r.ipynb)\n",
        "\n",
        "     \n",
        "2.8.11 [Boosted Regression Forest03-01](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-11-tree-based-models-bagging-grf-boosted-regression-forest-r.ipynb)\n",
        "\n",
        "\n",
        "### 3. [Gradient Boosted Trees (GBT)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-01-tree-based-models-gradient-boosted-introduction-r.ipynb)\n",
        "\n",
        "3.1 [Gradient Boosting Machine (GBM)](0https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-01-tree-based-models-gradient-boosted-gbm-r.ipynb)\n",
        "\n",
        "3.2 [Light Gradient Boosting Machine (lightGBM)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-02-tree-based-models-gradient-boosted-lightgbm-r.ipynb)\n",
        "\n",
        "3.3 [Extreme Gradient Boosting (XGboost)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-03-tree-based-models-gradient-boosted-xboost-r.ipynb)\n",
        "\n",
        "3.4 [Categorical Boosting (CatBoost)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-04-tree-based-models-gradient-boosted-catboost-r.ipynb)\n",
        "\n",
        "3.5 [Adaptive Boosting (AdaBoost)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-05-tree-based-models-gradient-boosted-adaboost-r.ipynb)\n",
        "\n",
        "3.6 [Gradient Boosted Survival Model](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-06-tree-based-models-gradient-boosted-survival-model-r.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuZdfdfGXN5g"
      },
      "source": [
        "## Summary and Conclusion\n",
        "\n",
        "Tree-based Algorithms are powerful tools in machine learning, offering a range of techniques for both classification and regression tasks. They are particularly useful for handling complex datasets with non-linear relationships and interactions between features. The choice of model depends on the specific problem, data characteristics, and desired interpretability versus predictive performance.\n",
        "In this section, we explored the foundational concepts of tree-based models, including decision trees, bagging, and gradient boosting. Each model has its strengths and weaknesses, and understanding these can help in selecting the right approach for a given problem.\n",
        "\n",
        "- **Decision Trees** are simple, interpretable models but prone to overfitting.\n",
        "- **Bagging (e.g., Random Forest)** reduces variance by averaging predictions from multiple trees trained on random data subsets.\n",
        "- **Gradient Boosted Trees (GBT)** achieve high accuracy by sequentially building trees to correct errors, using gradient descent to minimize a loss function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Further Reading and Resources\n",
        "\n",
        "Here are some recommended resources and further reading materials for learning about tree-based machine learning models, including decision trees, random forests, gradient boosting, and related techniques. These include books, academic papers, online courses, and tutorials, with links where available:\n",
        "\n",
        "### Books\n",
        "*\n",
        "1. **\"Introduction to Statistical Learning\" by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani**\n",
        "   - **Description**: A comprehensive introduction to machine learning, with detailed chapters on decision trees and ensemble methods like random forests and boosting.\n",
        "   - **Link**: [Springer](https://link.springer.com/book/10.1007/978-1-4614-7138-7) (free PDF available at [ISL Book](http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf)).\n",
        "   - **Relevance**: Chapters 8 and 10 cover tree-based methods and bagging/boosting.\n",
        "\n",
        "2. **\"The Elements of Statistical Learning\" by Trevor Hastie, Robert Tibshirani, and Jerome Friedman**\n",
        "   - **Description**: An advanced text that dives deep into tree-based models, including random forests and boosting algorithms like AdaBoost and XGBoost.\n",
        "   - **Link**: [Springer](https://link.springer.com/book/10.1007/978-0-387-84858-7) (free PDF available at [ESL Book](https://hastie.su.domains/ElemStatLearn/)).\n",
        "   - **Relevance**: Chapter 9 and 10 provide theoretical and practical insights into tree-based methods.\n",
        "\n",
        "### Academic Papers\n",
        "\n",
        "3. **\"Classification and Regression Trees (CART)\" by Leo Breiman et al. (1984)**\n",
        "   - **Description**: The foundational paper introducing the CART algorithm, a key decision tree method.\n",
        "   - **Link**: [Taylor & Francis](https://www.tandfonline.com/doi/abs/10.1080/01621459.1984.10478273) (requires subscription or institutional access).\n",
        "   - **Relevance**: Establishes the basis for modern tree-based modeling.\n",
        "\n",
        "4. **\"Random Forests\" by Leo Breiman (2001)**\n",
        "   - **Description**: Introduces the random forest algorithm, an ensemble method based on decision trees.\n",
        "   - **Link**: [Springer](https://link.springer.com/article/10.1023/A:1010933404324) (free access with registration).\n",
        "   - **Relevance**: Explains the concept of bagging and random feature selection in trees.\n",
        "\n",
        "5. **\"Stochastic Gradient Boosting\" by Jerome H. Friedman (1999)**\n",
        "   - **Description**: Details the gradient boosting framework, a powerful tree-based technique.\n",
        "   - **Link**: [Stanford](https://statweb.stanford.edu/~jhf/ftp/stobst.pdf) (free PDF).\n",
        "   - **Relevance**: Foundational for understanding modern boosting algorithms like XGBoost.\n",
        "\n",
        "### Online Courses and Tutorials\n",
        "\n",
        "6. **\"Machine Learning by Andrew Ng\" (Coursera)**\n",
        "   - **Description**: Covers decision trees and ensemble methods as part of supervised learning.\n",
        "   - **Link**: [Coursera](https://www.coursera.org/learn/machine-learning) (free to audit, subscription for certificate).\n",
        "   - **Relevance**: Week 6 includes decision trees and ensemble methods.\n",
        "\n",
        "7. **\"Practical Machine Learning with Python\" (edX)**\n",
        "   - **Description**: A hands-on course focusing on tree-based models like random forests and gradient boosting.\n",
        "   - **Link**: [edX](https://www.edx.org/course/practical-machine-learning-with-python) (free to audit, certificate with fee).\n",
        "   - **Relevance**: Includes practical implementations of tree-based algorithms.\n",
        "\n",
        "### Websites and Documentation\n",
        "\n",
        "8. **Scikit-Learn Documentation**\n",
        "   - **Description**: Provides detailed guides and examples for decision trees, random forests, and gradient boosting in Python.\n",
        "   - **Link**: [Scikit-Learn](https://scikit-learn.org/stable/modules/tree.html) (free).\n",
        "   - **Relevance**: Includes theory, code examples, and parameter tuning for tree-based models.\n",
        "\n",
        "9. **XGBoost Documentation**\n",
        "   - **Description**: Official documentation for the XGBoost library, a leading implementation of gradient boosting.\n",
        "   - **Link**: [XGBoost](https://xgboost.readthedocs.io/en/stable/) (free).\n",
        "   - **Relevance**: Practical guide for applying and optimizing tree-based boosting.\n",
        "\n",
        "10. **RDocumentation (rpart and randomForest Packages)**\n",
        "    - **Description**: Documentation for R packages implementing decision trees (`rpart`) and random forests (`randomForest`).\n",
        "    - **Link**: [rpart](https://www.rdocumentation.org/packages/rpart/versions/4.1-15) | [randomForest](https://www.rdocumentation.org/packages/randomForest/versions/4.7-1.1) (free).\n",
        "    - **Relevance**: Essential for tree-based modeling in R.\n",
        "\n",
        "### Additional Resources\n",
        "\n",
        "11. **\"Understanding Random Forests\" (Towards Data Science)**\n",
        "    - **Description**: A blog post explaining the intuition and mechanics of random forests with visualizations.\n",
        "    - **Link**: [Towards Data Science](https://towardsdatascience.com/understanding-random-forest-58381e0602d2) (free).\n",
        "    - **Relevance**: Beginner-friendly overview with practical examples.\n",
        "\n",
        "12. **YouTube: \"Gradient Boosting from Scratch\" by StatQuest with Josh Starmer**\n",
        "    - **Description**: A video tutorial explaining gradient boosting with clear animations.\n",
        "    - **Link**: [YouTube](https://www.youtube.com/watch?v=3CC4N4z3GJc) (free).\n",
        "    - **Relevance**: Visual and intuitive explanation of a key tree-based method.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "\n",
        "\n",
        "This section of tutorial will cover the following topics:\n",
        "\n",
        "### 1.  [Decision Trees](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-00-tree-based-models-decision-tree-introduction-r.ipynb)\n",
        "\n",
        "1.1 [CART (Classification and Regression Trees)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-01-tree-based-models-decision-tree-cart-r.ipynb)\n",
        "\n",
        "1.2 [Conditional Inference Trees (CITs)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-02-tree-based-models-decision-tree-cit-r.ipynb)\n",
        "\n",
        "1.3 [Model-based Recursive Partitioning (MOB)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-03-tree-based-models-decision-tree-mob-r.ipynb)\n",
        "\n",
        "1.4 [C4.5 Model](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-04-tree-based-models-decision-tree-c45-r.ipynb)\n",
        "\n",
        "1.5 [C5.0 Model](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-05-tree-based-models-decision-tree-c50-r.ipynb)\n",
        "\n",
        "\n",
        "### 2.  [Bagging or Bootstrap Aggregating](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-00-tree-based-models-bagging-introduction-r.ipynb)\n",
        "\n",
        "2.1 [Bagged Trees](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-01-tree-based-models-bagging-bagged-trees-r.ipynb)\n",
        "    \n",
        "2.2 [Random Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-02-tree-based-models-bagging-randomforest-r.ipynb)\n",
        "    \n",
        "2.3 [Conditional Random Forests (cforest)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-03-tree-based-models-bagging-cforest-r.ipynb)\n",
        "    \n",
        "2.4 [Extremely Randomized Trees (Extra Trees)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-04-tree-based-models-bagging-extremely-randomized-trees-r.ipynb)\n",
        "    \n",
        "2.5 [Quantile Regression Forest (QRF)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-05-tree-based-models-bagging-quantile-regression-forest-r.ipynb)\n",
        "    \n",
        "2.6 [Random Forests Quantile Classifier](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-06-tree-based-models-bagging-quantile-classifier-forest-r.ipynb)\n",
        "    \n",
        "2.7 [Random Survival Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-07-tree-based-models-bagging-survival-forest-r.ipynb)\n",
        "    \n",
        "2.8 [Generalized Random Forests (GRF)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-00-tree-based-models-bagging-grf-introduction-r.ipynb)\n",
        "\n",
        "    \n",
        "2.8.1 [Survial Forests (SF)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-01-tree-based-models-bagging-grf-survival-forest-r.ipynb)\n",
        "\n",
        "      \n",
        "2.8.2 [Causal Forests (CF)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-02-tree-based-models-bagging-grf-causal-forest-r.ipynb)\n",
        "\n",
        "      \n",
        "2.8.3 [Causal Survival Forests (CSF)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-03-tree-based-models-bagging-grf-causal-survival-forest-r.ipynb)\n",
        "\n",
        "      \n",
        "2.8.4 [Multi-arm/multi-outcome Causal Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-04-tree-based-models-bagging-grf-arm-causal-forest-r.ipynb)\n",
        "\n",
        "      \n",
        "2.8.5 [Instrumental Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-05-tree-based-models-bagging-grf-instrumental-forest-r.ipynb)\n",
        "\n",
        "      \n",
        "2.8.6 [Linear Model Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-06-tree-based-models-bagging-grf-linear-model-forest-r.ipynb)\n",
        "\n",
        "   \n",
        "2.8.7 [Probability Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-07-tree-based-models-bagging-grf-probability-forest-r.ipynb)\n",
        "\n",
        "      \n",
        "2.8.8 [Regression Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-08-tree-based-models-bagging-grf-regression-forest-r.ipynb)\n",
        "\n",
        "      \n",
        "2.8.9 [Multi-task Regression Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-09-tree-based-models-bagging-grf-multitask-regression-forest-r.ipynb)\n",
        "     \n",
        "2.8.10 [Local Linear Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-10-tree-based-models-bagging-grf-local-linear-forest-r.ipynb)\n",
        "\n",
        "     \n",
        "2.8.11 [Boosted Regression Forest03-01](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-11-tree-based-models-bagging-grf-boosted-regression-forest-r.ipynb)\n",
        "\n",
        "\n",
        "### 3. [Gradient Boosted Trees (GBT)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-01-tree-based-models-gradient-boosted-introduction-r.ipynb)\n",
        "\n",
        "3.1 [Gradient Boosting Machine (GBM)](0https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-01-tree-based-models-gradient-boosted-gbm-r.ipynb)\n",
        "\n",
        "3.2 [Light Gradient Boosting Machine (lightGBM)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-02-tree-based-models-gradient-boosted-lightgbm-r.ipynb)\n",
        "\n",
        "3.3 [Extreme Gradient Boosting (XGboost)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-03-tree-based-models-gradient-boosted-xboost-r.ipynb)\n",
        "\n",
        "3.4 [Categorical Boosting (CatBoost)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-04-tree-based-models-gradient-boosted-catboost-r.ipynb)\n",
        "\n",
        "3.5 [Adaptive Boosting (AdaBoost)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-05-tree-based-models-gradient-boosted-adaboost-r.ipynb)\n",
        "\n",
        "3.6 [Gradient Boosted Survival Model](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-06-tree-based-models-gradient-boosted-survival-model-r.ipynb)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPH8bMj6b9mIh7XyTor00ts",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
