{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNKWwuvQojQj+AIDktcjTbk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-00-tree-based-models-introduction-r.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![alt text](http://drive.google.com/uc?export=view&id=1bLQ3nhDbZrCCqy_WCxxckOne2lgVvn3l)"
      ],
      "metadata": {
        "id": "AH9ES7RYWUdv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tree-based Algorithms\n",
        "\n",
        "Tree-based Algorithms are a class of machine learning algorithms that use decision trees as building blocks to model relationships between features and target variables. These models partition the feature space into regions (via hierarchical splits) to make predictions. They are versatile, handling both classification and regression tasks, and are known for their interpretability (for single trees) and robustness (for ensembles).\n",
        "\n"
      ],
      "metadata": {
        "id": "UMDwUkD2NkMC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section of tutorial will cover the following topics:\n",
        "\n",
        "### 1.  [Decision Trees](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-00-tree-based-models-decision-tree-introduction-r.ipynb)\n",
        "\n",
        "1.1 [CART (Classification and Regression Trees)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-01-tree-based-models-decision-tree-cart-r.ipynb)\n",
        "\n",
        "1.2 [Conditional Inference Trees (CITs)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-02-tree-based-models-decision-tree-cit-r.ipynb)\n",
        "\n",
        "1.3 [Model-based Recursive Partitioning (MOB)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-03-tree-based-models-decision-tree-mob-r.ipynb)\n",
        "\n",
        "1.4 [C4.5 Model](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-04-tree-based-models-decision-tree-c45-r.ipynb)\n",
        "\n",
        "1.5 [C5.0 Model](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-05-tree-based-models-decision-tree-c50-r.ipynb)\n",
        "\n",
        "\n",
        "### 2.  [Bagging or Bootstrap Aggregating](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-00-tree-based-models-bagging-introduction-r.ipynb)\n",
        "\n",
        "2.1 [Bagged Trees](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-01-tree-based-models-bagging-bagged-trees-r.ipynb)\n",
        "    \n",
        "2.2 [Random Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-02-tree-based-models-bagging-randomforest-r.ipynb)\n",
        "    \n",
        "2.3 [Conditional Random Forests (cforest)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-03-tree-based-models-bagging-cforest-r.ipynb)\n",
        "    \n",
        "2.4 [Extremely Randomized Trees (Extra Trees)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-04-tree-based-models-bagging-extremely-randomized-trees-r.ipynb)\n",
        "    \n",
        "2.5 [Quantile Regression Forest (QRF)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-05-tree-based-models-bagging-quantile-regression-forest-r.ipynb)\n",
        "    \n",
        "2.6 [Random Forests Quantile Classifier](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-06-tree-based-models-bagging-quantile-classifier-forest-r.ipynb)\n",
        "    \n",
        "2.7 [Random Survival Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-07-tree-based-models-bagging-survival-forest-r.ipynb)\n",
        "    \n",
        "2.8 [Generalized Random Forests (GRF)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-00-tree-based-models-bagging-grf-introduction-r.ipynb)\n",
        "\n",
        "    \n",
        "2.8.1 [Survial Forests (SF)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-01-tree-based-models-bagging-grf-survival-forest-r.ipynb)\n",
        "\n",
        "      \n",
        "2.8.2 [Causal Forests (CF)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-02-tree-based-models-bagging-grf-causal-forest-r.ipynb)\n",
        "\n",
        "      \n",
        "2.8.3 [Causal Survival Forests (CSF)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-03-tree-based-models-bagging-grf-causal-survival-forest-r.ipynb)\n",
        "\n",
        "      \n",
        "2.8.4 [Multi-arm/multi-outcome Causal Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-04-tree-based-models-bagging-grf-arm-causal-forest-r.ipynb)\n",
        "\n",
        "      \n",
        "2.8.5 [Instrumental Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-05-tree-based-models-bagging-grf-instrumental-forest-r.ipynb)\n",
        "\n",
        "      \n",
        "2.8.6 [Linear Model Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-06-tree-based-models-bagging-grf-linear-model-forest-r.ipynb)\n",
        "\n",
        "   \n",
        "2.8.7 [Probability Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-07-tree-based-models-bagging-grf-probability-forest-r.ipynb)\n",
        "\n",
        "      \n",
        "2.8.8 [Regression Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-08-tree-based-models-bagging-grf-regression-forest-r.ipynb)\n",
        "\n",
        "      \n",
        "2.8.9 [Multi-task Regression Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-09-tree-based-models-bagging-grf-multitask-regression-forest-r.ipynb)\n",
        "     \n",
        "2.8.10 [Local Linear Forest](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-10-tree-based-models-bagging-grf-local-linear-forest-r.ipynb)\n",
        "\n",
        "     \n",
        "2.8.11 [Boosted Regression Forest03-01](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-02-08-11-tree-based-models-bagging-grf-boosted-regression-forest-r.ipynb)\n",
        "\n",
        "\n",
        "### 3. [Gradient Boosted Trees (GBT)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-01-tree-based-models-gradient-boosted-introduction-r.ipynb)\n",
        "\n",
        "3.1 [Gradient Boosting Machine (GBM)](0https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-01-tree-based-models-gradient-boosted-gbm-r.ipynb)\n",
        "\n",
        "3.2 [Light Gradient Boosting Machine (lightGBM)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-02-tree-based-models-gradient-boosted-lightgbm-r.ipynb)\n",
        "\n",
        "3.3 [Extreme Gradient Boosting (XGboost)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-03-tree-based-models-gradient-boosted-xboost-r.ipynb)\n",
        "\n",
        "3.4 [Categorical Boosting (CatBoost)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-04-tree-based-models-gradient-boosted-catboost-r.ipynb)\n",
        "\n",
        "3.5 [Adaptive Boosting (AdaBoost)](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-05-tree-based-models-gradient-boosted-adaboost-r.ipynb)\n",
        "\n",
        "3.6 [Gradient Boosted Survival Model](https://github.com/zia207/r-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-06-tree-based-models-gradient-boosted-survival-model-r.ipynb)\n"
      ],
      "metadata": {
        "id": "eR07i1HAPocM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview of Tree-based Algorithms\n",
        "\n",
        "> Add blockquote\n",
        "\n",
        "\n",
        "\n",
        "Tree-based models are a class of machine learning algorithms that use decision trees as building blocks to model relationships between features and target variables. These models partition the feature space into regions (via hierarchical splits) to make predictions. They are versatile, handling both classification and regression tasks, and are known for their interpretability (for single trees) and robustness (for ensembles).\n",
        "\n",
        "These models split the data into subsets based on feature values, forming a hierarchical structure where:\n",
        "\n",
        "- `Nodes` represent decisions or conditions based on input features.\n",
        "- `Branches` represent the outcomes of those decisions.\n",
        "- `Leaves` represent the final predictions or outputs (e.g., a class label or a continuous value).\n",
        "\n",
        "Tree-based models are widely used for both **classification** (assigning categories to inputs) and **regression** (predicting continuous values).\n",
        "\n"
      ],
      "metadata": {
        "id": "TrQshXqjWS7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advantages\n",
        "\n",
        "* Easy to interpret (especially single decision trees).\n",
        "* Handles both numerical and categorical data.\n",
        "* Robust to outliers and missing values.\n",
        "* Can model complex relationships without feature scaling."
      ],
      "metadata": {
        "id": "bhEJ07-7N28Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limitations\n",
        "\n",
        "* Single decision trees can overfit.\n",
        "* May require tuning to achieve optimal performance (e.g., tree depth, number of trees).\n",
        "* Can be computationally expensive for large datasets or deep trees."
      ],
      "metadata": {
        "id": "NJiqtoVpN3oD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Types of Tree-Based Models\n",
        "\n",
        "Tree-based models can be broadly categorized into two main types: **single tree models** and **ensemble methods**. Single tree models consist of a single decision tree, while ensemble methods combine multiple trees to improve performance and robustness. The most common tree-based models include:"
      ],
      "metadata": {
        "id": "9su3bmygWk-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree\n",
        "\n",
        "A **Decision Tree** is a predictive model that recursively splits the input data into regions based on feature values and makes a decision based on the majority class (for classification) or average value (for regression) in that region.\n",
        "\n",
        "- **How it works**:\n",
        "  - **Root Node**: Represents the entire dataset.\n",
        "  - **Splitting**: At each node, the algorithm selects a feature and a threshold that best splits the data into subsets, optimizing a criterion like Gini impurity (for classification) or mean squared error (for regression).\n",
        "  - **Branches**: Represent the decision rules (e.g., \"Age > 30\").\n",
        "  - **Leaf Nodes**: Terminal nodes that provide the final prediction.\n",
        "  - The tree is built by recursively splitting until a stopping condition is met (e.g., maximum depth, minimum samples per leaf).\n",
        "  \n",
        "- **Advantages**:\n",
        "  - Easy to interpret and visualize.\n",
        "  - Handles both categorical and numerical features.\n",
        "  - Non-linear relationships are captured naturally.\n",
        "\n",
        "- **Disadvantages**:\n",
        "  - Prone to **overfitting**, especially with deep trees, leading to poor generalization.\n",
        "  - Sensitive to small changes in data, which can result in entirely different trees.\n",
        "  - Biased toward features with more levels in categorical data.\n",
        "\n",
        "- **Example**:\n",
        "  To predict whether a person buys a product:\n",
        "  - Root: \"Is Income > $50K?\"\n",
        "    - Yes → Branch: \"Is Age > 25?\" → Leaf: \"Buy\"\n",
        "    - No → Branch: \"Is Student?\" → Leaf: \"Don’t Buy\"  \n",
        "- **Variants**: CART (Classification and Regression Trees), ID3, C4.5."
      ],
      "metadata": {
        "id": "UHx3FkVeWnxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bagging (Bootstrap Aggregating)\n",
        "\n",
        "**Bagging**, short for **Bootstrap Aggregating**, is an `ensemble` method that improves the stability and accuracy of decision trees by combining predictions from multiple trees trained on different subsets of the data.\n",
        "\n",
        "- **How it works**:\n",
        "  - **Bootstrap Sampling**: Randomly sample the dataset with replacement to create multiple subsets (bootstrap samples). Each subset is of the same size as the original dataset but may contain duplicates.\n",
        "  - **Train Independent Trees**: Build a decision tree for each bootstrap sample. Each tree is trained independently, without pruning, to capture diverse patterns.\n",
        "  - **Aggregation**:\n",
        "    - For **classification**: Combine predictions by majority voting across all trees.\n",
        "    - For **regression**: Average the predictions from all trees.\n",
        "  - The most well-known bagging algorithm is **Random Forest**, which adds randomness by selecting a random subset of features at each split.\n",
        "\n",
        "- **Advantages**:\n",
        "  - Reduces **variance** and overfitting by averaging predictions from diverse trees.\n",
        "  - Robust to noisy data and outliers.\n",
        "  - Random Forest improves on bagging by reducing correlation between trees through feature randomness.\n",
        "\n",
        "- **Disadvantages**:\n",
        "  - Less interpretable than a single decision tree.\n",
        "  - Computationally expensive due to training multiple trees.\n",
        "  - May not perform as well as boosting methods for complex datasets.\n",
        "\n",
        "- **Example**:\n",
        "  In a Random Forest for predicting house prices:\n",
        "  - Create 100 bootstrap samples from the dataset.\n",
        "  - Train 100 decision trees, each considering a random subset of features (e.g., square footage, location).\n",
        "  - Predict the price by averaging the predictions of all 100 trees.  "
      ],
      "metadata": {
        "id": "ZihdpLhOWoZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Boosted Trees (GBT)\n",
        "\n",
        "**Gradient Boosted Trees (GBT)** is another `ensemble` method that builds decision trees sequentially, where each tree corrects the errors of the previous ones, optimizing a loss function using gradient descent.\n",
        "\n",
        "- **How it works**:\n",
        "  - **Initialization**: Start with an initial prediction (e.g., mean for regression or log-odds for classification).\n",
        "  - **Sequential Training**:\n",
        "    - Compute the **residuals** (errors) of the current model’s predictions.\n",
        "    - Fit a new decision tree to predict these residuals.\n",
        "    - Update the model by adding the new tree’s predictions, scaled by a **learning rate** (to control step size).\n",
        "  - **Gradient Descent**: The process minimizes a loss function (e.g., mean squared error for regression, log-loss for classification) by iteratively moving in the direction of the negative gradient.\n",
        "  - **Final Prediction**: Combine predictions from all trees, weighted by the learning rate.\n",
        "  - Popular implementations include **XGBoost**, **LightGBM**, and **CatBoost**.\n",
        "\n",
        "- **Advantages**:\n",
        "  - Highly accurate, often outperforming other algorithms in predictive tasks.\n",
        "  - Handles complex, non-linear relationships and feature interactions well.\n",
        "  - Flexible with customizable loss functions and regularization to prevent overfitting.\n",
        "\n",
        "- **Disadvantages**:\n",
        "  - Computationally intensive and slower to train than bagging.\n",
        "  - Sensitive to hyperparameter tuning (e.g., learning rate, tree depth).\n",
        "  - Less interpretable than single decision trees or bagging.\n",
        "\n",
        "- **Example**:\n",
        "  To predict customer churn:\n",
        "  - Start with an initial prediction (e.g., average churn rate).\n",
        "  - Compute residuals for mispredicted customers.\n",
        "  - Train a tree to predict these residuals.\n",
        "  - Update predictions and repeat for, say, 100 trees, combining them to get the final churn probability.\n"
      ],
      "metadata": {
        "id": "sB4BIHXzWrFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison of Decision Trees, Bagging, and GBT\n",
        "\n",
        "| **Aspect**                | **Decision Tree**                     | **Bagging (e.g., Random Forest)**    | **Gradient Boosted Trees**          |\n",
        "|---------------------------|---------------------------------------|--------------------------------------|-------------------------------------|\n",
        "| `Structure`             | Single tree                          | Multiple independent trees          | Multiple sequential trees           |\n",
        "| `Training`              | Single pass                          | Parallel on bootstrap samples       | Sequential, correcting errors       |\n",
        "| `Goal`                  | Minimize impurity (e.g., Gini, MSE)  | Reduce variance via aggregation      | Minimize loss via gradient descent  |\n",
        "| `Strength`              | Simple, interpretable                | Robust, reduces overfitting         | High accuracy, handles complexity   |\n",
        "| `Weakness`              | Overfitting, unstable                | Less accurate than boosting         | Computationally intensive, complex  |\n",
        "| `Example Algorithm`     | CART                                 | Random Forest                       | XGBoost, LightGBM, CatBoost         |\n",
        "\n"
      ],
      "metadata": {
        "id": "kGtCtD5EW0hz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Applications  \n",
        "\n",
        "- **Decision Trees**: Simple rule-based systems (e.g., customer segmentation).  \n",
        "- **Random Forest/Gradient Boosting**: High-stakes tasks like fraud detection, ranking.  \n",
        "- **Extra Trees**: Large datasets requiring fast training.  "
      ],
      "metadata": {
        "id": "hl-amTyjW23x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary and Conclusion\n",
        "\n",
        "Tree-based Algorithms are powerful tools in machine learning, offering a range of techniques for both classification and regression tasks. They are particularly useful for handling complex datasets with non-linear relationships and interactions between features. The choice of model depends on the specific problem, data characteristics, and desired interpretability versus predictive performance.\n",
        "In this section, we explored the foundational concepts of tree-based models, including decision trees, bagging, and gradient boosting. Each model has its strengths and weaknesses, and understanding these can help in selecting the right approach for a given problem.\n",
        "\n",
        "- **Decision Trees** are simple, interpretable models but prone to overfitting.\n",
        "- **Bagging (e.g., Random Forest)** reduces variance by averaging predictions from multiple trees trained on random data subsets.\n",
        "- **Gradient Boosted Trees (GBT)** achieve high accuracy by sequentially building trees to correct errors, using gradient descent to minimize a loss function.\n"
      ],
      "metadata": {
        "id": "XuZdfdfGXN5g"
      }
    }
  ]
}